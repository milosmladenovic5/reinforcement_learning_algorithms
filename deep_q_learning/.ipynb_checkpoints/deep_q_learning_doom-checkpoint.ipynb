{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf     \n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "import environment_creation as ec\n",
    "import frame_preprocessing as fp\n",
    "import frames_stacking as fs\n",
    "import memory as mem\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = ec.create_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frame preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack_frames\n",
    "stack_size = 4\n",
    "stacked_frames = deque([np.zeros((84, 84), dtype = np.int) for i in range(stack_size)], maxlen = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model hyperparameters\n",
    "state_size = [84, 84, 4]\n",
    "action_size = game.get_available_buttons_size()\n",
    "learning_rate = 0.0002\n",
    "\n",
    "#training hyperparameters\n",
    "total_episodes = 500\n",
    "max_steps = 300\n",
    "batch_size = 128\n",
    "\n",
    "#exploration/epsilon greedy strategy parameters\n",
    "explore_start = 1.0\n",
    "explore_stop = 0.05\n",
    "decay_rate = 0.0003\n",
    "\n",
    "#q leraning hyperparameters\n",
    "gamma = 0.95 #discount rate\n",
    "\n",
    "###memory hyperparameters\n",
    "pretrain_length = batch_size #number of experiences stored in memory when initialized for the first time\n",
    "memory_size = 100000 #no of experiences memory can keep\n",
    "\n",
    "training = True\n",
    "episode_render = False\n",
    "\n",
    "stacked_frames = deque([np.zeros((84, 84), dtype = np.int) for i in range(stack_size)], maxlen = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name = \"DeepQNetwork\"):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            #here we create the placeholders\n",
    "            \n",
    "            #[None, 84, 84, 4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, 3], name = \"actions_\")\n",
    "        \n",
    "            #target_Q is the R(s, a) + ymax Q^(s', a')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name = \"targetQ\")\n",
    "            \n",
    "            # Input is 84x84x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                         filters = 32,\n",
    "                                         kernel_size = [8,8],\n",
    "                                         strides = [4,4],\n",
    "                                         padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv1\")\n",
    "            \n",
    "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm1')\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
    "            ## --> [20, 20, 32]\n",
    "            \n",
    "            #second\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv2\")\n",
    "        \n",
    "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm2')\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "            ## --> [9, 9, 64]\n",
    "            \n",
    "            #thirds\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv3\")\n",
    "        \n",
    "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm3')\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "            ## --> [3, 3, 128]\n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            ## --> [1152]\n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                      units = 512,\n",
    "                                      activation = tf.nn.elu,\n",
    "                                      kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                      name = 'fc1')\n",
    "            \n",
    "            self.output = tf.layers.dense(inputs = self.fc, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units = 3, \n",
    "                                        activation=None)\n",
    "            \n",
    "            #Q is our predicted Q value\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis = 1)\n",
    "            \n",
    "            #loss is the difference between predicted Q value and Q_target\n",
    "            # sum(q_target - q)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "DeepQNetwork = DeepQNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = fp.preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the memory class\n",
    "#instantiate the memory\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "#now dealing with the empty memory problem - prepopulating memory by taking random actions\n",
    "#and storing experience (state, action, reward, new_state)\n",
    "\n",
    "game.new_episode()\n",
    "for i in range(pretrain_length):\n",
    "    #if it's the first step\n",
    "    if i==0:\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    #take random action\n",
    "    action = random.choice(possible_actions)\n",
    "   \n",
    "    #get rewardd\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    #look if episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        #add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        #start a new episode\n",
    "        game.new_episode()\n",
    "        \n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames =  stack_frames(stacked_frames, state, True)\n",
    "    else:\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        #add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        #our state is now the next state\n",
    "        state = next_state    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup tensorboard writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
    "\n",
    "tf.summary.scalar(\"Loss\", DeepQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init the weights\n",
    "#init the environment\n",
    "#init decay rate\n",
    "\n",
    "#the following function does the epsilon-greedy strategy part - predicts action\n",
    "def predict_action(explore_start, explore_top, decay_rate, decay_step, state, actions):\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "    \n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if(explore_probability > exp_exp_tradeoff):\n",
    "        action = random.choice(possible_actions)\n",
    "    else:\n",
    "        #get action from q-network (exploitation)\n",
    "        Qs = sess.run(DeepQNetwork.output, feed_dict = {DeepQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        #take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "    \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: -380.0 Training loss: 4.9545 Explore P: 0.9182\n",
      "Model saved.\n",
      "Episode: 1 Total reward: 95.0 Training loss: 11.8917 Explore P: 0.9167\n",
      "Episode: 2 Total reward: -375.0 Training loss: 2.0303 Explore P: 0.8421\n",
      "Episode: 3 Total reward: -385.0 Training loss: 1.2267 Explore P: 0.7739\n",
      "Episode: 4 Total reward: -375.0 Training loss: 1.4879 Explore P: 0.7116\n",
      "Episode: 5 Total reward: 95.0 Training loss: 3.7746 Explore P: 0.7104\n",
      "Model saved.\n",
      "Episode: 6 Total reward: 95.0 Training loss: 3.4934 Explore P: 0.7092\n",
      "Episode: 7 Total reward: 95.0 Training loss: 0.8728 Explore P: 0.7080\n",
      "Episode: 8 Total reward: 95.0 Training loss: 4.9657 Explore P: 0.7069\n",
      "Episode: 9 Total reward: 64.0 Training loss: 10.2954 Explore P: 0.7006\n",
      "Episode: 10 Total reward: -224.0 Training loss: 2.6196 Explore P: 0.6518\n",
      "Model saved.\n",
      "Episode: 11 Total reward: 66.0 Training loss: 17.1618 Explore P: 0.6464\n",
      "Episode: 12 Total reward: 92.0 Training loss: 2.0068 Explore P: 0.6448\n",
      "Episode: 13 Total reward: 94.0 Training loss: 7.2322 Explore P: 0.6435\n",
      "Episode: 14 Total reward: 95.0 Training loss: 4.1446 Explore P: 0.6424\n",
      "Episode: 15 Total reward: 93.0 Training loss: 4.6013 Explore P: 0.6410\n",
      "Model saved.\n",
      "Episode: 16 Total reward: -156.0 Training loss: 3.1691 Explore P: 0.6046\n",
      "Episode: 17 Total reward: 95.0 Training loss: 2.9681 Explore P: 0.6036\n",
      "Episode: 18 Total reward: 95.0 Training loss: 4.5530 Explore P: 0.6026\n",
      "Episode: 19 Total reward: 95.0 Training loss: 3.1956 Explore P: 0.6016\n",
      "Episode: 20 Total reward: 95.0 Training loss: 2.9461 Explore P: 0.6006\n",
      "Model saved.\n",
      "Episode: 21 Total reward: 86.0 Training loss: 1.7808 Explore P: 0.5982\n",
      "Episode: 22 Total reward: 94.0 Training loss: 2.1065 Explore P: 0.5970\n",
      "Episode: 23 Total reward: -375.0 Training loss: 4.2262 Explore P: 0.5499\n",
      "Episode: 24 Total reward: 93.0 Training loss: 1.9984 Explore P: 0.5487\n",
      "Episode: 25 Total reward: 69.0 Training loss: 2.5169 Explore P: 0.5447\n",
      "Model saved.\n",
      "Episode: 26 Total reward: 91.0 Training loss: 1.6608 Explore P: 0.5432\n",
      "Episode: 27 Total reward: 60.0 Training loss: 5.5305 Explore P: 0.5379\n",
      "Episode: 28 Total reward: -375.0 Training loss: 1.2162 Explore P: 0.4959\n",
      "Episode: 29 Total reward: -36.0 Training loss: 2.0798 Explore P: 0.4812\n",
      "Episode: 30 Total reward: -380.0 Training loss: 1.9530 Explore P: 0.4441\n",
      "Model saved.\n",
      "Episode: 31 Total reward: 95.0 Training loss: 1.1986 Explore P: 0.4434\n",
      "Episode: 32 Total reward: 20.0 Training loss: 3.4762 Explore P: 0.4357\n",
      "Episode: 33 Total reward: 95.0 Training loss: 3.4296 Explore P: 0.4350\n",
      "Episode: 34 Total reward: -375.0 Training loss: 1.3229 Explore P: 0.4018\n",
      "Episode: 35 Total reward: 95.0 Training loss: 2.6066 Explore P: 0.4012\n",
      "Model saved.\n",
      "Episode: 36 Total reward: 50.0 Training loss: 2.3191 Explore P: 0.3969\n",
      "Episode: 37 Total reward: 95.0 Training loss: 2.6917 Explore P: 0.3963\n",
      "Episode: 38 Total reward: -365.0 Training loss: 1.5488 Explore P: 0.3665\n",
      "Episode: 39 Total reward: 95.0 Training loss: 1.9088 Explore P: 0.3659\n",
      "Episode: 40 Total reward: 94.0 Training loss: 3.0453 Explore P: 0.3652\n",
      "Model saved.\n",
      "Episode: 41 Total reward: 95.0 Training loss: 3.5396 Explore P: 0.3647\n",
      "Episode: 42 Total reward: 89.0 Training loss: 1.4298 Explore P: 0.3636\n",
      "Episode: 43 Total reward: -375.0 Training loss: 0.8470 Explore P: 0.3366\n",
      "Episode: 44 Total reward: -93.0 Training loss: 1.6777 Explore P: 0.3232\n",
      "Episode: 45 Total reward: 93.0 Training loss: 1.7032 Explore P: 0.3226\n",
      "Model saved.\n",
      "Episode: 46 Total reward: 95.0 Training loss: 1.0526 Explore P: 0.3221\n",
      "Episode: 47 Total reward: 95.0 Training loss: 2.5056 Explore P: 0.3216\n",
      "Episode: 48 Total reward: 95.0 Training loss: 1.5431 Explore P: 0.3211\n",
      "Episode: 49 Total reward: 44.0 Training loss: 2.3150 Explore P: 0.3173\n",
      "Episode: 50 Total reward: -370.0 Training loss: 0.8754 Explore P: 0.2943\n",
      "Model saved.\n",
      "Episode: 51 Total reward: 95.0 Training loss: 1.6975 Explore P: 0.2939\n",
      "Episode: 52 Total reward: -375.0 Training loss: 0.7940 Explore P: 0.2729\n",
      "Episode: 53 Total reward: -365.0 Training loss: 0.5773 Explore P: 0.2537\n",
      "Episode: 54 Total reward: 95.0 Training loss: 1.5719 Explore P: 0.2533\n",
      "Episode: 55 Total reward: 94.0 Training loss: 1.0316 Explore P: 0.2529\n",
      "Model saved.\n",
      "Episode: 56 Total reward: -239.0 Training loss: 0.8361 Explore P: 0.2368\n",
      "Episode: 57 Total reward: -370.0 Training loss: 0.8051 Explore P: 0.2207\n",
      "Episode: 58 Total reward: 95.0 Training loss: 1.8996 Explore P: 0.2204\n",
      "Episode: 59 Total reward: -360.0 Training loss: 1.0055 Explore P: 0.2058\n",
      "Episode: 60 Total reward: 95.0 Training loss: 0.8008 Explore P: 0.2055\n",
      "Model saved.\n",
      "Episode: 61 Total reward: 95.0 Training loss: 0.9601 Explore P: 0.2052\n",
      "Episode: 62 Total reward: 95.0 Training loss: 1.2390 Explore P: 0.2049\n",
      "Episode: 63 Total reward: -95.0 Training loss: 1.4073 Explore P: 0.1976\n",
      "Episode: 64 Total reward: -365.0 Training loss: 0.8136 Explore P: 0.1849\n",
      "Episode: 65 Total reward: 95.0 Training loss: 1.0307 Explore P: 0.1847\n",
      "Model saved.\n",
      "Episode: 66 Total reward: 88.0 Training loss: 3.4918 Explore P: 0.1842\n",
      "Episode: 67 Total reward: 15.0 Training loss: 0.9213 Explore P: 0.1813\n",
      "Episode: 68 Total reward: 63.0 Training loss: 0.9735 Explore P: 0.1800\n",
      "Episode: 69 Total reward: 95.0 Training loss: 1.3898 Explore P: 0.1798\n",
      "Episode: 70 Total reward: 95.0 Training loss: 2.6889 Explore P: 0.1796\n",
      "Model saved.\n",
      "Episode: 71 Total reward: 95.0 Training loss: 1.4797 Explore P: 0.1793\n",
      "Episode: 72 Total reward: -365.0 Training loss: 0.8281 Explore P: 0.1682\n",
      "Episode: 73 Total reward: 95.0 Training loss: 0.6767 Explore P: 0.1680\n",
      "Episode: 74 Total reward: 94.0 Training loss: 0.8691 Explore P: 0.1677\n",
      "Episode: 75 Total reward: -370.0 Training loss: 1.4082 Explore P: 0.1576\n",
      "Model saved.\n",
      "Episode: 76 Total reward: -360.0 Training loss: 2.5946 Explore P: 0.1483\n",
      "Episode: 77 Total reward: 95.0 Training loss: 1.1761 Explore P: 0.1482\n",
      "Episode: 78 Total reward: 95.0 Training loss: 0.9692 Explore P: 0.1480\n",
      "Episode: 79 Total reward: 95.0 Training loss: 1.0988 Explore P: 0.1478\n",
      "Episode: 80 Total reward: -370.0 Training loss: 1.2592 Explore P: 0.1394\n",
      "Model saved.\n",
      "Episode: 81 Total reward: -360.0 Training loss: 0.5571 Explore P: 0.1317\n",
      "Episode: 82 Total reward: 95.0 Training loss: 1.0882 Explore P: 0.1316\n",
      "Episode: 83 Total reward: 14.0 Training loss: 0.8672 Explore P: 0.1298\n",
      "Episode: 84 Total reward: 95.0 Training loss: 1.2108 Explore P: 0.1297\n",
      "Episode: 85 Total reward: -365.0 Training loss: 1.3713 Explore P: 0.1228\n",
      "Model saved.\n",
      "Episode: 86 Total reward: -68.0 Training loss: 1.1369 Explore P: 0.1198\n",
      "Episode: 87 Total reward: 95.0 Training loss: 1.3616 Explore P: 0.1197\n",
      "Episode: 88 Total reward: -81.0 Training loss: 1.2471 Explore P: 0.1165\n",
      "Episode: 89 Total reward: 95.0 Training loss: 1.9147 Explore P: 0.1164\n",
      "Episode: 90 Total reward: -365.0 Training loss: 1.3722 Explore P: 0.1107\n",
      "Model saved.\n",
      "Episode: 91 Total reward: 95.0 Training loss: 1.0895 Explore P: 0.1106\n",
      "Episode: 92 Total reward: 95.0 Training loss: 1.0240 Explore P: 0.1105\n",
      "Episode: 93 Total reward: -345.0 Training loss: 0.7710 Explore P: 0.1052\n",
      "Episode: 94 Total reward: 94.0 Training loss: 1.0999 Explore P: 0.1051\n",
      "Episode: 95 Total reward: -8.0 Training loss: 1.8255 Explore P: 0.1036\n",
      "Model saved.\n",
      "Episode: 96 Total reward: 95.0 Training loss: 2.4501 Explore P: 0.1035\n",
      "Episode: 97 Total reward: 95.0 Training loss: 1.7249 Explore P: 0.1034\n",
      "Episode: 98 Total reward: 27.0 Training loss: 1.0158 Explore P: 0.1024\n",
      "Episode: 99 Total reward: 95.0 Training loss: 1.7822 Explore P: 0.1023\n",
      "Episode: 100 Total reward: 95.0 Training loss: 6.3768 Explore P: 0.1022\n",
      "Model saved.\n",
      "Episode: 101 Total reward: 95.0 Training loss: 2.1773 Explore P: 0.1021\n",
      "Episode: 102 Total reward: 40.0 Training loss: 0.9791 Explore P: 0.1013\n",
      "Episode: 103 Total reward: 45.0 Training loss: 1.7185 Explore P: 0.1006\n",
      "Episode: 104 Total reward: 95.0 Training loss: 1.4283 Explore P: 0.1005\n",
      "Episode: 105 Total reward: 95.0 Training loss: 1.7787 Explore P: 0.1004\n",
      "Model saved.\n",
      "Episode: 106 Total reward: 95.0 Training loss: 1.3661 Explore P: 0.1003\n",
      "Episode: 107 Total reward: 95.0 Training loss: 27.3518 Explore P: 0.1003\n",
      "Episode: 108 Total reward: 93.0 Training loss: 1.3497 Explore P: 0.1001\n",
      "Episode: 109 Total reward: 95.0 Training loss: 1.3728 Explore P: 0.1000\n",
      "Episode: 110 Total reward: 3.0 Training loss: 1.1527 Explore P: 0.0988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n",
      "Episode: 111 Total reward: 92.0 Training loss: 1.3529 Explore P: 0.0987\n",
      "Episode: 112 Total reward: 45.0 Training loss: 1.6412 Explore P: 0.0980\n",
      "Episode: 113 Total reward: 95.0 Training loss: 0.9558 Explore P: 0.0979\n",
      "Episode: 114 Total reward: 50.0 Training loss: 4.5348 Explore P: 0.0973\n",
      "Episode: 115 Total reward: 95.0 Training loss: 1.3354 Explore P: 0.0973\n",
      "Model saved.\n",
      "Episode: 116 Total reward: 12.0 Training loss: 1.0225 Explore P: 0.0962\n",
      "Episode: 117 Total reward: 95.0 Training loss: 2.4799 Explore P: 0.0961\n",
      "Episode: 118 Total reward: 95.0 Training loss: 2.8702 Explore P: 0.0961\n",
      "Episode: 119 Total reward: 77.0 Training loss: 8.5631 Explore P: 0.0957\n",
      "Episode: 120 Total reward: 71.0 Training loss: 1.8125 Explore P: 0.0954\n",
      "Model saved.\n",
      "Episode: 121 Total reward: 95.0 Training loss: 3.4829 Explore P: 0.0953\n",
      "Episode: 122 Total reward: 95.0 Training loss: 1.2618 Explore P: 0.0952\n",
      "Episode: 123 Total reward: 95.0 Training loss: 2.3382 Explore P: 0.0951\n",
      "Episode: 124 Total reward: -360.0 Training loss: 1.6574 Explore P: 0.0913\n",
      "Episode: 125 Total reward: 95.0 Training loss: 1.2671 Explore P: 0.0912\n",
      "Model saved.\n",
      "Episode: 126 Total reward: 95.0 Training loss: 1.1619 Explore P: 0.0911\n",
      "Episode: 127 Total reward: 95.0 Training loss: 1.8090 Explore P: 0.0910\n",
      "Episode: 128 Total reward: 95.0 Training loss: 1.5296 Explore P: 0.0910\n",
      "Episode: 129 Total reward: 95.0 Training loss: 1.2102 Explore P: 0.0909\n",
      "Episode: 130 Total reward: 47.0 Training loss: 1.2486 Explore P: 0.0903\n",
      "Model saved.\n",
      "Episode: 131 Total reward: 95.0 Training loss: 2.8528 Explore P: 0.0903\n",
      "Episode: 132 Total reward: 76.0 Training loss: 1.4517 Explore P: 0.0900\n",
      "Episode: 133 Total reward: 95.0 Training loss: 1.2703 Explore P: 0.0899\n",
      "Episode: 134 Total reward: 94.0 Training loss: 2.3021 Explore P: 0.0898\n",
      "Episode: 135 Total reward: 95.0 Training loss: 1.2563 Explore P: 0.0897\n",
      "Model saved.\n",
      "Episode: 136 Total reward: 95.0 Training loss: 1.7378 Explore P: 0.0897\n",
      "Episode: 137 Total reward: -99.0 Training loss: 2.0528 Explore P: 0.0878\n",
      "Episode: 138 Total reward: 95.0 Training loss: 2.0415 Explore P: 0.0877\n",
      "Episode: 139 Total reward: 95.0 Training loss: 1.4460 Explore P: 0.0876\n",
      "Episode: 140 Total reward: 71.0 Training loss: 1.9826 Explore P: 0.0873\n",
      "Model saved.\n",
      "Episode: 141 Total reward: 95.0 Training loss: 1.5004 Explore P: 0.0873\n",
      "Episode: 142 Total reward: 95.0 Training loss: 1.7367 Explore P: 0.0872\n",
      "Episode: 143 Total reward: -375.0 Training loss: 2.6740 Explore P: 0.0840\n",
      "Episode: 144 Total reward: 95.0 Training loss: 1.2761 Explore P: 0.0839\n",
      "Episode: 145 Total reward: 95.0 Training loss: 1.4764 Explore P: 0.0839\n",
      "Model saved.\n",
      "Episode: 146 Total reward: 95.0 Training loss: 1.2927 Explore P: 0.0838\n",
      "Episode: 147 Total reward: 95.0 Training loss: 1.2439 Explore P: 0.0838\n",
      "Episode: 148 Total reward: 95.0 Training loss: 1.5231 Explore P: 0.0837\n",
      "Episode: 149 Total reward: 95.0 Training loss: 1.6896 Explore P: 0.0836\n",
      "Episode: 150 Total reward: 95.0 Training loss: 1.3312 Explore P: 0.0836\n",
      "Model saved.\n",
      "Episode: 151 Total reward: 65.0 Training loss: 1.1408 Explore P: 0.0833\n",
      "Episode: 152 Total reward: 93.0 Training loss: 1.8136 Explore P: 0.0832\n",
      "Episode: 153 Total reward: 95.0 Training loss: 1.6216 Explore P: 0.0831\n",
      "Episode: 154 Total reward: 32.0 Training loss: 3.4883 Explore P: 0.0825\n",
      "Episode: 155 Total reward: 94.0 Training loss: 1.9115 Explore P: 0.0825\n",
      "Model saved.\n",
      "Episode: 156 Total reward: 95.0 Training loss: 5.5657 Explore P: 0.0824\n",
      "Episode: 157 Total reward: -360.0 Training loss: 1.3602 Explore P: 0.0796\n",
      "Episode: 158 Total reward: 94.0 Training loss: 1.9616 Explore P: 0.0796\n",
      "Episode: 159 Total reward: -360.0 Training loss: 1.6445 Explore P: 0.0770\n",
      "Episode: 160 Total reward: -360.0 Training loss: 2.0499 Explore P: 0.0747\n",
      "Model saved.\n",
      "Episode: 161 Total reward: 71.0 Training loss: 1.4114 Explore P: 0.0745\n",
      "Episode: 162 Total reward: 95.0 Training loss: 0.9853 Explore P: 0.0745\n",
      "Episode: 163 Total reward: 95.0 Training loss: 1.4868 Explore P: 0.0744\n",
      "Episode: 164 Total reward: 95.0 Training loss: 1.7403 Explore P: 0.0744\n",
      "Episode: 165 Total reward: 95.0 Training loss: 1.0197 Explore P: 0.0743\n",
      "Model saved.\n",
      "Episode: 166 Total reward: 95.0 Training loss: 3.1043 Explore P: 0.0743\n",
      "Episode: 167 Total reward: 95.0 Training loss: 0.8366 Explore P: 0.0742\n",
      "Episode: 168 Total reward: 95.0 Training loss: 1.4118 Explore P: 0.0742\n",
      "Episode: 169 Total reward: 93.0 Training loss: 1.2716 Explore P: 0.0741\n",
      "Episode: 170 Total reward: 75.0 Training loss: 2.9692 Explore P: 0.0740\n",
      "Model saved.\n",
      "Episode: 171 Total reward: 75.0 Training loss: 6.8626 Explore P: 0.0738\n",
      "Episode: 172 Total reward: 95.0 Training loss: 2.0306 Explore P: 0.0738\n",
      "Episode: 173 Total reward: 43.0 Training loss: 2.2165 Explore P: 0.0735\n",
      "Episode: 174 Total reward: 95.0 Training loss: 13.2733 Explore P: 0.0734\n",
      "Episode: 175 Total reward: 95.0 Training loss: 3.6842 Explore P: 0.0734\n",
      "Model saved.\n",
      "Episode: 176 Total reward: 71.0 Training loss: 2.4414 Explore P: 0.0732\n",
      "Episode: 177 Total reward: 95.0 Training loss: 2.0252 Explore P: 0.0732\n",
      "Episode: 178 Total reward: 95.0 Training loss: 1.3799 Explore P: 0.0731\n",
      "Episode: 179 Total reward: 94.0 Training loss: 9.2205 Explore P: 0.0731\n",
      "Episode: 180 Total reward: 28.0 Training loss: 3.9616 Explore P: 0.0726\n",
      "Model saved.\n",
      "Episode: 181 Total reward: 95.0 Training loss: 2.3615 Explore P: 0.0726\n",
      "Episode: 182 Total reward: 95.0 Training loss: 1.6009 Explore P: 0.0726\n",
      "Episode: 183 Total reward: -365.0 Training loss: 1.3050 Explore P: 0.0706\n",
      "Episode: 184 Total reward: 69.0 Training loss: 2.2640 Explore P: 0.0704\n",
      "Episode: 185 Total reward: 73.0 Training loss: 2.4630 Explore P: 0.0703\n",
      "Model saved.\n",
      "Episode: 186 Total reward: 95.0 Training loss: 3.4463 Explore P: 0.0703\n",
      "Episode: 187 Total reward: 52.0 Training loss: 1.0624 Explore P: 0.0700\n",
      "Episode: 188 Total reward: 95.0 Training loss: 3.1144 Explore P: 0.0700\n",
      "Episode: 189 Total reward: 95.0 Training loss: 1.3748 Explore P: 0.0700\n",
      "Episode: 190 Total reward: -365.0 Training loss: 2.4625 Explore P: 0.0682\n",
      "Model saved.\n",
      "Episode: 191 Total reward: 95.0 Training loss: 1.3739 Explore P: 0.0682\n",
      "Episode: 192 Total reward: -360.0 Training loss: 1.1588 Explore P: 0.0666\n",
      "Episode: 193 Total reward: 95.0 Training loss: 1.8302 Explore P: 0.0666\n",
      "Episode: 194 Total reward: 95.0 Training loss: 1.8731 Explore P: 0.0666\n",
      "Episode: 195 Total reward: 95.0 Training loss: 1.5590 Explore P: 0.0666\n",
      "Model saved.\n",
      "Episode: 196 Total reward: 95.0 Training loss: 1.7451 Explore P: 0.0665\n",
      "Episode: 197 Total reward: 95.0 Training loss: 1.4858 Explore P: 0.0665\n",
      "Episode: 198 Total reward: 75.0 Training loss: 1.3433 Explore P: 0.0664\n",
      "Episode: 199 Total reward: 63.0 Training loss: 1.2594 Explore P: 0.0662\n",
      "Episode: 200 Total reward: 95.0 Training loss: 1.9859 Explore P: 0.0662\n",
      "Model saved.\n",
      "Episode: 201 Total reward: 62.0 Training loss: 1.4455 Explore P: 0.0660\n",
      "Episode: 202 Total reward: -365.0 Training loss: 1.2266 Explore P: 0.0647\n",
      "Episode: 203 Total reward: 95.0 Training loss: 1.0389 Explore P: 0.0646\n",
      "Episode: 204 Total reward: 95.0 Training loss: 1.8646 Explore P: 0.0646\n",
      "Episode: 205 Total reward: 95.0 Training loss: 1.1554 Explore P: 0.0646\n",
      "Model saved.\n",
      "Episode: 206 Total reward: 95.0 Training loss: 1.5359 Explore P: 0.0646\n",
      "Episode: 207 Total reward: 95.0 Training loss: 1.6176 Explore P: 0.0645\n",
      "Episode: 208 Total reward: 81.0 Training loss: 1.1872 Explore P: 0.0644\n",
      "Episode: 209 Total reward: 52.0 Training loss: 1.5427 Explore P: 0.0643\n",
      "Episode: 210 Total reward: 95.0 Training loss: 2.4417 Explore P: 0.0642\n",
      "Model saved.\n",
      "Episode: 211 Total reward: 75.0 Training loss: 1.0092 Explore P: 0.0642\n",
      "Episode: 212 Total reward: 95.0 Training loss: 1.3497 Explore P: 0.0641\n",
      "Episode: 213 Total reward: 18.0 Training loss: 1.8451 Explore P: 0.0638\n",
      "Episode: 214 Total reward: 95.0 Training loss: 2.0694 Explore P: 0.0638\n",
      "Episode: 215 Total reward: 95.0 Training loss: 1.8506 Explore P: 0.0638\n",
      "Model saved.\n",
      "Episode: 216 Total reward: 43.0 Training loss: 1.9153 Explore P: 0.0636\n",
      "Episode: 217 Total reward: 95.0 Training loss: 2.5466 Explore P: 0.0636\n",
      "Episode: 218 Total reward: 57.0 Training loss: 1.3064 Explore P: 0.0634\n",
      "Episode: 219 Total reward: 53.0 Training loss: 2.1874 Explore P: 0.0633\n",
      "Episode: 220 Total reward: 69.0 Training loss: 1.5936 Explore P: 0.0632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n",
      "Episode: 221 Total reward: 75.0 Training loss: 1.4695 Explore P: 0.0631\n",
      "Episode: 222 Total reward: 95.0 Training loss: 1.6176 Explore P: 0.0631\n",
      "Episode: 223 Total reward: -360.0 Training loss: 1.4097 Explore P: 0.0619\n",
      "Episode: 224 Total reward: 95.0 Training loss: 1.7599 Explore P: 0.0619\n",
      "Episode: 225 Total reward: 95.0 Training loss: 2.6157 Explore P: 0.0619\n",
      "Model saved.\n",
      "Episode: 226 Total reward: 95.0 Training loss: 1.4575 Explore P: 0.0619\n",
      "Episode: 227 Total reward: 74.0 Training loss: 1.2386 Explore P: 0.0618\n",
      "Episode: 228 Total reward: -370.0 Training loss: 1.2018 Explore P: 0.0608\n",
      "Episode: 229 Total reward: 95.0 Training loss: 1.0937 Explore P: 0.0608\n",
      "Episode: 230 Total reward: 83.0 Training loss: 0.8265 Explore P: 0.0607\n",
      "Model saved.\n",
      "Episode: 231 Total reward: 64.0 Training loss: 1.4378 Explore P: 0.0606\n",
      "Episode: 232 Total reward: 79.0 Training loss: 2.2027 Explore P: 0.0605\n",
      "Episode: 233 Total reward: 95.0 Training loss: 2.4747 Explore P: 0.0605\n",
      "Episode: 234 Total reward: 73.0 Training loss: 1.5348 Explore P: 0.0604\n",
      "Episode: 235 Total reward: 95.0 Training loss: 2.5043 Explore P: 0.0604\n",
      "Model saved.\n",
      "Episode: 236 Total reward: 66.0 Training loss: 17.4144 Explore P: 0.0603\n",
      "Episode: 237 Total reward: 59.0 Training loss: 1.2919 Explore P: 0.0602\n",
      "Episode: 238 Total reward: 95.0 Training loss: 1.3966 Explore P: 0.0602\n",
      "Episode: 239 Total reward: 57.0 Training loss: 0.6129 Explore P: 0.0601\n",
      "Episode: 240 Total reward: 56.0 Training loss: 5.9440 Explore P: 0.0599\n",
      "Model saved.\n",
      "Episode: 241 Total reward: 52.0 Training loss: 1.0885 Explore P: 0.0598\n",
      "Episode: 242 Total reward: -355.0 Training loss: 1.8151 Explore P: 0.0590\n",
      "Episode: 243 Total reward: 48.0 Training loss: 4.3732 Explore P: 0.0589\n",
      "Episode: 244 Total reward: 95.0 Training loss: 1.5463 Explore P: 0.0588\n",
      "Episode: 245 Total reward: 79.0 Training loss: 1.2593 Explore P: 0.0588\n",
      "Model saved.\n",
      "Episode: 246 Total reward: 68.0 Training loss: 5.2368 Explore P: 0.0587\n",
      "Episode: 247 Total reward: 95.0 Training loss: 1.7749 Explore P: 0.0587\n",
      "Episode: 248 Total reward: 95.0 Training loss: 1.5265 Explore P: 0.0587\n",
      "Episode: 249 Total reward: 95.0 Training loss: 1.9258 Explore P: 0.0587\n",
      "Episode: 250 Total reward: 52.0 Training loss: 3.0160 Explore P: 0.0585\n",
      "Model saved.\n",
      "Episode: 251 Total reward: 82.0 Training loss: 1.6330 Explore P: 0.0585\n",
      "Episode: 252 Total reward: 95.0 Training loss: 1.2156 Explore P: 0.0585\n",
      "Episode: 253 Total reward: 95.0 Training loss: 2.1215 Explore P: 0.0585\n",
      "Episode: 254 Total reward: 95.0 Training loss: 1.6125 Explore P: 0.0585\n",
      "Episode: 255 Total reward: 95.0 Training loss: 2.3728 Explore P: 0.0584\n",
      "Model saved.\n"
     ]
    },
    {
     "ename": "ViZDoomUnexpectedExitException",
     "evalue": "Controlled ViZDoom instance exited unexpectedly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-5b3156351484>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplore_probability\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexplore_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplore_stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossible_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                 \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_episode_finished\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m: Controlled ViZDoom instance exited unexpectedly."
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        #init the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        #init the decay rate that reduces the epsilon\n",
    "        decay_step = 0\n",
    "        \n",
    "        game.init()\n",
    "        for episode in range(total_episodes):\n",
    "            step = 0\n",
    "            \n",
    "            episode_rewards = []\n",
    "            \n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "            while step < max_steps:\n",
    "                step+=1                \n",
    "                decay_step +=1\n",
    "                \n",
    "                #predict the action to take and take it\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "                \n",
    "                reward = game.make_action(action)\n",
    "                done = game.is_episode_finished()\n",
    "                \n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                if done:\n",
    "                    next_state = np.zeros((84, 84), dtype = np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "                    step = max_steps\n",
    "                    \n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "                    \n",
    "                    print('Episode: {}'.format(episode), 'Total reward: {}'.format(total_reward), 'Training loss: {:.4f}'.format(loss),\n",
    "                                                       'Explore P: {:.4f}'.format(explore_probability))\n",
    "                    \n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                else:\n",
    "                    next_state = game.get_state().screen_buffer                    \n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames,next_state, False)\n",
    "                    \n",
    "                    memory.add((state, action, reward, next_state, done))                   \n",
    "                    state = next_state\n",
    "                \n",
    "                #learning part\n",
    "                #obtain random minibatch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin = 3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch])\n",
    "                next_states_mb = np.array([each[3] for each in batch])\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "                \n",
    "                target_Qs_batch = []\n",
    "                \n",
    "                Qs_next_state_vals = sess.run(DeepQNetwork.output, feed_dict = {DeepQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                #set q_target = r if the episode ends at s+1, otherwise q_target = r + gamma*maxQ(s', a')\n",
    "                \n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "                    \n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma*np.max(Qs_next_state_vals[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "                \n",
    "                loss, _ = sess.run([DeepQNetwork.loss, DeepQNetwork.optimizer],\n",
    "                                  feed_dict = {DeepQNetwork.inputs_: states_mb,\n",
    "                                              DeepQNetwork.target_Q: targets_mb, \n",
    "                                              DeepQNetwork.actions_: actions_mb})\n",
    "                \n",
    "                #write tensorflow summaries\n",
    "                summary = sess.run(write_op, feed_dict = {DeepQNetwork.inputs_: states_mb,\n",
    "                                                         DeepQNetwork.target_Q: targets_mb,\n",
    "                                                         DeepQNetwork.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model saved.\")              \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    game, possible_actions = ec.create_environment()\n",
    "    \n",
    "    totalScore = 0\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    for i in range(1):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        game.new_episode()\n",
    "        \n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "        while not game.is_episode_finished():\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            Qs = sess.run(DeepQNetwork.output, feed_dict = {DeepQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "            \n",
    "            # Take the biggest Q value (= the best action)\n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            score = game.get_total_reward()\n",
    "            \n",
    "            if done:\n",
    "                break  \n",
    "                \n",
    "            else:\n",
    "                print(\"else\")\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "                \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
