{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "import environment_creation as ec\n",
    "import frame_preprocessing as fp\n",
    "import frames_stacking as fs\n",
    "import memory as mem\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = ec.create_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frame preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack_frames\n",
    "stack_size = 4\n",
    "stacked_frames = deque([np.zeros((84, 84), dtype = np.int) for i in range(stack_size)], maxlen = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model hyperparameters\n",
    "state_size = [84, 84, 4]\n",
    "action_size = game.get_available_buttons_size()\n",
    "learning_rate = 0.0002\n",
    "\n",
    "#training hyperparameters\n",
    "total_episodes = 500\n",
    "max_steps = 100\n",
    "batch_size = 64\n",
    "\n",
    "#exploration/epsilon greedy strategy parameters\n",
    "explore_start = 1.0\n",
    "explore_stop = 0.01\n",
    "decay_rate = 0.0001\n",
    "\n",
    "#q leraning hyperparameters\n",
    "gamma = 0.95 #discount rate\n",
    "\n",
    "###memory hyperparameters\n",
    "pretrain_length = batch_size #number of experiences stored in memory when initialized for the first time\n",
    "memory_size = 100000 #no of experiences memory can keep\n",
    "\n",
    "training = True\n",
    "episode_render = False\n",
    "\n",
    "stacked_frames = deque([np.zeros((84, 84), dtype = np.int) for i in range(stack_size)], maxlen = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name = \"DeepQNetwork\"):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            #here we create the placeholders\n",
    "            \n",
    "            #[None, 84, 84, 4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, 3], name = \"actions_\")\n",
    "        \n",
    "            #target_Q is the R(s, a) + ymax Q^(s', a')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name = \"targetQ\")\n",
    "            \n",
    "            # Input is 84x84x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                         filters = 32,\n",
    "                                         kernel_size = [8,8],\n",
    "                                         strides = [4,4],\n",
    "                                         padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv1\")\n",
    "            \n",
    "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm1')\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
    "            ## --> [20, 20, 32]\n",
    "            \n",
    "            #second\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv2\")\n",
    "        \n",
    "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm2')\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "            ## --> [9, 9, 64]\n",
    "            \n",
    "            #thirds\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv3\")\n",
    "        \n",
    "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm3')\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "            ## --> [3, 3, 128]\n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            ## --> [1152]\n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                      units = 512,\n",
    "                                      activation = tf.nn.elu,\n",
    "                                      kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                      name = 'fc1')\n",
    "            \n",
    "            self.output = tf.layers.dense(inputs = self.fc, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units = 3, \n",
    "                                        activation=None)\n",
    "            \n",
    "            #Q is our predicted Q value\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis = 1)\n",
    "            \n",
    "            #loss is the difference between predicted Q value and Q_target\n",
    "            # sum(q_target - q)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "DeepQNetwork = DeepQNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = fp.preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the memory class\n",
    "#instantiate the memory\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "#now dealing with the empty memory problem - prepopulating memory by taking random actions\n",
    "#and storing experience (state, action, reward, new_state)\n",
    "\n",
    "game.new_episode()\n",
    "for i in range(pretrain_length):\n",
    "    #if it's the first step\n",
    "    if i==0:\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    #take random action\n",
    "    action = random.choice(possible_actions)\n",
    "   \n",
    "    #get rewardd\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    #look if episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        #add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        #start a new episode\n",
    "        game.new_episode()\n",
    "        \n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames =  stack_frames(stacked_frames, state, True)\n",
    "    else:\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        #add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        #our state is now the next state\n",
    "        state = next_state    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup tensorboard writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
    "\n",
    "tf.summary.scalar(\"Loss\", DeepQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init the weights\n",
    "#init the environment\n",
    "#init decay rate\n",
    "\n",
    "#the following function does the epsilon-greedy strategy part - predicts action\n",
    "def predict_action(explore_start, explore_top, decay_rate, decay_step, state, actions):\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "    \n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if(explore_probability > exp_exp_tradeoff):\n",
    "        action = random.choice(possible_actions)\n",
    "    else:\n",
    "        #get action from q-network (exploitation)\n",
    "        Qs = sess.run(DeepQNetwork.output, feed_dict = {DeepQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        #take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "    \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n",
      "Episode: 2 Total reward: 94.0 Training loss: 5.3815 Explore P: 1.0000\n",
      "Episode: 4 Total reward: 90.0 Training loss: 123.9396 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 6 Total reward: 89.0 Training loss: 6.9530 Explore P: 1.0000\n",
      "Episode: 7 Total reward: 95.0 Training loss: 17.1589 Explore P: 1.0000\n",
      "Episode: 8 Total reward: 75.0 Training loss: 22.6181 Explore P: 1.0000\n",
      "Episode: 10 Total reward: 95.0 Training loss: 16.6917 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 11 Total reward: 94.0 Training loss: 6.1417 Explore P: 1.0000\n",
      "Episode: 13 Total reward: 92.0 Training loss: 8.4667 Explore P: 1.0000\n",
      "Episode: 15 Total reward: 95.0 Training loss: 7.5991 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 17 Total reward: -23.0 Training loss: 5.8741 Explore P: 1.0000\n",
      "Episode: 19 Total reward: -16.0 Training loss: 6.7965 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 21 Total reward: 71.0 Training loss: 9.0128 Explore P: 1.0000\n",
      "Episode: 22 Total reward: 92.0 Training loss: 5.6467 Explore P: 1.0000\n",
      "Episode: 23 Total reward: 93.0 Training loss: 6.7464 Explore P: 1.0000\n",
      "Episode: 24 Total reward: 94.0 Training loss: 2.0535 Explore P: 1.0000\n",
      "Episode: 25 Total reward: -20.0 Training loss: 6.3312 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 26 Total reward: 88.0 Training loss: 6.4202 Explore P: 1.0000\n",
      "Episode: 27 Total reward: 90.0 Training loss: 5.0402 Explore P: 1.0000\n",
      "Episode: 28 Total reward: 95.0 Training loss: 12.9764 Explore P: 1.0000\n",
      "Episode: 29 Total reward: 95.0 Training loss: 2.2655 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 31 Total reward: 95.0 Training loss: 7.2346 Explore P: 1.0000\n",
      "Episode: 32 Total reward: 95.0 Training loss: 3.7127 Explore P: 1.0000\n",
      "Episode: 33 Total reward: 73.0 Training loss: 15.7076 Explore P: 1.0000\n",
      "Episode: 35 Total reward: -7.0 Training loss: 16.7472 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 37 Total reward: 95.0 Training loss: 3.6371 Explore P: 1.0000\n",
      "Episode: 38 Total reward: 85.0 Training loss: 4.7058 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 41 Total reward: 41.0 Training loss: 5.8983 Explore P: 1.0000\n",
      "Episode: 44 Total reward: 95.0 Training loss: 8.2693 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 47 Total reward: 95.0 Training loss: 3.3400 Explore P: 1.0000\n",
      "Episode: 49 Total reward: 95.0 Training loss: 2.4271 Explore P: 1.0000\n",
      "Episode: 50 Total reward: 92.0 Training loss: 2.1117 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 52 Total reward: 93.0 Training loss: 1.7258 Explore P: 1.0000\n",
      "Episode: 53 Total reward: 93.0 Training loss: 6.0011 Explore P: 1.0000\n",
      "Episode: 55 Total reward: 92.0 Training loss: 3.6080 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 58 Total reward: 71.0 Training loss: 5.5613 Explore P: 1.0000\n",
      "Episode: 59 Total reward: 69.0 Training loss: 3.3877 Explore P: 1.0000\n",
      "Episode: 60 Total reward: 95.0 Training loss: 4.8755 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 61 Total reward: 91.0 Training loss: 58.9995 Explore P: 1.0000\n",
      "Episode: 62 Total reward: 94.0 Training loss: 5.7409 Explore P: 1.0000\n",
      "Episode: 63 Total reward: 95.0 Training loss: 3.8960 Explore P: 1.0000\n",
      "Episode: 64 Total reward: 92.0 Training loss: 14.3581 Explore P: 1.0000\n",
      "Episode: 65 Total reward: 8.0 Training loss: 14.0941 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 66 Total reward: 95.0 Training loss: 23.7858 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 71 Total reward: 94.0 Training loss: 11.4520 Explore P: 1.0000\n",
      "Episode: 74 Total reward: 95.0 Training loss: 7.0962 Explore P: 1.0000\n",
      "Episode: 75 Total reward: 70.0 Training loss: 3.7014 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 76 Total reward: 93.0 Training loss: 2.9380 Explore P: 1.0000\n",
      "Episode: 78 Total reward: 76.0 Training loss: 5.0870 Explore P: 1.0000\n",
      "Episode: 79 Total reward: 90.0 Training loss: 4.7701 Explore P: 1.0000\n",
      "Episode: 80 Total reward: 67.0 Training loss: 6.1688 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 81 Total reward: 93.0 Training loss: 6.6240 Explore P: 1.0000\n",
      "Episode: 82 Total reward: 95.0 Training loss: 7.7527 Explore P: 1.0000\n",
      "Episode: 84 Total reward: 34.0 Training loss: 4.0840 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 88 Total reward: 94.0 Training loss: 3.9067 Explore P: 1.0000\n",
      "Episode: 90 Total reward: 94.0 Training loss: 3.3611 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 91 Total reward: 43.0 Training loss: 3.8502 Explore P: 1.0000\n",
      "Episode: 92 Total reward: 95.0 Training loss: 4.6564 Explore P: 1.0000\n",
      "Episode: 93 Total reward: 95.0 Training loss: 4.0444 Explore P: 1.0000\n",
      "Episode: 94 Total reward: -4.0 Training loss: 2.7283 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 96 Total reward: 22.0 Training loss: 6.9853 Explore P: 1.0000\n",
      "Episode: 97 Total reward: 95.0 Training loss: 8.1573 Explore P: 1.0000\n",
      "Episode: 98 Total reward: 93.0 Training loss: 7.6764 Explore P: 1.0000\n",
      "Episode: 99 Total reward: 91.0 Training loss: 4.9136 Explore P: 1.0000\n",
      "Episode: 100 Total reward: -5.0 Training loss: 4.8837 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 102 Total reward: 95.0 Training loss: 4.8398 Explore P: 1.0000\n",
      "Episode: 103 Total reward: 93.0 Training loss: 4.3980 Explore P: 1.0000\n",
      "Episode: 104 Total reward: 40.0 Training loss: 8.8908 Explore P: 1.0000\n",
      "Episode: 105 Total reward: 95.0 Training loss: 5.8612 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 106 Total reward: 74.0 Training loss: 9.0428 Explore P: 1.0000\n",
      "Episode: 107 Total reward: 95.0 Training loss: 4.3235 Explore P: 1.0000\n",
      "Episode: 108 Total reward: 93.0 Training loss: 7.0443 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 111 Total reward: 55.0 Training loss: 4.1127 Explore P: 1.0000\n",
      "Episode: 112 Total reward: 89.0 Training loss: 6.5506 Explore P: 1.0000\n",
      "Episode: 113 Total reward: 95.0 Training loss: 14.4102 Explore P: 1.0000\n",
      "Episode: 114 Total reward: 2.0 Training loss: 3.3498 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 118 Total reward: 95.0 Training loss: 16.3929 Explore P: 1.0000\n",
      "Episode: 120 Total reward: 93.0 Training loss: 7.3150 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 123 Total reward: 95.0 Training loss: 11.0663 Explore P: 1.0000\n",
      "Episode: 125 Total reward: 95.0 Training loss: 6.5741 Explore P: 1.0000\n",
      "Model saved.\n",
      "Model saved.\n",
      "Episode: 131 Total reward: 93.0 Training loss: 8.2384 Explore P: 1.0000\n",
      "Episode: 132 Total reward: 17.0 Training loss: 2.9382 Explore P: 1.0000\n",
      "Episode: 134 Total reward: 93.0 Training loss: 7.8686 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 136 Total reward: 76.0 Training loss: 4.5205 Explore P: 1.0000\n",
      "Episode: 137 Total reward: 95.0 Training loss: 5.7922 Explore P: 1.0000\n",
      "Episode: 138 Total reward: 91.0 Training loss: 7.8126 Explore P: 1.0000\n",
      "Episode: 139 Total reward: 95.0 Training loss: 3.0376 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 142 Total reward: 91.0 Training loss: 11.7153 Explore P: 1.0000\n",
      "Episode: 144 Total reward: 50.0 Training loss: 6.8145 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 146 Total reward: 65.0 Training loss: 2.6912 Explore P: 1.0000\n",
      "Episode: 147 Total reward: 94.0 Training loss: 2.1350 Explore P: 1.0000\n",
      "Episode: 148 Total reward: 95.0 Training loss: 5.2638 Explore P: 1.0000\n",
      "Episode: 150 Total reward: 94.0 Training loss: 7.9805 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 152 Total reward: 93.0 Training loss: 76.4106 Explore P: 1.0000\n",
      "Episode: 153 Total reward: 89.0 Training loss: 3.7028 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 156 Total reward: 93.0 Training loss: 3.1180 Explore P: 1.0000\n",
      "Episode: 157 Total reward: 63.0 Training loss: 3.3810 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 162 Total reward: 95.0 Training loss: 8.1043 Explore P: 1.0000\n",
      "Episode: 165 Total reward: 29.0 Training loss: 3.2528 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 166 Total reward: 91.0 Training loss: 6.3072 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 171 Total reward: 95.0 Training loss: 4.2906 Explore P: 1.0000\n",
      "Episode: 172 Total reward: 95.0 Training loss: 4.5691 Explore P: 1.0000\n",
      "Episode: 173 Total reward: 94.0 Training loss: 3.5984 Explore P: 1.0000\n",
      "Episode: 175 Total reward: 90.0 Training loss: 4.9504 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 179 Total reward: 94.0 Training loss: 5.0394 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 181 Total reward: 95.0 Training loss: 4.1763 Explore P: 1.0000\n",
      "Episode: 182 Total reward: -9.0 Training loss: 7.6559 Explore P: 1.0000\n",
      "Episode: 183 Total reward: 76.0 Training loss: 7.0094 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 187 Total reward: 94.0 Training loss: 4.9066 Explore P: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 188 Total reward: 90.0 Training loss: 6.1995 Explore P: 1.0000\n",
      "Episode: 189 Total reward: 94.0 Training loss: 2.7730 Explore P: 1.0000\n",
      "Episode: 190 Total reward: 17.0 Training loss: 6.4069 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 191 Total reward: 95.0 Training loss: 2.9121 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 197 Total reward: 91.0 Training loss: 2.9257 Explore P: 1.0000\n",
      "Episode: 198 Total reward: 92.0 Training loss: 4.3747 Explore P: 1.0000\n",
      "Episode: 199 Total reward: 93.0 Training loss: 2.3052 Explore P: 1.0000\n",
      "Episode: 200 Total reward: 94.0 Training loss: 5.2764 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 202 Total reward: 94.0 Training loss: 4.3524 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 206 Total reward: 94.0 Training loss: 3.7801 Explore P: 1.0000\n",
      "Episode: 208 Total reward: 19.0 Training loss: 4.1573 Explore P: 1.0000\n",
      "Episode: 210 Total reward: 94.0 Training loss: 3.5854 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 211 Total reward: 95.0 Training loss: 8.3586 Explore P: 1.0000\n",
      "Episode: 213 Total reward: 95.0 Training loss: 10.8776 Explore P: 1.0000\n",
      "Episode: 214 Total reward: 94.0 Training loss: 5.5513 Explore P: 1.0000\n",
      "Episode: 215 Total reward: 91.0 Training loss: 4.5361 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 216 Total reward: 69.0 Training loss: 3.5613 Explore P: 1.0000\n",
      "Episode: 217 Total reward: 92.0 Training loss: 4.2905 Explore P: 1.0000\n",
      "Episode: 219 Total reward: 92.0 Training loss: 5.8430 Explore P: 1.0000\n",
      "Episode: 220 Total reward: 6.0 Training loss: 12.9114 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 221 Total reward: 89.0 Training loss: 3.5051 Explore P: 1.0000\n",
      "Episode: 222 Total reward: 45.0 Training loss: 8.5200 Explore P: 1.0000\n",
      "Episode: 224 Total reward: 95.0 Training loss: 3.1476 Explore P: 1.0000\n",
      "Episode: 225 Total reward: 90.0 Training loss: 5.0391 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 226 Total reward: 94.0 Training loss: 3.5173 Explore P: 1.0000\n",
      "Episode: 230 Total reward: 5.0 Training loss: 2.1806 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 231 Total reward: 94.0 Training loss: 3.4010 Explore P: 1.0000\n",
      "Episode: 233 Total reward: 93.0 Training loss: 6.0168 Explore P: 1.0000\n",
      "Episode: 234 Total reward: 95.0 Training loss: 3.0702 Explore P: 1.0000\n",
      "Episode: 235 Total reward: 27.0 Training loss: 4.3608 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 237 Total reward: 94.0 Training loss: 1.9779 Explore P: 1.0000\n",
      "Episode: 239 Total reward: 68.0 Training loss: 4.5217 Explore P: 1.0000\n",
      "Episode: 240 Total reward: 69.0 Training loss: 2.1623 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 244 Total reward: -18.0 Training loss: 5.7951 Explore P: 1.0000\n",
      "Episode: 245 Total reward: 94.0 Training loss: 5.7622 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 247 Total reward: 94.0 Training loss: 7.6923 Explore P: 1.0000\n",
      "Episode: 250 Total reward: 94.0 Training loss: 2.7495 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 251 Total reward: 94.0 Training loss: 4.8059 Explore P: 1.0000\n",
      "Episode: 252 Total reward: 90.0 Training loss: 2.2376 Explore P: 1.0000\n",
      "Episode: 255 Total reward: 95.0 Training loss: 7.8591 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 256 Total reward: 94.0 Training loss: 7.3073 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 261 Total reward: 42.0 Training loss: 7.3720 Explore P: 1.0000\n",
      "Episode: 262 Total reward: 89.0 Training loss: 5.5314 Explore P: 1.0000\n",
      "Episode: 263 Total reward: 95.0 Training loss: 4.1164 Explore P: 1.0000\n",
      "Episode: 264 Total reward: 95.0 Training loss: 9.4673 Explore P: 1.0000\n",
      "Episode: 265 Total reward: 90.0 Training loss: 6.2699 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 267 Total reward: 93.0 Training loss: 5.9625 Explore P: 1.0000\n",
      "Episode: 270 Total reward: 94.0 Training loss: 4.6824 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 271 Total reward: 95.0 Training loss: 2.2259 Explore P: 1.0000\n",
      "Episode: 272 Total reward: 92.0 Training loss: 1.9523 Explore P: 1.0000\n",
      "Episode: 274 Total reward: 75.0 Training loss: 2.6181 Explore P: 1.0000\n",
      "Episode: 275 Total reward: 16.0 Training loss: 4.9599 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 276 Total reward: 95.0 Training loss: 19.1991 Explore P: 1.0000\n",
      "Episode: 279 Total reward: 95.0 Training loss: 2.5801 Explore P: 1.0000\n",
      "Episode: 280 Total reward: 23.0 Training loss: 5.5134 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 281 Total reward: 2.0 Training loss: 3.4963 Explore P: 1.0000\n",
      "Episode: 284 Total reward: 95.0 Training loss: 4.3912 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 286 Total reward: 45.0 Training loss: 4.8419 Explore P: 1.0000\n",
      "Episode: 289 Total reward: 70.0 Training loss: 4.6098 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 292 Total reward: 95.0 Training loss: 8.7625 Explore P: 1.0000\n",
      "Episode: 293 Total reward: 90.0 Training loss: 7.2135 Explore P: 1.0000\n",
      "Episode: 295 Total reward: 95.0 Training loss: 4.3965 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 296 Total reward: 94.0 Training loss: 6.1173 Explore P: 1.0000\n",
      "Episode: 297 Total reward: 40.0 Training loss: 3.8205 Explore P: 1.0000\n",
      "Episode: 298 Total reward: 93.0 Training loss: 3.4654 Explore P: 1.0000\n",
      "Episode: 300 Total reward: 95.0 Training loss: 4.2772 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 301 Total reward: 95.0 Training loss: 4.2810 Explore P: 1.0000\n",
      "Episode: 302 Total reward: 90.0 Training loss: 4.2214 Explore P: 1.0000\n",
      "Episode: 303 Total reward: 92.0 Training loss: 2.3492 Explore P: 1.0000\n",
      "Episode: 304 Total reward: 94.0 Training loss: 7.0189 Explore P: 1.0000\n",
      "Episode: 305 Total reward: 93.0 Training loss: 3.6367 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 306 Total reward: 64.0 Training loss: 6.2207 Explore P: 1.0000\n",
      "Episode: 307 Total reward: 10.0 Training loss: 5.9121 Explore P: 1.0000\n",
      "Episode: 309 Total reward: 94.0 Training loss: 2.6051 Explore P: 1.0000\n",
      "Episode: 310 Total reward: 41.0 Training loss: 28.9873 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 312 Total reward: 95.0 Training loss: 4.2349 Explore P: 1.0000\n",
      "Episode: 313 Total reward: 69.0 Training loss: 1.9136 Explore P: 1.0000\n",
      "Episode: 314 Total reward: 88.0 Training loss: 4.6670 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 316 Total reward: -3.0 Training loss: 1.8508 Explore P: 1.0000\n",
      "Episode: 319 Total reward: 72.0 Training loss: 4.1213 Explore P: 1.0000\n",
      "Episode: 320 Total reward: 82.0 Training loss: 3.5573 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 321 Total reward: 93.0 Training loss: 4.8001 Explore P: 1.0000\n",
      "Episode: 322 Total reward: 49.0 Training loss: 5.8220 Explore P: 1.0000\n",
      "Episode: 323 Total reward: 91.0 Training loss: 2.8069 Explore P: 1.0000\n",
      "Episode: 325 Total reward: -13.0 Training loss: 5.2152 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 327 Total reward: 95.0 Training loss: 3.3026 Explore P: 1.0000\n",
      "Episode: 328 Total reward: 95.0 Training loss: 4.1330 Explore P: 1.0000\n",
      "Episode: 330 Total reward: 95.0 Training loss: 5.3496 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 331 Total reward: 91.0 Training loss: 7.1360 Explore P: 1.0000\n",
      "Episode: 332 Total reward: 92.0 Training loss: 6.5381 Explore P: 1.0000\n",
      "Episode: 333 Total reward: 95.0 Training loss: 2.7244 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 336 Total reward: 93.0 Training loss: 5.6520 Explore P: 1.0000\n",
      "Episode: 337 Total reward: 91.0 Training loss: 4.6494 Explore P: 1.0000\n",
      "Episode: 338 Total reward: 95.0 Training loss: 2.1503 Explore P: 1.0000\n",
      "Episode: 340 Total reward: 94.0 Training loss: 3.1678 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 342 Total reward: 51.0 Training loss: 4.8517 Explore P: 1.0000\n",
      "Episode: 343 Total reward: 94.0 Training loss: 4.0028 Explore P: 1.0000\n",
      "Episode: 344 Total reward: 94.0 Training loss: 5.2778 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 346 Total reward: 94.0 Training loss: 2.2005 Explore P: 1.0000\n",
      "Episode: 347 Total reward: 68.0 Training loss: 2.8653 Explore P: 1.0000\n",
      "Episode: 348 Total reward: 9.0 Training loss: 3.8388 Explore P: 1.0000\n",
      "Episode: 349 Total reward: 94.0 Training loss: 4.7241 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 351 Total reward: 51.0 Training loss: 3.2198 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 356 Total reward: 93.0 Training loss: 3.7091 Explore P: 1.0000\n",
      "Episode: 357 Total reward: 95.0 Training loss: 6.3367 Explore P: 1.0000\n",
      "Episode: 358 Total reward: 20.0 Training loss: 4.9048 Explore P: 1.0000\n",
      "Episode: 359 Total reward: 95.0 Training loss: 3.4050 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 361 Total reward: 95.0 Training loss: 8.2632 Explore P: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 362 Total reward: -23.0 Training loss: 4.9420 Explore P: 1.0000\n",
      "Episode: 363 Total reward: 24.0 Training loss: 2.8818 Explore P: 1.0000\n",
      "Episode: 365 Total reward: 94.0 Training loss: 5.8920 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 366 Total reward: 94.0 Training loss: 1.4393 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 371 Total reward: 95.0 Training loss: 2.3773 Explore P: 1.0000\n",
      "Episode: 373 Total reward: 92.0 Training loss: 3.1392 Explore P: 1.0000\n",
      "Episode: 374 Total reward: 95.0 Training loss: 2.3877 Explore P: 1.0000\n",
      "Episode: 375 Total reward: 92.0 Training loss: 3.9482 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 377 Total reward: 27.0 Training loss: 2.3631 Explore P: 1.0000\n",
      "Episode: 378 Total reward: 69.0 Training loss: 2.0178 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 381 Total reward: 72.0 Training loss: 3.4088 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 386 Total reward: 95.0 Training loss: 4.4144 Explore P: 1.0000\n",
      "Episode: 387 Total reward: 94.0 Training loss: 3.1238 Explore P: 1.0000\n",
      "Episode: 388 Total reward: 87.0 Training loss: 4.4646 Explore P: 1.0000\n",
      "Episode: 389 Total reward: 95.0 Training loss: 6.0254 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 391 Total reward: 69.0 Training loss: 9.3505 Explore P: 1.0000\n",
      "Episode: 392 Total reward: -19.0 Training loss: 4.7863 Explore P: 1.0000\n",
      "Episode: 393 Total reward: 95.0 Training loss: 5.2715 Explore P: 1.0000\n",
      "Episode: 394 Total reward: 91.0 Training loss: 3.7258 Explore P: 1.0000\n",
      "Model saved.\n",
      "Model saved.\n",
      "Episode: 401 Total reward: 90.0 Training loss: 4.0414 Explore P: 1.0000\n",
      "Episode: 402 Total reward: 95.0 Training loss: 3.4701 Explore P: 1.0000\n",
      "Episode: 403 Total reward: 94.0 Training loss: 5.5074 Explore P: 1.0000\n",
      "Episode: 404 Total reward: 68.0 Training loss: 3.9220 Explore P: 1.0000\n",
      "Episode: 405 Total reward: 94.0 Training loss: 4.7165 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 406 Total reward: 56.0 Training loss: 10.9717 Explore P: 1.0000\n",
      "Episode: 409 Total reward: 92.0 Training loss: 4.6022 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 411 Total reward: 95.0 Training loss: 2.9551 Explore P: 1.0000\n",
      "Episode: 414 Total reward: 69.0 Training loss: 7.5870 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 418 Total reward: 1.0 Training loss: 6.4182 Explore P: 1.0000\n",
      "Episode: 419 Total reward: 73.0 Training loss: 2.5685 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 424 Total reward: 94.0 Training loss: 4.8970 Explore P: 1.0000\n",
      "Episode: 425 Total reward: 92.0 Training loss: 3.8685 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 427 Total reward: 93.0 Training loss: 4.5630 Explore P: 1.0000\n",
      "Episode: 429 Total reward: 95.0 Training loss: 3.8211 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 431 Total reward: 94.0 Training loss: 5.6296 Explore P: 1.0000\n",
      "Episode: 432 Total reward: 95.0 Training loss: 1.7638 Explore P: 1.0000\n",
      "Episode: 434 Total reward: 95.0 Training loss: 6.9924 Explore P: 1.0000\n",
      "Episode: 435 Total reward: -3.0 Training loss: 5.3928 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 436 Total reward: -5.0 Training loss: 4.0208 Explore P: 1.0000\n",
      "Episode: 437 Total reward: 95.0 Training loss: 3.3044 Explore P: 1.0000\n",
      "Episode: 438 Total reward: 92.0 Training loss: 5.8205 Explore P: 1.0000\n",
      "Episode: 440 Total reward: 94.0 Training loss: 3.6013 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 441 Total reward: 91.0 Training loss: 4.5421 Explore P: 1.0000\n",
      "Episode: 442 Total reward: 91.0 Training loss: 6.7539 Explore P: 1.0000\n",
      "Episode: 443 Total reward: 62.0 Training loss: 2.5107 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 447 Total reward: 89.0 Training loss: 4.0078 Explore P: 1.0000\n",
      "Episode: 449 Total reward: 95.0 Training loss: 7.4524 Explore P: 1.0000\n",
      "Episode: 450 Total reward: 93.0 Training loss: 3.4002 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 452 Total reward: 89.0 Training loss: 6.4666 Explore P: 1.0000\n",
      "Episode: 453 Total reward: 26.0 Training loss: 7.6020 Explore P: 1.0000\n",
      "Episode: 454 Total reward: 94.0 Training loss: 4.7920 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 457 Total reward: 50.0 Training loss: 6.6751 Explore P: 1.0000\n",
      "Episode: 458 Total reward: 86.0 Training loss: 4.7472 Explore P: 1.0000\n",
      "Episode: 459 Total reward: 95.0 Training loss: 3.7197 Explore P: 1.0000\n",
      "Episode: 460 Total reward: 95.0 Training loss: 5.3104 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 463 Total reward: 89.0 Training loss: 7.3725 Explore P: 1.0000\n",
      "Episode: 464 Total reward: 75.0 Training loss: 9.1946 Explore P: 1.0000\n",
      "Episode: 465 Total reward: 68.0 Training loss: 11.6712 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 467 Total reward: 88.0 Training loss: 2.2705 Explore P: 1.0000\n",
      "Episode: 468 Total reward: 93.0 Training loss: 4.4990 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 471 Total reward: 95.0 Training loss: 3.5574 Explore P: 1.0000\n",
      "Episode: 472 Total reward: 93.0 Training loss: 4.0046 Explore P: 1.0000\n",
      "Episode: 473 Total reward: 94.0 Training loss: 5.3835 Explore P: 1.0000\n",
      "Episode: 474 Total reward: 95.0 Training loss: 3.5977 Explore P: 1.0000\n",
      "Episode: 475 Total reward: 94.0 Training loss: 3.3269 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 476 Total reward: 94.0 Training loss: 5.4218 Explore P: 1.0000\n",
      "Episode: 477 Total reward: 66.0 Training loss: 2.9598 Explore P: 1.0000\n",
      "Episode: 478 Total reward: 93.0 Training loss: 8.7795 Explore P: 1.0000\n",
      "Episode: 480 Total reward: 93.0 Training loss: 3.9949 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 481 Total reward: 92.0 Training loss: 4.4812 Explore P: 1.0000\n",
      "Episode: 485 Total reward: 95.0 Training loss: 4.0830 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 487 Total reward: 94.0 Training loss: 3.3890 Explore P: 1.0000\n",
      "Episode: 488 Total reward: 93.0 Training loss: 3.5761 Explore P: 1.0000\n",
      "Episode: 489 Total reward: 95.0 Training loss: 6.1096 Explore P: 1.0000\n",
      "Episode: 490 Total reward: 91.0 Training loss: 3.6648 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 492 Total reward: 26.0 Training loss: 5.5002 Explore P: 1.0000\n",
      "Episode: 495 Total reward: 94.0 Training loss: 4.4106 Explore P: 1.0000\n",
      "Model saved.\n",
      "Episode: 496 Total reward: 93.0 Training loss: 3.7477 Explore P: 1.0000\n",
      "Episode: 497 Total reward: 95.0 Training loss: 2.9123 Explore P: 1.0000\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        #init the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        #init the decay rate that reduces the epsilon\n",
    "        decay_step = 0\n",
    "        \n",
    "        game.init()\n",
    "        for episode in range(total_episodes):\n",
    "            step = 0\n",
    "            \n",
    "            episode_rewards = []\n",
    "            \n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "            while step < max_steps:\n",
    "                step+=1                \n",
    "                decay_step +=1\n",
    "                \n",
    "                #predict the action to take and take it\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "                \n",
    "                reward = game.make_action(action)\n",
    "                done = game.is_episode_finished()\n",
    "                \n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                if done:\n",
    "                    next_state = np.zeros((84, 84), dtype = np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "                    step = max_steps\n",
    "                    \n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "                    \n",
    "                    print('Episode: {}'.format(episode), 'Total reward: {}'.format(total_reward), 'Training loss: {:.4f}'.format(loss),\n",
    "                                                       'Explore P: {:.4f}'.format(explore_probability))\n",
    "                    \n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                else:\n",
    "                    next_state = game.get_state().screen_buffer                    \n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames,next_state, False)\n",
    "                    \n",
    "                    memory.add((state, action, reward, next_state, done))                   \n",
    "                    state = next_state\n",
    "                \n",
    "                #learning part\n",
    "                #obtain random minibatch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin = 3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch])\n",
    "                next_states_mb = np.array([each[3] for each in batch])\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "                \n",
    "                target_Qs_batch = []\n",
    "                \n",
    "                Qs_next_state_vals = sess.run(DeepQNetwork.output, feed_dict = {DeepQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                #set q_target = r if the episode ends at s+1, otherwise q_target = r + gamma*maxQ(s', a')\n",
    "                \n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "                    \n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma*np.max(Qs_next_state_vals[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "                \n",
    "                loss, _ = sess.run([DeepQNetwork.loss, DeepQNetwork.optimizer],\n",
    "                                  feed_dict = {DeepQNetwork.inputs_: states_mb,\n",
    "                                              DeepQNetwork.target_Q: targets_mb, \n",
    "                                              DeepQNetwork.actions_: actions_mb})\n",
    "                \n",
    "                #write tensorflow summaries\n",
    "                summary = sess.run(write_op, feed_dict = {DeepQNetwork.inputs_: states_mb,\n",
    "                                                         DeepQNetwork.target_Q: targets_mb,\n",
    "                                                         DeepQNetwork.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model saved.\")              \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "Score:  73.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    game, possible_actions = ec.create_environment()\n",
    "    \n",
    "    totalScore = 0\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    for i in range(1):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        game.new_episode()\n",
    "        \n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "        while not game.is_episode_finished():\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            Qs = sess.run(DeepQNetwork.output, feed_dict = {DeepQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "            \n",
    "            # Take the biggest Q value (= the best action)\n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            score = game.get_total_reward()\n",
    "            \n",
    "            if done:\n",
    "                break  \n",
    "                \n",
    "            else:\n",
    "                print(\"else\")\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "                \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
