{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf     \n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "import environment_creation as ec\n",
    "import frame_preprocessing as fp\n",
    "import frames_stacking as fs\n",
    "import memory as mem\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = ec.create_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack_frames\n",
    "stack_size = 4\n",
    "stacked_frames = deque([np.zeros((84, 84), dtype = np.int) for i in range(stack_size)], maxlen = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model hyperparameters\n",
    "state_size = [84, 84, 4]\n",
    "action_size = game.get_available_buttons_size()\n",
    "learning_rate = 0.0001\n",
    "\n",
    "#training hyperparameters\n",
    "total_episodes = 500\n",
    "max_steps = 500\n",
    "batch_size = 128\n",
    "\n",
    "#exploration/epsilon greedy strategy parameters\n",
    "explore_start = 1.0\n",
    "explore_stop = 0.09\n",
    "decay_rate = 0.0003\n",
    "\n",
    "#q leraning hyperparameters\n",
    "gamma = 0.95 #discount rate\n",
    "\n",
    "###memory hyperparameters\n",
    "pretrain_length = batch_size #number of experiences stored in memory when initialized for the first time\n",
    "memory_size = 100000 #no of experiences memory can keep\n",
    "\n",
    "training = True\n",
    "episode_render = False\n",
    "\n",
    "stacked_frames = deque([np.zeros((84, 84), dtype = np.int) for i in range(stack_size)], maxlen = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name = \"DeepQNetwork\"):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            #here we create the placeholders\n",
    "            \n",
    "            #[None, 84, 84, 4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, 3], name = \"actions_\")\n",
    "        \n",
    "            #target_Q is the R(s, a) + ymax Q^(s', a')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name = \"targetQ\")\n",
    "            \n",
    "            # Input is 84x84x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                         filters = 32,\n",
    "                                         kernel_size = [8,8],\n",
    "                                         strides = [4,4],\n",
    "                                         padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv1\")\n",
    "            \n",
    "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm1')\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
    "            ## --> [20, 20, 32]\n",
    "            \n",
    "            #second\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv2\")\n",
    "        \n",
    "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm2')\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "            ## --> [9, 9, 64]\n",
    "            \n",
    "            #thirds\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv3\")\n",
    "        \n",
    "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm3')\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "            ## --> [3, 3, 128]\n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            ## --> [1152]\n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                      units = 512,\n",
    "                                      activation = tf.nn.elu,\n",
    "                                      kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                      name = 'fc1')\n",
    "            \n",
    "            self.output = tf.layers.dense(inputs = self.fc, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units = 3, \n",
    "                                        activation=None)\n",
    "            \n",
    "            #Q is our predicted Q value\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis = 1)\n",
    "            \n",
    "            #loss is the difference between predicted Q value and Q_target\n",
    "            # sum(q_target - q)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "DeepQNetwork = DeepQNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = fp.preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the memory class\n",
    "#instantiate the memory\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "#now dealing with the empty memory problem - prepopulating memory by taking random actions\n",
    "#and storing experience (state, action, reward, new_state)\n",
    "\n",
    "game.new_episode()\n",
    "for i in range(pretrain_length):\n",
    "    #if it's the first step\n",
    "    if i==0:\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    #take random action\n",
    "    action = random.choice(possible_actions)\n",
    "   \n",
    "    #get rewardd\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    #look if episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        #add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        #start a new episode\n",
    "        game.new_episode()\n",
    "        \n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames =  stack_frames(stacked_frames, state, True)\n",
    "    else:\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        #add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        #our state is now the next state\n",
    "        state = next_state    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup tensorboard writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
    "\n",
    "tf.summary.scalar(\"Loss\", DeepQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init the weights\n",
    "#init the environment\n",
    "#init decay rate\n",
    "\n",
    "#the following function does the epsilon-greedy strategy part - predicts action\n",
    "def predict_action(explore_start, explore_top, decay_rate, decay_step, state, actions):\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "    \n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if(explore_probability > exp_exp_tradeoff):\n",
    "        action = random.choice(possible_actions)\n",
    "    else:\n",
    "        #get action from q-network (exploitation)\n",
    "        Qs = sess.run(DeepQNetwork.output, feed_dict = {DeepQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        #take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "    \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 95.0 Training loss: 83.2392 Explore P: 0.9984\n",
      "Model saved.\n",
      "Episode: 1 Total reward: 57.0 Training loss: 146.3920 Explore P: 0.9891\n",
      "Episode: 2 Total reward: 91.0 Training loss: 179.2342 Explore P: 0.9865\n",
      "Episode: 3 Total reward: 87.0 Training loss: 176.5741 Explore P: 0.9827\n",
      "Episode: 4 Total reward: -261.0 Training loss: 69.4273 Explore P: 0.9090\n",
      "Episode: 5 Total reward: 95.0 Training loss: 9.0820 Explore P: 0.9076\n",
      "Model saved.\n",
      "Episode: 6 Total reward: 94.0 Training loss: 17.3524 Explore P: 0.9059\n",
      "Episode: 7 Total reward: 0.0 Training loss: 6.9515 Explore P: 0.8863\n",
      "Episode: 8 Total reward: 19.0 Training loss: 5.4686 Explore P: 0.8704\n",
      "Episode: 9 Total reward: -380.0 Training loss: 2.9481 Explore P: 0.8033\n",
      "Episode: 10 Total reward: 94.0 Training loss: 2.3296 Explore P: 0.8018\n",
      "Model saved.\n",
      "Episode: 11 Total reward: 94.0 Training loss: 2.5870 Explore P: 0.8003\n",
      "Episode: 12 Total reward: -26.0 Training loss: 6.2945 Explore P: 0.7789\n",
      "Episode: 13 Total reward: -380.0 Training loss: 3.2300 Explore P: 0.7196\n",
      "Episode: 14 Total reward: -29.0 Training loss: 3.0488 Explore P: 0.7001\n",
      "Episode: 15 Total reward: -2.0 Training loss: 13.2994 Explore P: 0.6850\n",
      "Model saved.\n",
      "Episode: 16 Total reward: -380.0 Training loss: 2.0416 Explore P: 0.6338\n",
      "Episode: 17 Total reward: 92.0 Training loss: 4.3725 Explore P: 0.6324\n",
      "Episode: 18 Total reward: -380.0 Training loss: 1.5776 Explore P: 0.5857\n",
      "Episode: 19 Total reward: 93.0 Training loss: 1.4052 Explore P: 0.5845\n",
      "Episode: 20 Total reward: 26.0 Training loss: 1.7639 Explore P: 0.5757\n",
      "Model saved.\n",
      "Episode: 21 Total reward: 95.0 Training loss: 3.2384 Explore P: 0.5748\n",
      "Episode: 22 Total reward: 37.0 Training loss: 2.0638 Explore P: 0.5670\n",
      "Episode: 23 Total reward: 68.0 Training loss: 2.0306 Explore P: 0.5630\n",
      "Episode: 24 Total reward: 95.0 Training loss: 3.2178 Explore P: 0.5622\n",
      "Episode: 25 Total reward: 95.0 Training loss: 6.9192 Explore P: 0.5613\n",
      "Model saved.\n",
      "Episode: 26 Total reward: 50.0 Training loss: 8.2280 Explore P: 0.5556\n",
      "Episode: 27 Total reward: 95.0 Training loss: 6.4349 Explore P: 0.5547\n",
      "Episode: 28 Total reward: 36.0 Training loss: 6.2889 Explore P: 0.5471\n",
      "Episode: 29 Total reward: -133.0 Training loss: 5.3382 Explore P: 0.5219\n",
      "Episode: 30 Total reward: 95.0 Training loss: 7.6789 Explore P: 0.5211\n",
      "Model saved.\n",
      "Episode: 31 Total reward: -380.0 Training loss: 2.1947 Explore P: 0.4840\n",
      "Episode: 32 Total reward: 94.0 Training loss: 2.3245 Explore P: 0.4832\n",
      "Episode: 33 Total reward: 95.0 Training loss: 2.4291 Explore P: 0.4825\n",
      "Episode: 34 Total reward: 94.0 Training loss: 1.4423 Explore P: 0.4817\n",
      "Episode: 35 Total reward: 95.0 Training loss: 2.1177 Explore P: 0.4810\n",
      "Model saved.\n",
      "Episode: 36 Total reward: 92.0 Training loss: 2.9047 Explore P: 0.4799\n",
      "Episode: 37 Total reward: 94.0 Training loss: 3.3198 Explore P: 0.4791\n",
      "Episode: 38 Total reward: 94.0 Training loss: 1.4125 Explore P: 0.4783\n",
      "Episode: 39 Total reward: 69.0 Training loss: 9.2204 Explore P: 0.4752\n",
      "Episode: 40 Total reward: 23.0 Training loss: 21.7208 Explore P: 0.4679\n",
      "Model saved.\n",
      "Episode: 41 Total reward: 95.0 Training loss: 6.4461 Explore P: 0.4673\n",
      "Episode: 42 Total reward: 94.0 Training loss: 8.8826 Explore P: 0.4665\n",
      "Episode: 43 Total reward: -380.0 Training loss: 2.2720 Explore P: 0.4341\n",
      "Episode: 44 Total reward: 94.0 Training loss: 2.2064 Explore P: 0.4333\n",
      "Episode: 45 Total reward: -97.0 Training loss: 2.3721 Explore P: 0.4179\n",
      "Model saved.\n",
      "Episode: 46 Total reward: 95.0 Training loss: 1.6724 Explore P: 0.4174\n",
      "Episode: 47 Total reward: 95.0 Training loss: 1.9284 Explore P: 0.4168\n",
      "Episode: 48 Total reward: 26.0 Training loss: 3.5567 Explore P: 0.4109\n",
      "Episode: 49 Total reward: 76.0 Training loss: 4.7508 Explore P: 0.4090\n",
      "Episode: 50 Total reward: 95.0 Training loss: 3.6002 Explore P: 0.4084\n",
      "Model saved.\n",
      "Episode: 51 Total reward: 52.0 Training loss: 1.8671 Explore P: 0.4047\n",
      "Episode: 52 Total reward: 95.0 Training loss: 5.3931 Explore P: 0.4042\n",
      "Episode: 53 Total reward: 56.0 Training loss: 4.0191 Explore P: 0.4009\n",
      "Episode: 54 Total reward: 94.0 Training loss: 2.1822 Explore P: 0.4002\n",
      "Episode: 55 Total reward: -144.0 Training loss: 2.8535 Explore P: 0.3822\n",
      "Model saved.\n",
      "Episode: 56 Total reward: 95.0 Training loss: 2.0007 Explore P: 0.3816\n",
      "Episode: 57 Total reward: 95.0 Training loss: 3.5905 Explore P: 0.3811\n",
      "Episode: 58 Total reward: 95.0 Training loss: 3.3465 Explore P: 0.3806\n",
      "Episode: 59 Total reward: 95.0 Training loss: 3.4064 Explore P: 0.3801\n",
      "Episode: 60 Total reward: 45.0 Training loss: 3.2146 Explore P: 0.3761\n",
      "Model saved.\n",
      "Episode: 61 Total reward: 95.0 Training loss: 1.6510 Explore P: 0.3756\n",
      "Episode: 62 Total reward: -6.0 Training loss: 4.4015 Explore P: 0.3682\n",
      "Episode: 63 Total reward: -375.0 Training loss: 2.3891 Explore P: 0.3443\n",
      "Episode: 64 Total reward: 95.0 Training loss: 4.2825 Explore P: 0.3438\n",
      "Episode: 65 Total reward: 95.0 Training loss: 3.6931 Explore P: 0.3434\n",
      "Model saved.\n",
      "Episode: 66 Total reward: 95.0 Training loss: 3.0535 Explore P: 0.3429\n",
      "Episode: 67 Total reward: 95.0 Training loss: 1.2156 Explore P: 0.3425\n",
      "Episode: 68 Total reward: 26.0 Training loss: 5.3262 Explore P: 0.3380\n",
      "Episode: 69 Total reward: 95.0 Training loss: 4.1422 Explore P: 0.3375\n",
      "Episode: 70 Total reward: 95.0 Training loss: 11.1650 Explore P: 0.3371\n",
      "Model saved.\n",
      "Episode: 71 Total reward: -179.0 Training loss: 3.2928 Explore P: 0.3209\n",
      "Episode: 72 Total reward: 95.0 Training loss: 1.9266 Explore P: 0.3205\n",
      "Episode: 73 Total reward: 95.0 Training loss: 5.6445 Explore P: 0.3201\n",
      "Episode: 74 Total reward: 44.0 Training loss: 3.7738 Explore P: 0.3169\n",
      "Episode: 75 Total reward: 17.0 Training loss: 2.7322 Explore P: 0.3122\n",
      "Model saved.\n",
      "Episode: 76 Total reward: 95.0 Training loss: 6.4229 Explore P: 0.3118\n",
      "Episode: 77 Total reward: -61.0 Training loss: 4.0236 Explore P: 0.3032\n",
      "Episode: 78 Total reward: 42.0 Training loss: 8.5507 Explore P: 0.3001\n",
      "Episode: 79 Total reward: 95.0 Training loss: 7.4258 Explore P: 0.2997\n",
      "Episode: 80 Total reward: 95.0 Training loss: 8.6516 Explore P: 0.2994\n",
      "Model saved.\n",
      "Episode: 81 Total reward: 76.0 Training loss: 2.6446 Explore P: 0.2981\n",
      "Episode: 82 Total reward: 62.0 Training loss: 7.1994 Explore P: 0.2960\n",
      "Episode: 83 Total reward: 95.0 Training loss: 4.3361 Explore P: 0.2956\n",
      "Episode: 84 Total reward: 95.0 Training loss: 3.6870 Explore P: 0.2953\n",
      "Episode: 85 Total reward: 92.0 Training loss: 5.9311 Explore P: 0.2947\n",
      "Model saved.\n",
      "Episode: 86 Total reward: 95.0 Training loss: 3.7733 Explore P: 0.2943\n",
      "Episode: 87 Total reward: 76.0 Training loss: 2.5851 Explore P: 0.2931\n",
      "Episode: 88 Total reward: 95.0 Training loss: 6.8114 Explore P: 0.2927\n",
      "Episode: 89 Total reward: 93.0 Training loss: 4.6307 Explore P: 0.2923\n",
      "Episode: 90 Total reward: 62.0 Training loss: 2.1182 Explore P: 0.2902\n",
      "Model saved.\n",
      "Episode: 91 Total reward: 95.0 Training loss: 3.6563 Explore P: 0.2898\n",
      "Episode: 92 Total reward: 95.0 Training loss: 6.2680 Explore P: 0.2895\n",
      "Episode: 93 Total reward: 95.0 Training loss: 4.4387 Explore P: 0.2891\n",
      "Episode: 94 Total reward: 50.0 Training loss: 5.9155 Explore P: 0.2867\n",
      "Episode: 95 Total reward: 95.0 Training loss: 3.0785 Explore P: 0.2863\n",
      "Model saved.\n",
      "Episode: 96 Total reward: -11.0 Training loss: 9.8319 Explore P: 0.2810\n",
      "Episode: 97 Total reward: 95.0 Training loss: 2.6182 Explore P: 0.2807\n",
      "Episode: 98 Total reward: 21.0 Training loss: 2.7401 Explore P: 0.2770\n",
      "Episode: 99 Total reward: -19.0 Training loss: 2.3182 Explore P: 0.2717\n",
      "Episode: 100 Total reward: 95.0 Training loss: 3.0040 Explore P: 0.2714\n",
      "Model saved.\n",
      "Episode: 101 Total reward: 55.0 Training loss: 7.8087 Explore P: 0.2694\n",
      "Episode: 102 Total reward: 95.0 Training loss: 12.8935 Explore P: 0.2691\n",
      "Episode: 103 Total reward: 65.0 Training loss: 4.1631 Explore P: 0.2675\n",
      "Episode: 104 Total reward: 94.0 Training loss: 5.3276 Explore P: 0.2671\n",
      "Episode: 105 Total reward: 95.0 Training loss: 8.6832 Explore P: 0.2668\n",
      "Model saved.\n",
      "Episode: 106 Total reward: 95.0 Training loss: 5.1683 Explore P: 0.2665\n",
      "Episode: 107 Total reward: -380.0 Training loss: 5.2301 Explore P: 0.2513\n",
      "Episode: 108 Total reward: -17.0 Training loss: 2.9939 Explore P: 0.2468\n",
      "Episode: 109 Total reward: -72.0 Training loss: 5.2272 Explore P: 0.2405\n",
      "Episode: 110 Total reward: 51.0 Training loss: 3.7259 Explore P: 0.2387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n",
      "Episode: 111 Total reward: 26.0 Training loss: 4.6887 Explore P: 0.2360\n",
      "Episode: 112 Total reward: 40.0 Training loss: 4.8520 Explore P: 0.2338\n",
      "Episode: 113 Total reward: 94.0 Training loss: 5.6482 Explore P: 0.2335\n",
      "Episode: 114 Total reward: -88.0 Training loss: 4.2906 Explore P: 0.2272\n",
      "Episode: 115 Total reward: 95.0 Training loss: 3.4606 Explore P: 0.2270\n",
      "Model saved.\n",
      "Episode: 116 Total reward: 52.0 Training loss: 4.4201 Explore P: 0.2254\n",
      "Episode: 117 Total reward: 26.0 Training loss: 9.5299 Explore P: 0.2230\n",
      "Episode: 118 Total reward: 95.0 Training loss: 4.9021 Explore P: 0.2227\n",
      "Episode: 119 Total reward: 95.0 Training loss: 5.8654 Explore P: 0.2225\n",
      "Episode: 120 Total reward: 76.0 Training loss: 3.4980 Explore P: 0.2217\n",
      "Model saved.\n",
      "Episode: 121 Total reward: 95.0 Training loss: 3.8044 Explore P: 0.2215\n",
      "Episode: 122 Total reward: 47.0 Training loss: 4.6074 Explore P: 0.2197\n",
      "Episode: 123 Total reward: -53.0 Training loss: 7.6473 Explore P: 0.2152\n",
      "Episode: 124 Total reward: 9.0 Training loss: 2.7925 Explore P: 0.2123\n",
      "Episode: 125 Total reward: 94.0 Training loss: 4.5936 Explore P: 0.2121\n",
      "Model saved.\n",
      "Episode: 126 Total reward: 95.0 Training loss: 4.2220 Explore P: 0.2119\n",
      "Episode: 127 Total reward: 95.0 Training loss: 5.9012 Explore P: 0.2116\n",
      "Episode: 128 Total reward: 95.0 Training loss: 2.1151 Explore P: 0.2114\n",
      "Episode: 129 Total reward: 95.0 Training loss: 3.6365 Explore P: 0.2112\n",
      "Episode: 130 Total reward: 95.0 Training loss: 3.1185 Explore P: 0.2110\n",
      "Model saved.\n",
      "Episode: 131 Total reward: 61.0 Training loss: 4.7874 Explore P: 0.2097\n",
      "Episode: 132 Total reward: 95.0 Training loss: 4.7038 Explore P: 0.2095\n",
      "Episode: 133 Total reward: -124.0 Training loss: 2.7646 Explore P: 0.2032\n",
      "Episode: 134 Total reward: 94.0 Training loss: 1.4454 Explore P: 0.2030\n",
      "Episode: 135 Total reward: -370.0 Training loss: 5.5608 Explore P: 0.1933\n",
      "Model saved.\n",
      "Episode: 136 Total reward: 94.0 Training loss: 3.7922 Explore P: 0.1930\n",
      "Episode: 137 Total reward: 94.0 Training loss: 3.2466 Explore P: 0.1928\n",
      "Episode: 138 Total reward: -360.0 Training loss: 2.2182 Explore P: 0.1840\n",
      "Episode: 139 Total reward: 66.0 Training loss: 2.3676 Explore P: 0.1831\n",
      "Episode: 140 Total reward: 95.0 Training loss: 1.9832 Explore P: 0.1830\n",
      "Model saved.\n",
      "Episode: 141 Total reward: 95.0 Training loss: 4.7391 Explore P: 0.1828\n",
      "Episode: 142 Total reward: 68.0 Training loss: 2.1809 Explore P: 0.1820\n",
      "Episode: 143 Total reward: 95.0 Training loss: 4.6037 Explore P: 0.1819\n",
      "Episode: 144 Total reward: 95.0 Training loss: 4.1982 Explore P: 0.1817\n",
      "Episode: 145 Total reward: 95.0 Training loss: 2.3225 Explore P: 0.1815\n",
      "Model saved.\n",
      "Episode: 146 Total reward: 82.0 Training loss: 2.7521 Explore P: 0.1810\n",
      "Episode: 147 Total reward: 95.0 Training loss: 2.4025 Explore P: 0.1808\n",
      "Episode: 148 Total reward: 95.0 Training loss: 1.9091 Explore P: 0.1807\n",
      "Episode: 149 Total reward: 57.0 Training loss: 3.2896 Explore P: 0.1798\n",
      "Episode: 150 Total reward: 95.0 Training loss: 2.7554 Explore P: 0.1796\n",
      "Model saved.\n",
      "Episode: 151 Total reward: 55.0 Training loss: 2.7937 Explore P: 0.1786\n",
      "Episode: 152 Total reward: 57.0 Training loss: 2.2880 Explore P: 0.1777\n",
      "Episode: 153 Total reward: -375.0 Training loss: 3.0319 Explore P: 0.1702\n",
      "Episode: 154 Total reward: 95.0 Training loss: 2.2307 Explore P: 0.1700\n",
      "Episode: 155 Total reward: 15.0 Training loss: 2.0908 Explore P: 0.1684\n",
      "Model saved.\n",
      "Episode: 156 Total reward: 94.0 Training loss: 4.7493 Explore P: 0.1682\n",
      "Episode: 157 Total reward: 67.0 Training loss: 2.8074 Explore P: 0.1675\n",
      "Episode: 158 Total reward: -365.0 Training loss: 3.6389 Explore P: 0.1608\n",
      "Episode: 159 Total reward: 95.0 Training loss: 3.2548 Explore P: 0.1607\n",
      "Episode: 160 Total reward: 34.0 Training loss: 1.9074 Explore P: 0.1595\n",
      "Model saved.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-5b3156351484>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     79\u001b[0m                                   feed_dict = {DeepQNetwork.inputs_: states_mb,\n\u001b[0;32m     80\u001b[0m                                               \u001b[0mDeepQNetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_Q\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargets_mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                                               DeepQNetwork.actions_: actions_mb})\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;31m#write tensorflow summaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1102\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \"\"\"\n\u001b[1;32m--> 492\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        #init the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        #init the decay rate that reduces the epsilon\n",
    "        decay_step = 0\n",
    "        \n",
    "        game.init()\n",
    "        for episode in range(total_episodes):\n",
    "            step = 0\n",
    "            \n",
    "            episode_rewards = []\n",
    "            \n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "            while step < max_steps:\n",
    "                step+=1                \n",
    "                decay_step +=1\n",
    "                \n",
    "                #predict the action to take and take it\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "                \n",
    "                reward = game.make_action(action)\n",
    "                done = game.is_episode_finished()\n",
    "                \n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                if done:\n",
    "                    next_state = np.zeros((84, 84), dtype = np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "                    step = max_steps\n",
    "                    \n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "                    \n",
    "                    print('Episode: {}'.format(episode), 'Total reward: {}'.format(total_reward), 'Training loss: {:.4f}'.format(loss),\n",
    "                                                       'Explore P: {:.4f}'.format(explore_probability))\n",
    "                    \n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                else:\n",
    "                    next_state = game.get_state().screen_buffer                    \n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames,next_state, False)\n",
    "                    \n",
    "                    memory.add((state, action, reward, next_state, done))                   \n",
    "                    state = next_state\n",
    "                \n",
    "                #learning part\n",
    "                #obtain random minibatch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin = 3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch])\n",
    "                next_states_mb = np.array([each[3] for each in batch])\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "                \n",
    "                target_Qs_batch = []\n",
    "                \n",
    "                Qs_next_state_vals = sess.run(DeepQNetwork.output, feed_dict = {DeepQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                #set q_target = r if the episode ends at s+1, otherwise q_target = r + gamma*maxQ(s', a')\n",
    "                \n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "                    \n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma*np.max(Qs_next_state_vals[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "                \n",
    "                loss, _ = sess.run([DeepQNetwork.loss, DeepQNetwork.optimizer],\n",
    "                                  feed_dict = {DeepQNetwork.inputs_: states_mb,\n",
    "                                              DeepQNetwork.target_Q: targets_mb, \n",
    "                                              DeepQNetwork.actions_: actions_mb})\n",
    "                \n",
    "                #write tensorflow summaries\n",
    "                summary = sess.run(write_op, feed_dict = {DeepQNetwork.inputs_: states_mb,\n",
    "                                                         DeepQNetwork.target_Q: targets_mb,\n",
    "                                                         DeepQNetwork.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model saved.\")              \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "Score:  95.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    game, possible_actions = ec.create_environment()\n",
    "    \n",
    "    totalScore = 0\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    for i in range(1):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        game.new_episode()\n",
    "        \n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "        while not game.is_episode_finished():\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            Qs = sess.run(DeepQNetwork.output, feed_dict = {DeepQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "            \n",
    "            # Take the biggest Q value (= the best action)\n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            score = game.get_total_reward()\n",
    "            \n",
    "            if done:\n",
    "                break  \n",
    "                \n",
    "            else:\n",
    "                print(\"else\")\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "                \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
