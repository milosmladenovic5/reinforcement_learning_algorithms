{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "from environment_creation import create_environment\n",
    "import pg_network as pg\n",
    "from frame_preprocessing import preprocess_frame\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()\n",
    "\n",
    "stack_size = 4\n",
    "\n",
    "stacked_frames = deque([np.zeros((84,84), dtype = np.int) for i in range(stack_size)], maxlen = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames (stacked_frames, state, is_new_episode):\n",
    "    frame = preprocess_frame(state)\n",
    "\n",
    "    if is_new_episode:\n",
    "        #clear stacked frames\n",
    "        stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "        for i in (0,4):\n",
    "            stacked_frames.append(frame)\n",
    "    else:\n",
    "        #append the frame to deque, automatically removes the oldest one\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        #build the stacked state\n",
    "\n",
    "    stacked_state = np.stack(stacked_frames, axis = 2)\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards, gamma):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "\n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean)/std\n",
    "\n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "###Environment hyperparameters\n",
    "state_size = [84, 84, 4] #our input is a stack of 4 frames , 84x84\n",
    "action_size = game.get_available_buttons_size() # 3 possible actions, turn left, turn right, go forward\n",
    "stack_size = 4 #how many frames are stacked together\n",
    "\n",
    "#deep learning model hyperparameters\n",
    "learning_rate = 0.002\n",
    "num_epochs = 1000\n",
    "\n",
    "batch_size = 1000\n",
    "gamma = 0.95 #discount rate\n",
    "\n",
    "training = True\n",
    "######################################\n",
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='PGNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope(\"inputs\"):\n",
    "                # We create the placeholders\n",
    "                # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "                # [None, 84, 84, 4]\n",
    "                self.inputs_= tf.placeholder(tf.float32, [None, *state_size], name=\"inputs_\")\n",
    "                self.actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
    "                self.discounted_episode_rewards_ = tf.placeholder(tf.float32, [None, ], name=\"discounted_episode_rewards_\")\n",
    "            \n",
    "                \n",
    "                # Add this placeholder for having this variable in tensorboard\n",
    "                self.mean_reward_ = tf.placeholder(tf.float32, name=\"mean_reward\")\n",
    "                \n",
    "            with tf.name_scope(\"conv1\"):\n",
    "                \"\"\"\n",
    "                First convnet:\n",
    "                CNN\n",
    "                BatchNormalization\n",
    "                ELU\n",
    "                \"\"\"\n",
    "                # Input is 84x84x4\n",
    "                self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                             filters = 32,\n",
    "                                             kernel_size = [8,8],\n",
    "                                             strides = [4,4],\n",
    "                                             padding = \"VALID\",\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                             name = \"conv1\")\n",
    "\n",
    "                self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                       training = True,\n",
    "                                                       epsilon = 1e-5,\n",
    "                                                         name = 'batch_norm1')\n",
    "\n",
    "                self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
    "                ## --> [20, 20, 32]\n",
    "            \n",
    "            with tf.name_scope(\"conv2\"):\n",
    "                \"\"\"\n",
    "                Second convnet:\n",
    "                CNN\n",
    "                BatchNormalization\n",
    "                ELU\n",
    "                \"\"\"\n",
    "                self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                     filters = 64,\n",
    "                                     kernel_size = [4,4],\n",
    "                                     strides = [2,2],\n",
    "                                     padding = \"VALID\",\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                     name = \"conv2\")\n",
    "\n",
    "                self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                       training = True,\n",
    "                                                       epsilon = 1e-5,\n",
    "                                                         name = 'batch_norm2')\n",
    "\n",
    "                self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "                ## --> [9, 9, 64]\n",
    "            \n",
    "            with tf.name_scope(\"conv3\"):\n",
    "                \"\"\"\n",
    "                Third convnet:\n",
    "                CNN\n",
    "                BatchNormalization\n",
    "                ELU\n",
    "                \"\"\"\n",
    "                self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                     filters = 128,\n",
    "                                     kernel_size = [4,4],\n",
    "                                     strides = [2,2],\n",
    "                                     padding = \"VALID\",\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                     name = \"conv3\")\n",
    "\n",
    "                self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                       training = True,\n",
    "                                                       epsilon = 1e-5,\n",
    "                                                         name = 'batch_norm3')\n",
    "\n",
    "                self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "                ## --> [3, 3, 128]\n",
    "            \n",
    "            with tf.name_scope(\"flatten\"):\n",
    "                self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "                ## --> [1152]\n",
    "            \n",
    "            with tf.name_scope(\"fc1\"):\n",
    "                self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                      units = 512,\n",
    "                                      activation = tf.nn.elu,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                    name=\"fc1\")\n",
    "            \n",
    "            with tf.name_scope(\"logits\"):\n",
    "                self.logits = tf.layers.dense(inputs = self.fc, \n",
    "                                               kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              units = 3, \n",
    "                                            activation=None)\n",
    "            \n",
    "            with tf.name_scope(\"softmax\"):\n",
    "                self.action_distribution = tf.nn.softmax(self.logits)\n",
    "                \n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
    "                # If you have single-class labels, where an object can only belong to one class, you might now consider using \n",
    "                # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array. \n",
    "                self.neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.logits, labels = self.actions)\n",
    "                self.loss = tf.reduce_mean(self.neg_log_prob * self.discounted_episode_rewards_) \n",
    "        \n",
    "    \n",
    "            with tf.name_scope(\"train\"):\n",
    "                self.train_opt = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "policy_gradient_net = PGNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup tensorflow writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/pg/test\")\n",
    "\n",
    "#losses\n",
    "tf.summary.scalar(\"Loss\", PGNetwork.loss)\n",
    "\n",
    "#reward mean\n",
    "tf.summary.scalar(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now is the time to train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(batch_size, stacked_frames):\n",
    "    #init lists: states, actions, rewards_of_episodes, discount_rewards\n",
    "    states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards = [], [], [], [], []\n",
    "\n",
    "    #reward of batch is also a trick \n",
    "    #keep track of how many episodes in our batch\n",
    "    episode_num = 1\n",
    "\n",
    "    #launch a new episode\n",
    "    game.new_episode()\n",
    "\n",
    "    #get a new state\n",
    "    state = game.get_state().screen_buffer\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "    while True:\n",
    "        #run state through policy & calculate action\n",
    "        action_probability_distribution = sess.run(PGNetwork.action_distribution, \n",
    "                                                    feed_dict = {PGNetwork.inputs_: state.reshape(1, *state_size)})\n",
    "\n",
    "        # stochastic policy\n",
    "        action = np.random.choice(range(action_probability_distribution.shape[1],\n",
    "                                  p = action_probability_distribution.ravel()))\n",
    "                                  #select the action with respec to the action probab.\n",
    "        \n",
    "        action = possible_actions[action]\n",
    "\n",
    "        #perform action\n",
    "        reward = game.make_action(action)\n",
    "        done = game.is_episode_finished()\n",
    "\n",
    "        #store results\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards_of_episode.append(reward)\n",
    "\n",
    "        if done:\n",
    "            next_state = np.zeros((84, 84), dtype = np.int)\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "            #append the rewards_of_batch\n",
    "            rewards_of_batch.append(rewards_of_episode)\n",
    "\n",
    "            #calculate gamma Gt\n",
    "            discounted_rewards.append(discount_and_normalize_rewards(rewards_of_episode))\n",
    "\n",
    "            #if the number of rewards_of_batch > batch_size stop the minibatch creation\n",
    "            # because we have sufficient number of episode mb\n",
    "            # we want the entire episode (Monte Carlo)\n",
    "            # so we cant check that condition for earch step, but only if an episode is finished\n",
    "\n",
    "            if len(np.concatenate(rewards_of_batch)) > batch_size:\n",
    "                break\n",
    "\n",
    "            #reset the transition stores\n",
    "            rewards_of_episode = []\n",
    "\n",
    "            episode_num += 1 \n",
    "\n",
    "            game.new_episode()\n",
    "\n",
    "            state = game.get_state().screen_buffer\n",
    "\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "        else:\n",
    "            # if not done, the next_state becomes the current one\n",
    "            next_state = game.get_state().screen_buffer\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            state = next_state\n",
    "\n",
    "        return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards), episode_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the Neural Network\n",
    "- Init the weights\n",
    "- Init the environment\n",
    "- maxReward = 0 #keep track of max reward\n",
    "- for epochs in range(num_epochs):\n",
    "    - get batches\n",
    "    - optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allRewards = []\n",
    "\n",
    "total_rewards = 0\n",
    "maximumRewardsRecorded = 0\n",
    "mean_reward_total = []\n",
    "epoch = 1\n",
    "average_reward = []\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training:\n",
    "    #load the model\n",
    "    # saver.restore(sess, \"./models/model.ckpt\")\n",
    "    \n",
    "    while epoch < num_epochs + 1:\n",
    "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, nb_episodes_mb = make_batch(batch_size, stacked_frames)\n",
    "        \n",
    "        #analytics part\n",
    "        # calculate the total reward of the batch\n",
    "        total_reward_of_that_batch = np.sum(rewards_of_batch)\n",
    "        allRewards.append(total_reward_of_that_batch)\n",
    "        \n",
    "        #calculate the mean reward of the batch\n",
    "        mean_reward_of_that_batch = np.divide(total_reward_of_that_batch, nb_episodes_mb)\n",
    "        mean_reward_total.append(mean_reward_of_that_batch)\n",
    "        \n",
    "        #calculate the average reward of all training\n",
    "        #mean_reward_of_that_batch / epoch\n",
    "        average_reward_of_all_training = np.divide(np.sum(mean_reward_total), epoch)\n",
    "        \n",
    "        #calculate max reward recorded\n",
    "        maximumRewardsRecorded = np.amax(allRewards)\n",
    "        \n",
    "        print(\"==========================================\")\n",
    "        print(\"Epoch: \", epoch, \"/\", num_epochs)\n",
    "        print(\"-----------\")\n",
    "        print(\"Number of training episodes: {}\".format(nb_episodes_mb))\n",
    "        print(\"Total reward: {}\".format(total_reward_of_that_batch, nb_episodes_mb))\n",
    "        print(\"Mean Reward of that batch {}\".format(mean_reward_of_that_batch))\n",
    "        print(\"Average Reward of all training: {}\".format(average_reward_of_all_training))\n",
    "        print(\"Max reward for a batch so far: {}\".format(maximumRewardRecorded))\n",
    "        \n",
    "        #feedforward, gradient and backpropagation\n",
    "        loss_, _ = sess.run([policy_gradient_net.loss, policy_gradient_net.train_opt], feed_dict = {policy_gradient_net.inputs_: states_mb.reshape((len(states_mb), 84, 84, 4)),\n",
    "                                                            policy_gradient_net.actions: actions_mb,\n",
    "                                                                     policy_gradient_net.discounted_episode_rewards_: discounted_rewards_mb \n",
    "                                                                    })\n",
    "        print(\"Training Loss: {}\".format(loss_))\n",
    "        \n",
    "        summary = sess.run(write_op, feed_dict={policy_gradient_net.inputs_: states_mb.reshape((len(states_mb), 84,84,4)),\n",
    "                                                            policy_gradient_net.actions: actions_mb,\n",
    "                                                                     policy_gradient_net.discounted_episode_rewards_: discounted_rewards_mb,\n",
    "                                                                    policy_gradient_net.mean_reward_: mean_reward_of_that_batch\n",
    "                                                                   })\n",
    "        \n",
    "        writer.add_summary(summary, epoch)\n",
    "        writer.flush()\n",
    "        \n",
    "        # save model\n",
    "        if epoch%10 == 0:\n",
    "            saver.save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model saved\")\n",
    "        epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
