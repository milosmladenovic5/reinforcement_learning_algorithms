{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "from environment_creation import create_environment\n",
    "import policy_gradient_network as pgn\n",
    "from frame_preprocessing import preprocess_frame\n",
    "from frames_stacking import stack_frames \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()\n",
    "\n",
    "stack_size = 4\n",
    "stacked_frames = deque([np.zeros((84,84), dtype = np.int) for i in range(stack_size)], maxlen = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards, gamma):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "\n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################\n",
    "###Environment hyperparameters\n",
    "state_size = [84, 84, 4] #our input is a stack of 4 frames , 84x84\n",
    "action_size = game.get_available_buttons_size() # 3 possible actions, turn left, turn right, go forward\n",
    "stack_size = 4 #how many frames are stacked together\n",
    "\n",
    "#deep learning model hyperparameters\n",
    "learning_rate = 0.002\n",
    "num_epochs = 1000\n",
    "\n",
    "batch_size = 2000\n",
    "gamma = 0.95 #discount rate\n",
    "\n",
    "training = True\n",
    "######################################\n",
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "PGNetwork = pgn.PGNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup tensorflow writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/pg/test\")\n",
    "\n",
    "#losses\n",
    "tf.summary.scalar(\"Loss\", PGNetwork.loss)\n",
    "\n",
    "tf.summary.scalar(\"Reward_mean\", PGNetwork.mean_reward_ )\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now is the time to train the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll create batches.\n",
    "These batches contains episodes (their number depends on how many rewards we collect: for instance if we have episodes with only 10 rewards we can put batch_size/10 episodes\n",
    "\n",
    "    Make a batch\n",
    "        For each step:\n",
    "            Choose action a\n",
    "            Perform action a\n",
    "            Store s, a, r\n",
    "            If done:\n",
    "                Calculate sum reward\n",
    "                Calculate gamma Gt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(batch_size, stacked_frames):\n",
    "    # Initialize lists: states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards\n",
    "    states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards = [], [], [], [], []\n",
    "    \n",
    "    # Reward of batch is also a trick to keep track of how many timestep we made.\n",
    "    # We use to to verify at the end of each episode if > batch_size or not.\n",
    "    \n",
    "    # Keep track of how many episodes in our batch (useful when we'll need to calculate the average reward per episode)\n",
    "    episode_num  = 1\n",
    "    \n",
    "    # Launch a new episode\n",
    "    game.new_episode()\n",
    "        \n",
    "    # Get a new state\n",
    "    state = game.get_state().screen_buffer\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, True, stack_size)\n",
    "\n",
    "    while True:\n",
    "        # Run State Through Policy & Calculate Action\n",
    "        action_probability_distribution = sess.run(PGNetwork.action_distribution, \n",
    "                                                   feed_dict={PGNetwork.inputs_: state.reshape(1, *state_size)})\n",
    "        \n",
    "        # REMEMBER THAT WE ARE IN A STOCHASTIC POLICY SO WE DON'T ALWAYS TAKE THE ACTION WITH THE HIGHEST PROBABILITY\n",
    "        # (For instance if the action with the best probability for state S is a1 with 70% chances, there is\n",
    "        #30% chance that we take action a2)\n",
    "        action = np.random.choice(range(action_probability_distribution.shape[1]), \n",
    "                                  p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "        action = possible_actions[action]\n",
    "\n",
    "        # Perform action\n",
    "        reward = game.make_action(action)\n",
    "        done = game.is_episode_finished()\n",
    "\n",
    "        # Store results\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards_of_episode.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            # The episode ends so no next state\n",
    "            next_state = np.zeros((84, 84), dtype=np.int)\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False, stack_size)\n",
    "            \n",
    "            # Append the rewards_of_batch to reward_of_episode\n",
    "            rewards_of_batch.append(rewards_of_episode)\n",
    "            \n",
    "            # Calculate gamma Gt\n",
    "            discounted_rewards.append(discount_and_normalize_rewards(rewards_of_episode, gamma))\n",
    "           \n",
    "            # If the number of rewards_of_batch > batch_size stop the minibatch creation\n",
    "            # (Because we have sufficient number of episode mb)\n",
    "            # Remember that we put this condition here, because we want entire episode (Monte Carlo)\n",
    "            # so we can't check that condition for each step but only if an episode is finished\n",
    "            if len(np.concatenate(rewards_of_batch)) > batch_size:\n",
    "                break\n",
    "                \n",
    "            # Reset the transition stores\n",
    "            rewards_of_episode = []\n",
    "            \n",
    "            # Add episode\n",
    "            episode_num += 1\n",
    "            \n",
    "            # Start a new episode\n",
    "            game.new_episode()\n",
    "\n",
    "            # First we need a state\n",
    "            state = game.get_state().screen_buffer\n",
    "\n",
    "            # Stack the frames\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True, stack_size)\n",
    "         \n",
    "        else:\n",
    "            # If not done, the next_state become the current state\n",
    "            next_state = game.get_state().screen_buffer\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False, stack_size)\n",
    "            state = next_state\n",
    "                         \n",
    "    return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards), episode_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the Neural Network\n",
    "- Init the weights\n",
    "- Init the environment\n",
    "- maxReward = 0 #keep track of max reward\n",
    "- for epochs in range(num_epochs):\n",
    "    - get batches\n",
    "    - optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  1 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1840.0\n",
      "Mean Reward of that batch 460.0\n",
      "Average Reward of all training: 460.0\n",
      "Max reward for a batch so far: 1840.0\n",
      "Training Loss: -0.0076517038978636265\n",
      "==========================================\n",
      "Epoch:  2 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1680.0\n",
      "Mean Reward of that batch 420.0\n",
      "Average Reward of all training: 440.0\n",
      "Max reward for a batch so far: 1840.0\n",
      "Training Loss: -0.010891149751842022\n",
      "==========================================\n",
      "Epoch:  3 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1840.0\n",
      "Mean Reward of that batch 460.0\n",
      "Average Reward of all training: 446.6666666666667\n",
      "Max reward for a batch so far: 1840.0\n",
      "Training Loss: -0.014711610041558743\n",
      "==========================================\n",
      "Epoch:  4 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1968.0\n",
      "Mean Reward of that batch 492.0\n",
      "Average Reward of all training: 458.0\n",
      "Max reward for a batch so far: 1968.0\n",
      "Training Loss: -0.0057493350468575954\n",
      "==========================================\n",
      "Epoch:  5 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1904.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 461.6\n",
      "Max reward for a batch so far: 1968.0\n",
      "Training Loss: 0.013580620288848877\n",
      "==========================================\n",
      "Epoch:  6 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2064.0\n",
      "Mean Reward of that batch 516.0\n",
      "Average Reward of all training: 470.6666666666667\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: -0.012366589158773422\n",
      "==========================================\n",
      "Epoch:  7 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1872.0\n",
      "Mean Reward of that batch 468.0\n",
      "Average Reward of all training: 470.2857142857143\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: -0.016132375225424767\n",
      "==========================================\n",
      "Epoch:  8 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1680.0\n",
      "Mean Reward of that batch 420.0\n",
      "Average Reward of all training: 464.0\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: -0.0133976386860013\n",
      "==========================================\n",
      "Epoch:  9 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1712.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 460.0\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: 0.024504095315933228\n",
      "==========================================\n",
      "Epoch:  10 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1968.0\n",
      "Mean Reward of that batch 492.0\n",
      "Average Reward of all training: 463.2\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: -0.04246050491929054\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  11 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1872.0\n",
      "Mean Reward of that batch 468.0\n",
      "Average Reward of all training: 463.6363636363636\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: -0.052199967205524445\n",
      "==========================================\n",
      "Epoch:  12 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1772.0\n",
      "Mean Reward of that batch 354.4\n",
      "Average Reward of all training: 454.5333333333333\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: -0.07306401431560516\n",
      "==========================================\n",
      "Epoch:  13 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1844.0\n",
      "Mean Reward of that batch 614.6666666666666\n",
      "Average Reward of all training: 466.85128205128206\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: -0.03334096819162369\n",
      "==========================================\n",
      "Epoch:  14 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1680.0\n",
      "Mean Reward of that batch 420.0\n",
      "Average Reward of all training: 463.5047619047619\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: 0.02154506742954254\n",
      "==========================================\n",
      "Epoch:  15 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1996.0\n",
      "Mean Reward of that batch 399.2\n",
      "Average Reward of all training: 459.21777777777777\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: 0.012127644382417202\n",
      "==========================================\n",
      "Epoch:  16 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1648.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 456.26666666666665\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: -0.00024169310927391052\n",
      "==========================================\n",
      "Epoch:  17 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1964.0\n",
      "Mean Reward of that batch 392.8\n",
      "Average Reward of all training: 452.5333333333333\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: 0.015019454061985016\n",
      "==========================================\n",
      "Epoch:  18 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1712.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 451.1703703703704\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: 0.02830083854496479\n",
      "==========================================\n",
      "Epoch:  19 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2064.0\n",
      "Mean Reward of that batch 516.0\n",
      "Average Reward of all training: 454.5824561403508\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: 0.003980438224971294\n",
      "==========================================\n",
      "Epoch:  20 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1996.0\n",
      "Mean Reward of that batch 399.2\n",
      "Average Reward of all training: 451.81333333333333\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: 0.010962415486574173\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  21 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1936.0\n",
      "Mean Reward of that batch 484.0\n",
      "Average Reward of all training: 453.3460317460317\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: -0.00023389515990857035\n",
      "==========================================\n",
      "Epoch:  22 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1648.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 451.46666666666664\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: 0.02352815866470337\n",
      "==========================================\n",
      "Epoch:  23 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1712.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 450.4463768115942\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: 0.028683453798294067\n",
      "==========================================\n",
      "Epoch:  24 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2128.0\n",
      "Mean Reward of that batch 532.0\n",
      "Average Reward of all training: 453.84444444444443\n",
      "Max reward for a batch so far: 2128.0\n",
      "Training Loss: 0.02835732512176037\n",
      "==========================================\n",
      "Epoch:  25 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1936.0\n",
      "Mean Reward of that batch 484.0\n",
      "Average Reward of all training: 455.05066666666664\n",
      "Max reward for a batch so far: 2128.0\n",
      "Training Loss: -0.010247544385492802\n",
      "==========================================\n",
      "Epoch:  26 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2192.0\n",
      "Mean Reward of that batch 548.0\n",
      "Average Reward of all training: 458.625641025641\n",
      "Max reward for a batch so far: 2192.0\n",
      "Training Loss: 0.00797234009951353\n",
      "==========================================\n",
      "Epoch:  27 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2360.0\n",
      "Mean Reward of that batch 1180.0\n",
      "Average Reward of all training: 485.3432098765432\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: 0.018636628985404968\n",
      "==========================================\n",
      "Epoch:  28 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1748.0\n",
      "Mean Reward of that batch 582.6666666666666\n",
      "Average Reward of all training: 488.8190476190476\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: 0.03027176856994629\n",
      "==========================================\n",
      "Epoch:  29 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2000.0\n",
      "Mean Reward of that batch 500.0\n",
      "Average Reward of all training: 489.2045977011494\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: 0.007362270262092352\n",
      "==========================================\n",
      "Epoch:  30 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1744.0\n",
      "Mean Reward of that batch 436.0\n",
      "Average Reward of all training: 487.4311111111111\n",
      "Max reward for a batch so far: 2360.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.013952933251857758\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  31 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1712.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 485.51397849462364\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: -0.13070173561573029\n",
      "==========================================\n",
      "Epoch:  32 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2064.0\n",
      "Mean Reward of that batch 516.0\n",
      "Average Reward of all training: 486.4666666666667\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: 0.007631264626979828\n",
      "==========================================\n",
      "Epoch:  33 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1780.0\n",
      "Mean Reward of that batch 593.3333333333334\n",
      "Average Reward of all training: 489.70505050505056\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: 0.036599233746528625\n",
      "==========================================\n",
      "Epoch:  34 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2320.0\n",
      "Mean Reward of that batch 580.0\n",
      "Average Reward of all training: 492.3607843137256\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: -0.0027234386652708054\n",
      "==========================================\n",
      "Epoch:  35 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1840.0\n",
      "Mean Reward of that batch 460.0\n",
      "Average Reward of all training: 491.4361904761906\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: -0.03405342623591423\n",
      "==========================================\n",
      "Epoch:  36 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 2292.0\n",
      "Mean Reward of that batch 764.0\n",
      "Average Reward of all training: 499.0074074074075\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: -0.030873192474246025\n",
      "==========================================\n",
      "Epoch:  37 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 2220.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 497.5207207207208\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: 0.04438580572605133\n",
      "==========================================\n",
      "Epoch:  38 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1904.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 496.9543859649124\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: 0.035806238651275635\n",
      "==========================================\n",
      "Epoch:  39 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1840.0\n",
      "Mean Reward of that batch 460.0\n",
      "Average Reward of all training: 496.0068376068377\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: -0.009762478061020374\n",
      "==========================================\n",
      "Epoch:  40 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1776.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 494.7066666666666\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: -0.031927499920129776\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  41 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2544.0\n",
      "Mean Reward of that batch 636.0\n",
      "Average Reward of all training: 498.1528455284552\n",
      "Max reward for a batch so far: 2544.0\n",
      "Training Loss: 0.011976877227425575\n",
      "==========================================\n",
      "Epoch:  42 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 2988.0\n",
      "Mean Reward of that batch 996.0\n",
      "Average Reward of all training: 510.0063492063491\n",
      "Max reward for a batch so far: 2988.0\n",
      "Training Loss: -0.015270262025296688\n",
      "==========================================\n",
      "Epoch:  43 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 2260.0\n",
      "Mean Reward of that batch 753.3333333333334\n",
      "Average Reward of all training: 515.6651162790696\n",
      "Max reward for a batch so far: 2988.0\n",
      "Training Loss: -0.03349686414003372\n",
      "==========================================\n",
      "Epoch:  44 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 3400.0\n",
      "Mean Reward of that batch 850.0\n",
      "Average Reward of all training: 523.2636363636362\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: 0.024260465055704117\n",
      "==========================================\n",
      "Epoch:  45 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1808.0\n",
      "Mean Reward of that batch 452.0\n",
      "Average Reward of all training: 521.6799999999998\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.10309715569019318\n",
      "==========================================\n",
      "Epoch:  46 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1880.0\n",
      "Mean Reward of that batch 940.0\n",
      "Average Reward of all training: 530.7739130434782\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.005431214347481728\n",
      "==========================================\n",
      "Epoch:  47 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2288.0\n",
      "Mean Reward of that batch 572.0\n",
      "Average Reward of all training: 531.6510638297872\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.0686018317937851\n",
      "==========================================\n",
      "Epoch:  48 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1800.0\n",
      "Mean Reward of that batch 300.0\n",
      "Average Reward of all training: 526.8249999999999\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: 0.009719985537230968\n",
      "==========================================\n",
      "Epoch:  49 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1680.0\n",
      "Mean Reward of that batch 420.0\n",
      "Average Reward of all training: 524.6448979591836\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: 0.02617698907852173\n",
      "==========================================\n",
      "Epoch:  50 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2544.0\n",
      "Mean Reward of that batch 1272.0\n",
      "Average Reward of all training: 539.592\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: 0.05654479190707207\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  51 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1880.0\n",
      "Mean Reward of that batch 940.0\n",
      "Average Reward of all training: 547.4431372549019\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: 0.02413313277065754\n",
      "==========================================\n",
      "Epoch:  52 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1772.0\n",
      "Mean Reward of that batch 354.4\n",
      "Average Reward of all training: 543.7307692307693\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.007208424620330334\n",
      "==========================================\n",
      "Epoch:  53 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1868.0\n",
      "Mean Reward of that batch 373.6\n",
      "Average Reward of all training: 540.5207547169811\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.05024634301662445\n",
      "==========================================\n",
      "Epoch:  54 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2000.0\n",
      "Mean Reward of that batch 500.0\n",
      "Average Reward of all training: 539.7703703703703\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: 0.052588239312171936\n",
      "==========================================\n",
      "Epoch:  55 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2128.0\n",
      "Mean Reward of that batch 532.0\n",
      "Average Reward of all training: 539.6290909090909\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.003507493529468775\n",
      "==========================================\n",
      "Epoch:  56 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1772.0\n",
      "Mean Reward of that batch 354.4\n",
      "Average Reward of all training: 536.3214285714286\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.008917996659874916\n",
      "==========================================\n",
      "Epoch:  57 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1644.0\n",
      "Mean Reward of that batch 328.8\n",
      "Average Reward of all training: 532.680701754386\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.021917980164289474\n",
      "==========================================\n",
      "Epoch:  58 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1644.0\n",
      "Mean Reward of that batch 328.8\n",
      "Average Reward of all training: 529.1655172413792\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.028399411588907242\n",
      "==========================================\n",
      "Epoch:  59 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1644.0\n",
      "Mean Reward of that batch 328.8\n",
      "Average Reward of all training: 525.7694915254237\n",
      "Max reward for a batch so far: 3400.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: -0.028602292761206627\n",
      "==========================================\n",
      "Epoch:  60 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1516.0\n",
      "Mean Reward of that batch 303.2\n",
      "Average Reward of all training: 522.06\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.05140203982591629\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  61 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1580.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 518.6819672131147\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.002965521765872836\n",
      "==========================================\n",
      "Epoch:  62 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1612.0\n",
      "Mean Reward of that batch 322.4\n",
      "Average Reward of all training: 515.516129032258\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.053374722599983215\n",
      "==========================================\n",
      "Epoch:  63 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1644.0\n",
      "Mean Reward of that batch 328.8\n",
      "Average Reward of all training: 512.552380952381\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.03388804942369461\n",
      "==========================================\n",
      "Epoch:  64 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1548.0\n",
      "Mean Reward of that batch 309.6\n",
      "Average Reward of all training: 509.38125\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.04100339859724045\n",
      "==========================================\n",
      "Epoch:  65 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1740.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 506.89846153846156\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.012039457447826862\n",
      "==========================================\n",
      "Epoch:  66 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1612.0\n",
      "Mean Reward of that batch 322.4\n",
      "Average Reward of all training: 504.1030303030303\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.03088325448334217\n",
      "==========================================\n",
      "Epoch:  67 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1772.0\n",
      "Mean Reward of that batch 354.4\n",
      "Average Reward of all training: 501.868656716418\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.00439809076488018\n",
      "==========================================\n",
      "Epoch:  68 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1772.0\n",
      "Mean Reward of that batch 354.4\n",
      "Average Reward of all training: 499.7000000000001\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.041029445827007294\n",
      "==========================================\n",
      "Epoch:  69 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1708.0\n",
      "Mean Reward of that batch 341.6\n",
      "Average Reward of all training: 497.408695652174\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: 0.016478514298796654\n",
      "==========================================\n",
      "Epoch:  70 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1612.0\n",
      "Mean Reward of that batch 322.4\n",
      "Average Reward of all training: 494.9085714285715\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: 0.034737732261419296\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  71 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1644.0\n",
      "Mean Reward of that batch 328.8\n",
      "Average Reward of all training: 492.56901408450716\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: 0.00899332296103239\n",
      "==========================================\n",
      "Epoch:  72 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1804.0\n",
      "Mean Reward of that batch 360.8\n",
      "Average Reward of all training: 490.7388888888888\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: 0.04717564582824707\n",
      "==========================================\n",
      "Epoch:  73 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1644.0\n",
      "Mean Reward of that batch 328.8\n",
      "Average Reward of all training: 488.52054794520546\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.037102002650499344\n",
      "==========================================\n",
      "Epoch:  74 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1516.0\n",
      "Mean Reward of that batch 303.2\n",
      "Average Reward of all training: 486.01621621621615\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.04247259721159935\n",
      "==========================================\n",
      "Epoch:  75 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1708.0\n",
      "Mean Reward of that batch 341.6\n",
      "Average Reward of all training: 484.0906666666666\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.05383967608213425\n",
      "==========================================\n",
      "Epoch:  76 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1516.0\n",
      "Mean Reward of that batch 303.2\n",
      "Average Reward of all training: 481.71052631578937\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.02427307330071926\n",
      "==========================================\n",
      "Epoch:  77 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1676.0\n",
      "Mean Reward of that batch 335.2\n",
      "Average Reward of all training: 479.8077922077921\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.07672141492366791\n",
      "==========================================\n",
      "Epoch:  78 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1644.0\n",
      "Mean Reward of that batch 328.8\n",
      "Average Reward of all training: 477.8717948717948\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.00968723464757204\n",
      "==========================================\n",
      "Epoch:  79 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1580.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 475.8227848101265\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: 0.06441059708595276\n",
      "==========================================\n",
      "Epoch:  80 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1836.0\n",
      "Mean Reward of that batch 367.2\n",
      "Average Reward of all training: 474.465\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.011384895071387291\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  81 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1804.0\n",
      "Mean Reward of that batch 360.8\n",
      "Average Reward of all training: 473.0617283950617\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.052425120025873184\n",
      "==========================================\n",
      "Epoch:  82 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1744.0\n",
      "Mean Reward of that batch 436.0\n",
      "Average Reward of all training: 472.609756097561\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: 0.030198296532034874\n",
      "==========================================\n",
      "Epoch:  83 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 2356.0\n",
      "Mean Reward of that batch 785.3333333333334\n",
      "Average Reward of all training: 476.37751004016064\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.01669071614742279\n",
      "==========================================\n",
      "Epoch:  84 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2192.0\n",
      "Mean Reward of that batch 548.0\n",
      "Average Reward of all training: 477.23015873015873\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.03815358504652977\n",
      "==========================================\n",
      "Epoch:  85 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1964.0\n",
      "Mean Reward of that batch 392.8\n",
      "Average Reward of all training: 476.2368627450981\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.03440476208925247\n",
      "==========================================\n",
      "Epoch:  86 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1868.0\n",
      "Mean Reward of that batch 373.6\n",
      "Average Reward of all training: 475.04341085271324\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: 0.046019770205020905\n",
      "==========================================\n",
      "Epoch:  87 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1748.0\n",
      "Mean Reward of that batch 582.6666666666666\n",
      "Average Reward of all training: 476.280459770115\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: 0.07932423055171967\n",
      "==========================================\n",
      "Epoch:  88 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1972.0\n",
      "Mean Reward of that batch 657.3333333333334\n",
      "Average Reward of all training: 478.3378787878788\n",
      "Max reward for a batch so far: 3400.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.048488933593034744\n",
      "==========================================\n",
      "Epoch:  89 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1780.0\n",
      "Mean Reward of that batch 593.3333333333334\n",
      "Average Reward of all training: 479.62996254681656\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: 0.0163747426122427\n",
      "==========================================\n",
      "Epoch:  90 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1680.0\n",
      "Mean Reward of that batch 420.0\n",
      "Average Reward of all training: 478.96740740740745\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.022535840049386024\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  91 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 3080.0\n",
      "Mean Reward of that batch 770.0\n",
      "Average Reward of all training: 482.16556776556786\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.06412021070718765\n",
      "==========================================\n",
      "Epoch:  92 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 2132.0\n",
      "Mean Reward of that batch 710.6666666666666\n",
      "Average Reward of all training: 484.64927536231886\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: 0.06411831080913544\n",
      "==========================================\n",
      "Epoch:  93 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 700.0\n",
      "Average Reward of all training: 486.9648745519714\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: 0.04618333280086517\n",
      "==========================================\n",
      "Epoch:  94 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1936.0\n",
      "Mean Reward of that batch 484.0\n",
      "Average Reward of all training: 486.9333333333334\n",
      "Max reward for a batch so far: 3400.0\n",
      "Training Loss: -0.031130904331803322\n",
      "==========================================\n",
      "Epoch:  95 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 3820.0\n",
      "Mean Reward of that batch 1273.3333333333333\n",
      "Average Reward of all training: 495.2112280701755\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.2676951289176941\n",
      "==========================================\n",
      "Epoch:  96 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 700.0\n",
      "Average Reward of all training: 497.34444444444443\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.06619973480701447\n",
      "==========================================\n",
      "Epoch:  97 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1844.0\n",
      "Mean Reward of that batch 614.6666666666666\n",
      "Average Reward of all training: 498.55395189003434\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.185876727104187\n",
      "==========================================\n",
      "Epoch:  98 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1708.0\n",
      "Mean Reward of that batch 341.6\n",
      "Average Reward of all training: 496.9523809523809\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.6322969794273376\n",
      "==========================================\n",
      "Epoch:  99 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1644.0\n",
      "Mean Reward of that batch 328.8\n",
      "Average Reward of all training: 495.25387205387204\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.7126114964485168\n",
      "==========================================\n",
      "Epoch:  100 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1844.0\n",
      "Mean Reward of that batch 614.6666666666666\n",
      "Average Reward of all training: 496.448\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.3025148808956146\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  101 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 2420.0\n",
      "Mean Reward of that batch 806.6666666666666\n",
      "Average Reward of all training: 499.51947194719463\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.7176750302314758\n",
      "==========================================\n",
      "Epoch:  102 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2608.0\n",
      "Mean Reward of that batch 1304.0\n",
      "Average Reward of all training: 507.40653594771237\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.5592434406280518\n",
      "==========================================\n",
      "Epoch:  103 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 522.8686084142394\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -11.463056564331055\n",
      "==========================================\n",
      "Epoch:  104 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 538.0333333333333\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.5301743745803833\n",
      "==========================================\n",
      "Epoch:  105 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 552.9092063492063\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.4654976427555084\n",
      "==========================================\n",
      "Epoch:  106 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2704.0\n",
      "Mean Reward of that batch 1352.0\n",
      "Average Reward of all training: 560.4477987421383\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.271688461303711\n",
      "==========================================\n",
      "Epoch:  107 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 574.836137071651\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.5961411595344543\n",
      "==========================================\n",
      "Epoch:  108 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 588.958024691358\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.6472011804580688\n",
      "==========================================\n",
      "Epoch:  109 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 602.8207951070336\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.9681825637817383\n",
      "==========================================\n",
      "Epoch:  110 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 616.4315151515151\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.7585033774375916\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  111 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 629.7969969969969\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.06520987302064896\n",
      "==========================================\n",
      "Epoch:  112 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 3276.0\n",
      "Mean Reward of that batch 1092.0\n",
      "Average Reward of all training: 633.9238095238096\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.7022680640220642\n",
      "==========================================\n",
      "Epoch:  113 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 646.8979351032449\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.105656199157238\n",
      "==========================================\n",
      "Epoch:  114 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1936.0\n",
      "Mean Reward of that batch 484.0\n",
      "Average Reward of all training: 645.4690058479533\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.6849592924118042\n",
      "==========================================\n",
      "Epoch:  115 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1912.0\n",
      "Mean Reward of that batch 956.0\n",
      "Average Reward of all training: 648.1692753623189\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -1.5558539628982544\n",
      "==========================================\n",
      "Epoch:  116 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1836.0\n",
      "Mean Reward of that batch 367.2\n",
      "Average Reward of all training: 645.7471264367816\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.5467553734779358\n",
      "==========================================\n",
      "Epoch:  117 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1580.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 642.9287749287749\n",
      "Max reward for a batch so far: 3820.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.12492426484823227\n",
      "==========================================\n",
      "Epoch:  118 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1932.0\n",
      "Mean Reward of that batch 386.4\n",
      "Average Reward of all training: 640.754802259887\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.12698735296726227\n",
      "==========================================\n",
      "Epoch:  119 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1900.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 638.5635854341737\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.07102356106042862\n",
      "==========================================\n",
      "Epoch:  120 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1616.0\n",
      "Mean Reward of that batch 404.0\n",
      "Average Reward of all training: 636.6088888888888\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.05223986506462097\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  121 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1712.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 634.8848484848485\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.06975892186164856\n",
      "==========================================\n",
      "Epoch:  122 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1804.0\n",
      "Mean Reward of that batch 360.8\n",
      "Average Reward of all training: 632.6382513661202\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.026167718693614006\n",
      "==========================================\n",
      "Epoch:  123 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1804.0\n",
      "Mean Reward of that batch 360.8\n",
      "Average Reward of all training: 630.4281842818428\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.00972839817404747\n",
      "==========================================\n",
      "Epoch:  124 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1996.0\n",
      "Mean Reward of that batch 399.2\n",
      "Average Reward of all training: 628.5634408602151\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.007603556849062443\n",
      "==========================================\n",
      "Epoch:  125 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1712.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 626.9589333333333\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.01034348364919424\n",
      "==========================================\n",
      "Epoch:  126 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1804.0\n",
      "Mean Reward of that batch 360.8\n",
      "Average Reward of all training: 624.8465608465609\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.020633529871702194\n",
      "==========================================\n",
      "Epoch:  127 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1676.0\n",
      "Mean Reward of that batch 335.2\n",
      "Average Reward of all training: 622.5658792650919\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.017151594161987305\n",
      "==========================================\n",
      "Epoch:  128 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1616.0\n",
      "Mean Reward of that batch 404.0\n",
      "Average Reward of all training: 620.8583333333333\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.005710836034268141\n",
      "==========================================\n",
      "Epoch:  129 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1872.0\n",
      "Mean Reward of that batch 468.0\n",
      "Average Reward of all training: 619.6733850129199\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.04615284875035286\n",
      "==========================================\n",
      "Epoch:  130 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1708.0\n",
      "Mean Reward of that batch 341.6\n",
      "Average Reward of all training: 617.5343589743591\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.00923817791044712\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  131 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1804.0\n",
      "Mean Reward of that batch 360.8\n",
      "Average Reward of all training: 615.5745547073791\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.012965815141797066\n",
      "==========================================\n",
      "Epoch:  132 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1900.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 613.7898989898989\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.00291094440035522\n",
      "==========================================\n",
      "Epoch:  133 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1744.0\n",
      "Mean Reward of that batch 436.0\n",
      "Average Reward of all training: 612.4531328320802\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.02042948268353939\n",
      "==========================================\n",
      "Epoch:  134 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1708.0\n",
      "Mean Reward of that batch 341.6\n",
      "Average Reward of all training: 610.43184079602\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.007376478984951973\n",
      "==========================================\n",
      "Epoch:  135 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1712.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 609.0804938271605\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.015914849936962128\n",
      "==========================================\n",
      "Epoch:  136 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1516.0\n",
      "Mean Reward of that batch 303.2\n",
      "Average Reward of all training: 606.8313725490198\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0014221150195226073\n",
      "==========================================\n",
      "Epoch:  137 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1516.0\n",
      "Mean Reward of that batch 303.2\n",
      "Average Reward of all training: 604.6150851581508\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.006498503033071756\n",
      "==========================================\n",
      "Epoch:  138 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1744.0\n",
      "Mean Reward of that batch 436.0\n",
      "Average Reward of all training: 603.3932367149758\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.02739003300666809\n",
      "==========================================\n",
      "Epoch:  139 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1680.0\n",
      "Mean Reward of that batch 420.0\n",
      "Average Reward of all training: 602.0738609112709\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.015479172579944134\n",
      "==========================================\n",
      "Epoch:  140 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1996.0\n",
      "Mean Reward of that batch 399.2\n",
      "Average Reward of all training: 600.624761904762\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0010593521874397993\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  141 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1712.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 599.4004728132388\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.00047212495701387525\n",
      "==========================================\n",
      "Epoch:  142 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1616.0\n",
      "Mean Reward of that batch 404.0\n",
      "Average Reward of all training: 598.0244131455399\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.00038945741835050285\n",
      "==========================================\n",
      "Epoch:  143 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1712.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 596.8354312354313\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0005391329759731889\n",
      "==========================================\n",
      "Epoch:  144 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1516.0\n",
      "Mean Reward of that batch 303.2\n",
      "Average Reward of all training: 594.7962962962962\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.0024809623137116432\n",
      "==========================================\n",
      "Epoch:  145 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1708.0\n",
      "Mean Reward of that batch 341.6\n",
      "Average Reward of all training: 593.0501149425287\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.002924883272498846\n",
      "==========================================\n",
      "Epoch:  146 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1804.0\n",
      "Mean Reward of that batch 360.8\n",
      "Average Reward of all training: 591.4593607305935\n",
      "Max reward for a batch so far: 3820.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00037287946906872094\n",
      "==========================================\n",
      "Epoch:  147 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1516.0\n",
      "Mean Reward of that batch 303.2\n",
      "Average Reward of all training: 589.4984126984127\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.007514607161283493\n",
      "==========================================\n",
      "Epoch:  148 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1708.0\n",
      "Mean Reward of that batch 341.6\n",
      "Average Reward of all training: 587.8234234234234\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0008210219093598425\n",
      "==========================================\n",
      "Epoch:  149 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1708.0\n",
      "Mean Reward of that batch 341.6\n",
      "Average Reward of all training: 586.1709172259507\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0057106320746243\n",
      "==========================================\n",
      "Epoch:  150 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1644.0\n",
      "Mean Reward of that batch 328.8\n",
      "Average Reward of all training: 584.4551111111111\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0075381845235824585\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  151 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1900.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 583.1011037527594\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.002579300431534648\n",
      "==========================================\n",
      "Epoch:  152 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1804.0\n",
      "Mean Reward of that batch 360.8\n",
      "Average Reward of all training: 581.6385964912281\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.004395201802253723\n",
      "==========================================\n",
      "Epoch:  153 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1804.0\n",
      "Mean Reward of that batch 360.8\n",
      "Average Reward of all training: 580.1952069716775\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.004176019225269556\n",
      "==========================================\n",
      "Epoch:  154 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1516.0\n",
      "Mean Reward of that batch 303.2\n",
      "Average Reward of all training: 578.3965367965368\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0028653894551098347\n",
      "==========================================\n",
      "Epoch:  155 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1676.0\n",
      "Mean Reward of that batch 335.2\n",
      "Average Reward of all training: 576.8275268817204\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.006325447466224432\n",
      "==========================================\n",
      "Epoch:  156 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1708.0\n",
      "Mean Reward of that batch 341.6\n",
      "Average Reward of all training: 575.3196581196581\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.021293509751558304\n",
      "==========================================\n",
      "Epoch:  157 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1804.0\n",
      "Mean Reward of that batch 360.8\n",
      "Average Reward of all training: 573.9532908704882\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.005635593552142382\n",
      "==========================================\n",
      "Epoch:  158 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1708.0\n",
      "Mean Reward of that batch 341.6\n",
      "Average Reward of all training: 572.4827004219409\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.004037787672132254\n",
      "==========================================\n",
      "Epoch:  159 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1836.0\n",
      "Mean Reward of that batch 367.2\n",
      "Average Reward of all training: 571.1916142557651\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0011351261055096984\n",
      "==========================================\n",
      "Epoch:  160 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1900.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 569.9966666666667\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0024870866909623146\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  161 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1712.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 569.1146997929607\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0047537037171423435\n",
      "==========================================\n",
      "Epoch:  162 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1808.0\n",
      "Mean Reward of that batch 452.0\n",
      "Average Reward of all training: 568.3917695473251\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.005972582381218672\n",
      "==========================================\n",
      "Epoch:  163 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1516.0\n",
      "Mean Reward of that batch 303.2\n",
      "Average Reward of all training: 566.7648261758691\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.004026469308882952\n",
      "==========================================\n",
      "Epoch:  164 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 565.040650406504\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.023603111505508423\n",
      "==========================================\n",
      "Epoch:  165 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 563.3373737373737\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.03490517660975456\n",
      "==========================================\n",
      "Epoch:  166 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 561.6546184738955\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.06865408271551132\n",
      "==========================================\n",
      "Epoch:  167 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1800.0\n",
      "Mean Reward of that batch 300.0\n",
      "Average Reward of all training: 560.0878243512974\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.2706530690193176\n",
      "==========================================\n",
      "Epoch:  168 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 558.4444444444445\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.3385443091392517\n",
      "==========================================\n",
      "Epoch:  169 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 556.8205128205128\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.27998942136764526\n",
      "==========================================\n",
      "Epoch:  170 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 555.2156862745098\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.35381656885147095\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  171 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 553.6296296296297\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.04286952689290047\n",
      "==========================================\n",
      "Epoch:  172 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 552.062015503876\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.07694096118211746\n",
      "==========================================\n",
      "Epoch:  173 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 550.5125240847784\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.004519586451351643\n",
      "==========================================\n",
      "Epoch:  174 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 548.9808429118774\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.016719095408916473\n",
      "==========================================\n",
      "Epoch:  175 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1580.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 547.6495238095239\n",
      "Max reward for a batch so far: 3820.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.017114298418164253\n",
      "==========================================\n",
      "Epoch:  176 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1708.0\n",
      "Mean Reward of that batch 341.6\n",
      "Average Reward of all training: 546.4787878787879\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.04695768281817436\n",
      "==========================================\n",
      "Epoch:  177 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1804.0\n",
      "Mean Reward of that batch 360.8\n",
      "Average Reward of all training: 545.4297551789078\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0816003754734993\n",
      "==========================================\n",
      "Epoch:  178 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1804.0\n",
      "Mean Reward of that batch 360.8\n",
      "Average Reward of all training: 544.3925093632959\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.012833541259169579\n",
      "==========================================\n",
      "Epoch:  179 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1548.0\n",
      "Mean Reward of that batch 309.6\n",
      "Average Reward of all training: 543.0808193668529\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.01204012706875801\n",
      "==========================================\n",
      "Epoch:  180 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 541.6414814814815\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.008435739204287529\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  181 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 540.2180478821364\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.03203390911221504\n",
      "==========================================\n",
      "Epoch:  182 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1612.0\n",
      "Mean Reward of that batch 322.4\n",
      "Average Reward of all training: 539.0212454212455\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.013690907508134842\n",
      "==========================================\n",
      "Epoch:  183 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1644.0\n",
      "Mean Reward of that batch 328.8\n",
      "Average Reward of all training: 537.872495446266\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.016155065968632698\n",
      "==========================================\n",
      "Epoch:  184 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1616.0\n",
      "Mean Reward of that batch 404.0\n",
      "Average Reward of all training: 537.1449275362319\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.030403053387999535\n",
      "==========================================\n",
      "Epoch:  185 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1516.0\n",
      "Mean Reward of that batch 303.2\n",
      "Average Reward of all training: 535.8803603603603\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.07569322735071182\n",
      "==========================================\n",
      "Epoch:  186 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1580.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 534.6982078853047\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.06858565658330917\n",
      "==========================================\n",
      "Epoch:  187 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1832.0\n",
      "Mean Reward of that batch 305.3333333333333\n",
      "Average Reward of all training: 533.4716577540107\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.09052946418523788\n",
      "==========================================\n",
      "Epoch:  188 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1768.0\n",
      "Mean Reward of that batch 294.6666666666667\n",
      "Average Reward of all training: 532.2014184397163\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.015400116331875324\n",
      "==========================================\n",
      "Epoch:  189 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 530.8881834215167\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.03384409844875336\n",
      "==========================================\n",
      "Epoch:  190 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 529.5887719298246\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.013202137313783169\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  191 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 528.3029668411867\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.02643832564353943\n",
      "==========================================\n",
      "Epoch:  192 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 527.0305555555556\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 2.7491339551488636e-06\n",
      "==========================================\n",
      "Epoch:  193 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 525.7713298791019\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.000652928720228374\n",
      "==========================================\n",
      "Epoch:  194 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 524.5250859106529\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.6704539973488863e-08\n",
      "==========================================\n",
      "Epoch:  195 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 523.2916239316239\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 2.7450511197457672e-08\n",
      "==========================================\n",
      "Epoch:  196 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 522.0707482993197\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 6.709803710691631e-05\n",
      "==========================================\n",
      "Epoch:  197 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 520.8622673434857\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 6.262541152324275e-09\n",
      "==========================================\n",
      "Epoch:  198 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 519.6659932659933\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 5.943920555750992e-08\n",
      "==========================================\n",
      "Epoch:  199 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 518.4817420435511\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 2.487624328750826e-09\n",
      "==========================================\n",
      "Epoch:  200 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 517.3093333333334\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 5.678844900103286e-05\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  201 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 516.1485903814262\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  202 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 514.9993399339934\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 3.543025428598412e-09\n",
      "==========================================\n",
      "Epoch:  203 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 513.8614121510674\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 6.610487712777058e-10\n",
      "==========================================\n",
      "Epoch:  204 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 512.7346405228758\n",
      "Max reward for a batch so far: 3820.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.722204379106529e-09\n",
      "==========================================\n",
      "Epoch:  205 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 511.6188617886179\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 4.1750458645850586e-10\n",
      "==========================================\n",
      "Epoch:  206 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 510.5139158576052\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.356887602277368e-09\n",
      "==========================================\n",
      "Epoch:  207 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 509.4196457326892\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 2.6441904221741197e-09\n",
      "==========================================\n",
      "Epoch:  208 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 508.3358974358974\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 2.5746065279719232e-09\n",
      "==========================================\n",
      "Epoch:  209 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 507.2625199362041\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 9.915728238496513e-10\n",
      "==========================================\n",
      "Epoch:  210 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 506.199365079365\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.0611556078288231e-09\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  211 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 505.14628751974715\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.8265787771198916e-09\n",
      "==========================================\n",
      "Epoch:  212 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 504.103144654088\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 2.139706634096683e-09\n",
      "==========================================\n",
      "Epoch:  213 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 503.06979655712047\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.4786593061089093e-09\n",
      "==========================================\n",
      "Epoch:  214 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 502.0461059190031\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.7743909674905467e-09\n",
      "==========================================\n",
      "Epoch:  215 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 501.03193798449604\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 8.449856068182271e-07\n",
      "==========================================\n",
      "Epoch:  216 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 500.0271604938271\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 3.305237861184196e-09\n",
      "==========================================\n",
      "Epoch:  217 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 499.031643625192\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 3.809720539038608e-09\n",
      "==========================================\n",
      "Epoch:  218 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 498.0452599388379\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 3.478309640314592e-09\n",
      "==========================================\n",
      "Epoch:  219 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 497.06788432267876\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 2.361118411897678e-09\n",
      "==========================================\n",
      "Epoch:  220 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 496.0993939393939\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 2.724111982388422e-05\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  221 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 495.13966817496225\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 8.69800897973505e-10\n",
      "==========================================\n",
      "Epoch:  222 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 494.1885885885885\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.1307406122540442e-09\n",
      "==========================================\n",
      "Epoch:  223 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 493.246038863976\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.1133448607481e-09\n",
      "==========================================\n",
      "Epoch:  224 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 492.31190476190477\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.3742845750286392e-09\n",
      "==========================================\n",
      "Epoch:  225 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 491.3860740740741\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.2003246174785431e-09\n",
      "==========================================\n",
      "Epoch:  226 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 490.4684365781711\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 6.610487712777058e-10\n",
      "==========================================\n",
      "Epoch:  227 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 489.5588839941263\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 2.572949779278133e-05\n",
      "==========================================\n",
      "Epoch:  228 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 488.65730994152045\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 8.209274255932542e-07\n",
      "==========================================\n",
      "Epoch:  229 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 487.76360989810775\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.496056611927088e-09\n",
      "==========================================\n",
      "Epoch:  230 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 486.8776811594203\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.1307406122540442e-09\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  231 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 485.9994227994228\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 2.2266877230947557e-09\n",
      "==========================================\n",
      "Epoch:  232 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 485.1287356321839\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 2.0875170481104988e-09\n",
      "==========================================\n",
      "Epoch:  233 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 484.2655221745351\n",
      "Max reward for a batch so far: 3820.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.913559088961847e-09\n",
      "==========================================\n",
      "Epoch:  234 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 483.4096866096866\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.496056611927088e-09\n",
      "==========================================\n",
      "Epoch:  235 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 482.56113475177307\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 5.075236675367023e-09\n",
      "==========================================\n",
      "Epoch:  236 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 481.7197740112994\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.3220969874438993e-09\n",
      "==========================================\n",
      "Epoch:  237 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 480.88551336146276\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.704804630797696e-09\n",
      "==========================================\n",
      "Epoch:  238 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 480.05826330532216\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.8787680300391685e-09\n",
      "==========================================\n",
      "Epoch:  239 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 479.2379358437936\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.2873039301197764e-09\n",
      "==========================================\n",
      "Epoch:  240 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 478.4244444444445\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 6.436520205710394e-10\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  241 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 477.61770401106503\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 2.7133879587637466e-09\n",
      "==========================================\n",
      "Epoch:  242 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 476.8176308539945\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.1481359196707785e-09\n",
      "==========================================\n",
      "Epoch:  243 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 476.0241426611797\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.9135604212294766e-09\n",
      "==========================================\n",
      "Epoch:  244 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 475.23715846994537\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 3.210701704503549e-10\n",
      "==========================================\n",
      "Epoch:  245 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 474.45659863945576\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.8787680300391685e-09\n",
      "==========================================\n",
      "Epoch:  246 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 473.6823848238482\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 9.154415181455988e-08\n",
      "==========================================\n",
      "Epoch:  247 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 472.9144399460189\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.008968686377898e-09\n",
      "==========================================\n",
      "Epoch:  248 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 472.15268817204304\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 2.11348916145937e-09\n",
      "==========================================\n",
      "Epoch:  249 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 471.3970548862115\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 2.470231796891653e-09\n",
      "==========================================\n",
      "Epoch:  250 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1736.0\n",
      "Mean Reward of that batch 289.3333333333333\n",
      "Average Reward of all training: 470.6688\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.009482291527092457\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  251 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 469.9250996015936\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 9.458447607357812e-07\n",
      "==========================================\n",
      "Epoch:  252 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 469.1873015873016\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0002659290039446205\n",
      "==========================================\n",
      "Epoch:  253 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 468.4553359683794\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 8.73367653753121e-08\n",
      "==========================================\n",
      "Epoch:  254 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 467.7291338582677\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  255 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 467.0086274509804\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  256 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 466.29375000000005\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  257 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 465.58443579766544\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  258 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 464.8806201550388\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  259 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 464.1822393822394\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  260 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 463.4892307692308\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  261 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 462.80153256704983\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  262 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 462.1190839694657\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  263 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 461.4418250950571\n",
      "Max reward for a batch so far: 3820.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  264 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 460.769696969697\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  265 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 460.102641509434\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  266 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 459.44060150375947\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  267 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 458.783520599251\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  268 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 458.1313432835821\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  269 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 457.48401486988854\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  270 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 456.84148148148154\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  271 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 456.2036900369004\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  272 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 455.57058823529417\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  273 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 454.9421245421246\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  274 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 454.31824817518253\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  275 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 453.6989090909091\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  276 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 453.08405797101454\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  277 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 452.47364620938635\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  278 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 451.8676258992806\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  279 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 451.2659498207886\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  280 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 450.66857142857145\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  281 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 450.0754448398577\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  282 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 449.48652482269506\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  283 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 448.9017667844523\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  284 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 448.3211267605634\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  285 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 447.74456140350884\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  286 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 447.172027972028\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  287 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 446.6034843205575\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  288 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 446.03888888888883\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  289 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 445.47820069204147\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  290 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 444.9213793103448\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  291 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 444.368384879725\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  292 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 443.81917808219174\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  293 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 443.2737201365187\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  294 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 442.73197278911556\n",
      "Max reward for a batch so far: 3820.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  295 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 442.1938983050847\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  296 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 441.6594594594594\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  297 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 441.1286195286195\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  298 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 440.6013422818791\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  299 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 440.07759197324407\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  300 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 439.55733333333325\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  301 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 439.04053156146176\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  302 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 438.52715231788073\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  303 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 438.0171617161716\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  304 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 437.5105263157895\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  305 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 437.0072131147541\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  306 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 436.5071895424837\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  307 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 436.01042345276875\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  308 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 435.51688311688315\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  309 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 435.0265372168285\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  310 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 434.5393548387097\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  311 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 434.055305466238\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  312 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 433.57435897435903\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  313 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 433.0964856230032\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  314 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 432.62165605095544\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  315 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 432.1498412698413\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  316 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 431.68101265822787\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  317 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 431.215141955836\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  318 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 430.75220125786166\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  319 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 430.2921630094044\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  320 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 429.83500000000004\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  321 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 429.3806853582555\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  322 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 428.92919254658386\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  323 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 428.4804953560372\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  324 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 428.0345679012346\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  325 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 427.59138461538464\n",
      "Max reward for a batch so far: 3820.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  326 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 427.1509202453988\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  327 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 426.7131498470948\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  328 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 426.27804878048784\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  329 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 425.84559270516723\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  330 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 425.4157575757576\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  331 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 424.9885196374623\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  332 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 424.5638554216868\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  333 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 424.1417417417418\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  334 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 423.72215568862276\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  335 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 423.3050746268657\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  336 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 422.89047619047625\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  337 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 422.4783382789318\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  338 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 422.06863905325446\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  339 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 421.6613569321534\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  340 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 421.25647058823535\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  341 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 420.85395894428154\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  342 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 420.4538011695907\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  343 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 420.0559766763849\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  344 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 419.6604651162791\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  345 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 419.2672463768116\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  346 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 418.8763005780347\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  347 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 418.4876080691643\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  348 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 418.1011494252874\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  349 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 417.7169054441261\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  350 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 417.3348571428572\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  351 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 416.9549857549858\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  352 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 416.57727272727277\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  353 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 416.2016997167139\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  354 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 415.82824858757067\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  355 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 415.45690140845073\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  356 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 415.08764044943825\n",
      "Max reward for a batch so far: 3820.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  357 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 414.7204481792717\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  358 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 414.35530726256985\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  359 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 413.9922005571031\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  360 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 413.6311111111111\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  361 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 413.27202216066485\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  362 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 412.9149171270719\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  363 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 412.5597796143251\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  364 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 412.2065934065934\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  365 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 411.85534246575344\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  366 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 411.50601092896176\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  367 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 411.1585831062671\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  368 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 410.8130434782609\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  369 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 410.46937669376695\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  370 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 410.1275675675676\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  371 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 409.78760107816714\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  372 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 409.44946236559144\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  373 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 409.11313672922256\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  374 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 408.7786096256685\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  375 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 408.4458666666667\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  376 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 408.1148936170213\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  377 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 407.78567639257295\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  378 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 407.4582010582011\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  379 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 407.13245382585757\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  380 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 406.8084210526316\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  381 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 406.4860892388452\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  382 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 406.165445026178\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  383 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 405.8464751958225\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  384 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 405.5291666666667\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  385 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 405.21350649350654\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  386 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 404.899481865285\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  387 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 404.5870801033592\n",
      "Max reward for a batch so far: 3820.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  388 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 404.27628865979386\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  389 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 403.9670951156813\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  390 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 403.6594871794872\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  391 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 403.35345268542204\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  392 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 403.04897959183677\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  393 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 402.7460559796438\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  394 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 402.44467005076143\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  395 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 402.1448101265823\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  396 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 401.84646464646465\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  397 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 401.5496221662469\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  398 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 401.25427135678393\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  399 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 400.9604010025063\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  400 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 400.668\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  401 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 400.37705735660853\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  402 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 400.0875621890548\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  403 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 399.7995037220844\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  404 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 399.51287128712875\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  405 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 399.2276543209877\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  406 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 398.94384236453203\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  407 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 398.66142506142506\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  408 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 398.3803921568628\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  409 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 398.10073349633257\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  410 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 397.8224390243903\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  411 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 397.545498783455\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  412 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 397.2699029126214\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  413 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 396.9956416464891\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  414 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 396.7227053140097\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  415 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 396.4510843373494\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  416 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 396.1807692307692\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  417 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 395.91175059952036\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  418 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 395.64401913875594\n",
      "Max reward for a batch so far: 3820.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  419 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 395.3775656324582\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  420 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 395.11238095238093\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  421 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 394.84845605700707\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  422 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 394.5857819905213\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  423 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 394.32434988179665\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  424 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 394.06415094339616\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  425 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 393.8051764705882\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  426 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 393.54741784037554\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  427 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 393.2908665105386\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  428 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 393.0355140186916\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  429 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 392.7813519813519\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  430 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 392.5283720930232\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  431 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 392.27656612529\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  432 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 392.0259259259259\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  433 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 391.7764434180138\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  434 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 391.5281105990783\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  435 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 391.28091954022983\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  436 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 391.03486238532105\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  437 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 390.7899313501144\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  438 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 390.54611872146114\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  439 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 390.303416856492\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  440 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 390.0618181818181\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  441 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 389.8213151927437\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  442 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 389.58190045248864\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  443 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 389.3435665914221\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  444 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 389.1063063063063\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  445 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 388.87011235955055\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  446 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 388.6349775784753\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  447 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 388.4008948545861\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  448 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 388.16785714285714\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  449 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 387.9358574610245\n",
      "Max reward for a batch so far: 3820.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  450 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 387.7048888888889\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  451 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 387.4749445676275\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  452 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 387.24601769911504\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  453 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 387.0181015452539\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  454 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 386.7911894273128\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  455 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 386.5652747252748\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  456 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 386.34035087719303\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  457 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 386.1164113785558\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  458 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 385.8934497816594\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  459 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 385.67145969498915\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  460 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 385.45043478260874\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  461 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 385.2303687635575\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  462 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 385.01125541125543\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  463 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 384.7930885529158\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  464 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 384.57586206896553\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  465 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 384.35956989247313\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  466 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 384.1442060085837\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  467 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 383.92976445396147\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  468 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 383.71623931623935\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  469 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 383.5036247334755\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  470 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 383.29191489361705\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  471 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 383.0811040339703\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  472 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 382.871186440678\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  473 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 382.662156448203\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  474 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 382.45400843881856\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  475 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 382.2467368421053\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  476 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 382.0403361344538\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  477 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 381.83480083857444\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  478 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 381.63012552301257\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  479 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 381.4263048016702\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  480 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 381.22333333333336\n",
      "Max reward for a batch so far: 3820.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  481 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 381.02120582120585\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0022350987419486046\n",
      "==========================================\n",
      "Epoch:  482 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 380.8199170124482\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.02094390243291855\n",
      "==========================================\n",
      "Epoch:  483 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1712.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 380.9175983436853\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.3517810106277466\n",
      "==========================================\n",
      "Epoch:  484 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1708.0\n",
      "Mean Reward of that batch 341.6\n",
      "Average Reward of all training: 380.8363636363636\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.014548123814165592\n",
      "==========================================\n",
      "Epoch:  485 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 380.63670103092784\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.02433306723833084\n",
      "==========================================\n",
      "Epoch:  486 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 380.4378600823045\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  487 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 380.23983572895276\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  488 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 380.04262295081963\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  489 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 379.8462167689161\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  490 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 379.65061224489796\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.4692730587739788e-07\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  491 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 379.4558044806517\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.001679637236520648\n",
      "==========================================\n",
      "Epoch:  492 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 379.26178861788617\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  493 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 379.0685598377282\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  494 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1800.0\n",
      "Mean Reward of that batch 300.0\n",
      "Average Reward of all training: 378.90850202429147\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.3278545374362238e-08\n",
      "==========================================\n",
      "Epoch:  495 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 378.71676767676763\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0050171297043561935\n",
      "==========================================\n",
      "Epoch:  496 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 378.5258064516129\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.004947445821017027\n",
      "==========================================\n",
      "Epoch:  497 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1708.0\n",
      "Mean Reward of that batch 341.6\n",
      "Average Reward of all training: 378.4515090543259\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.008522571995854378\n",
      "==========================================\n",
      "Epoch:  498 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 378.2618473895582\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.002654049778357148\n",
      "==========================================\n",
      "Epoch:  499 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 378.07294589178355\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0020573390647768974\n",
      "==========================================\n",
      "Epoch:  500 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 377.8848\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.002360283862799406\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  501 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1516.0\n",
      "Mean Reward of that batch 303.2\n",
      "Average Reward of all training: 377.7357285429141\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0008665271452628076\n",
      "==========================================\n",
      "Epoch:  502 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 377.5490039840637\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0006005496834404767\n",
      "==========================================\n",
      "Epoch:  503 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 377.36302186878726\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0004642204730771482\n",
      "==========================================\n",
      "Epoch:  504 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 377.1777777777778\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0021036784164607525\n",
      "==========================================\n",
      "Epoch:  505 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 376.9932673267327\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0004055951430927962\n",
      "==========================================\n",
      "Epoch:  506 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 376.8094861660079\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.004359537735581398\n",
      "==========================================\n",
      "Epoch:  507 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1644.0\n",
      "Mean Reward of that batch 328.8\n",
      "Average Reward of all training: 376.7147928994083\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.002290058881044388\n",
      "==========================================\n",
      "Epoch:  508 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 376.53228346456694\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.07964539527893066\n",
      "==========================================\n",
      "Epoch:  509 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1616.0\n",
      "Mean Reward of that batch 404.0\n",
      "Average Reward of all training: 376.5862475442043\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0231759175658226\n",
      "==========================================\n",
      "Epoch:  510 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1676.0\n",
      "Mean Reward of that batch 335.2\n",
      "Average Reward of all training: 376.5050980392157\n",
      "Max reward for a batch so far: 3820.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0029644498135894537\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  511 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1648.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 376.57455968688845\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.008004232309758663\n",
      "==========================================\n",
      "Epoch:  512 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1612.0\n",
      "Mean Reward of that batch 322.4\n",
      "Average Reward of all training: 376.46875\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.002963816514238715\n",
      "==========================================\n",
      "Epoch:  513 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1804.0\n",
      "Mean Reward of that batch 360.8\n",
      "Average Reward of all training: 376.43820662768036\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.007630860432982445\n",
      "==========================================\n",
      "Epoch:  514 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1768.0\n",
      "Mean Reward of that batch 294.6666666666667\n",
      "Average Reward of all training: 376.2791180285344\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.004332686774432659\n",
      "==========================================\n",
      "Epoch:  515 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1708.0\n",
      "Mean Reward of that batch 341.6\n",
      "Average Reward of all training: 376.2117799352751\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0030916952528059483\n",
      "==========================================\n",
      "Epoch:  516 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1612.0\n",
      "Mean Reward of that batch 322.4\n",
      "Average Reward of all training: 376.1074935400517\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0014853066531941295\n",
      "==========================================\n",
      "Epoch:  517 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 375.9293359123146\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.00023119943216443062\n",
      "==========================================\n",
      "Epoch:  518 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1612.0\n",
      "Mean Reward of that batch 322.4\n",
      "Average Reward of all training: 375.8259974259975\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.00014953284699004143\n",
      "==========================================\n",
      "Epoch:  519 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 375.64906872190113\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0015031186630949378\n",
      "==========================================\n",
      "Epoch:  520 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 375.47282051282053\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0008396312477998435\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  521 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 375.29724888035827\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 4.4606782466871664e-05\n",
      "==========================================\n",
      "Epoch:  522 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1516.0\n",
      "Mean Reward of that batch 303.2\n",
      "Average Reward of all training: 375.1591315453385\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.008265292271971703\n",
      "==========================================\n",
      "Epoch:  523 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 374.98483110261316\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.004004770889878273\n",
      "==========================================\n",
      "Epoch:  524 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1612.0\n",
      "Mean Reward of that batch 322.4\n",
      "Average Reward of all training: 374.8844783715013\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.010033526457846165\n",
      "==========================================\n",
      "Epoch:  525 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1708.0\n",
      "Mean Reward of that batch 341.6\n",
      "Average Reward of all training: 374.8210793650794\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.01306428574025631\n",
      "==========================================\n",
      "Epoch:  526 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1612.0\n",
      "Mean Reward of that batch 322.4\n",
      "Average Reward of all training: 374.72141951837773\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0027824812568724155\n",
      "==========================================\n",
      "Epoch:  527 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1612.0\n",
      "Mean Reward of that batch 322.4\n",
      "Average Reward of all training: 374.6221378874131\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.007046408951282501\n",
      "==========================================\n",
      "Epoch:  528 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 374.45050505050506\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0007584661943838\n",
      "==========================================\n",
      "Epoch:  529 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1964.0\n",
      "Mean Reward of that batch 392.8\n",
      "Average Reward of all training: 374.48519218651546\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0022240718826651573\n",
      "==========================================\n",
      "Epoch:  530 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 374.31446540880506\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.00025000679306685925\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  531 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 374.14438166980545\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.001220155507326126\n",
      "==========================================\n",
      "Epoch:  532 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1516.0\n",
      "Mean Reward of that batch 303.2\n",
      "Average Reward of all training: 374.0110275689223\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0005796928307972848\n",
      "==========================================\n",
      "Epoch:  533 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1800.0\n",
      "Mean Reward of that batch 300.0\n",
      "Average Reward of all training: 373.87217010631645\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.00035649770870804787\n",
      "==========================================\n",
      "Epoch:  534 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 373.7038701622971\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.0014804344391450286\n",
      "==========================================\n",
      "Epoch:  535 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1516.0\n",
      "Mean Reward of that batch 303.2\n",
      "Average Reward of all training: 373.57208722741433\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0028716304805129766\n",
      "==========================================\n",
      "Epoch:  536 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1676.0\n",
      "Mean Reward of that batch 335.2\n",
      "Average Reward of all training: 373.5004975124378\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.006219564005732536\n",
      "==========================================\n",
      "Epoch:  537 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1612.0\n",
      "Mean Reward of that batch 322.4\n",
      "Average Reward of all training: 373.4053382991931\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0021061054430902004\n",
      "==========================================\n",
      "Epoch:  538 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1900.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 373.41759603469643\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0017052121693268418\n",
      "==========================================\n",
      "Epoch:  539 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 373.25170068027217\n",
      "Max reward for a batch so far: 3820.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00018088592332787812\n",
      "==========================================\n",
      "Epoch:  540 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 373.08641975308643\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0013839465100318193\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  541 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 372.9217498459643\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0037478746380656958\n",
      "==========================================\n",
      "Epoch:  542 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 372.7576875768758\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0004559890949167311\n",
      "==========================================\n",
      "Epoch:  543 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 372.59422958870476\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.028545064851641655\n",
      "==========================================\n",
      "Epoch:  544 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 372.43137254901967\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.20451124012470245\n",
      "==========================================\n",
      "Epoch:  545 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1900.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 372.445259938838\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.021577823907136917\n",
      "==========================================\n",
      "Epoch:  546 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1768.0\n",
      "Mean Reward of that batch 294.6666666666667\n",
      "Average Reward of all training: 372.30280830280833\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.007652508094906807\n",
      "==========================================\n",
      "Epoch:  547 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 372.1413772090189\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.015405288897454739\n",
      "==========================================\n",
      "Epoch:  548 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 371.9805352798054\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 5.34918553896091e-10\n",
      "==========================================\n",
      "Epoch:  549 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1516.0\n",
      "Mean Reward of that batch 303.2\n",
      "Average Reward of all training: 371.8552519732848\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.004443668760359287\n",
      "==========================================\n",
      "Epoch:  550 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 371.69551515151517\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 3.859121022742329e-08\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  551 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 371.53635813672116\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0005813392344862223\n",
      "==========================================\n",
      "Epoch:  552 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 371.3777777777778\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 2.3053287279140022e-08\n",
      "==========================================\n",
      "Epoch:  553 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 371.21977094635326\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 2.97263582638152e-08\n",
      "==========================================\n",
      "Epoch:  554 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 371.0623345367028\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0020651931408792734\n",
      "==========================================\n",
      "Epoch:  555 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 370.9054654654655\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 5.618878695656804e-09\n",
      "==========================================\n",
      "Epoch:  556 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 370.74916067146285\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 7.810737834290649e-09\n",
      "==========================================\n",
      "Epoch:  557 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 370.59341711549973\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0007915438036434352\n",
      "==========================================\n",
      "Epoch:  558 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 370.4382317801673\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.22855901718139648\n",
      "==========================================\n",
      "Epoch:  559 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1736.0\n",
      "Mean Reward of that batch 289.3333333333333\n",
      "Average Reward of all training: 370.2931425163984\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.014275611378252506\n",
      "==========================================\n",
      "Epoch:  560 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 370.13904761904763\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  561 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 369.9855020796197\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  562 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 369.8325029655991\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  563 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 369.6800473653049\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  564 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 369.5281323877069\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  565 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 369.37675516224186\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.002564513823017478\n",
      "==========================================\n",
      "Epoch:  566 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 369.2259128386337\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  567 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 369.0756025867137\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  568 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 368.92582159624413\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: -0.77305668592453\n",
      "==========================================\n",
      "Epoch:  569 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 368.7765670767428\n",
      "Max reward for a batch so far: 3820.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  570 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 368.6278362573099\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  571 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 368.4796263864565\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  572 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 368.3319347319347\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 3.47920720011885e-11\n",
      "==========================================\n",
      "Epoch:  573 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 368.1847585805701\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  574 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 368.03809523809525\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  575 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 367.8919420289855\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 3.47920720011885e-11\n",
      "==========================================\n",
      "Epoch:  576 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 367.74629629629624\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  577 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 367.60115540150196\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  578 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 367.4565167243367\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  579 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 367.31237766263666\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  580 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 367.1687356321839\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  581 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 367.02558806655185\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  582 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 366.882932416953\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  583 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 366.74076615208685\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.739603600059425e-11\n",
      "==========================================\n",
      "Epoch:  584 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 366.59908675799085\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  585 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 366.4578917378917\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  586 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 366.31717861205914\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  587 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 366.17694491766036\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  588 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 366.0371882086167\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  589 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 365.8979060554612\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  590 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 365.7590960451977\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  591 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 365.62075578116185\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  592 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 365.48288288288285\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  593 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 365.34547498594713\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  594 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 365.208529741863\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  595 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 365.0720448179271\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  596 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 364.9360178970917\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  597 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 364.8004466778336\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  598 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 364.66532887402445\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  599 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 364.5306622148024\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.7012332875765424e-08\n",
      "==========================================\n",
      "Epoch:  600 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 364.3964444444444\n",
      "Max reward for a batch so far: 3820.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  601 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 364.2626733222407\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  602 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 364.12934662236984\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  603 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 363.99646213377554\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 2.2614830841316547e-10\n",
      "==========================================\n",
      "Epoch:  604 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 363.8640176600441\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  605 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 363.7320110192837\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  606 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 363.60044004400436\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  607 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 363.4693025809994\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.739603600059425e-11\n",
      "==========================================\n",
      "Epoch:  608 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 363.3385964912281\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  609 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 363.208319649699\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  610 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 363.0784699453552\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 1.739603600059425e-11\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  611 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 362.9490452809602\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  612 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 362.82004357298473\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  613 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 362.6914627514954\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  614 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 362.56330076004343\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n",
      "==========================================\n",
      "Epoch:  615 / 1000\n",
      "-----------\n",
      "Number of training episodes: 6\n",
      "Total reward: 1704.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 362.43555555555554\n",
      "Max reward for a batch so far: 3820.0\n",
      "Training Loss: 0.0\n"
     ]
    },
    {
     "ename": "ViZDoomUnexpectedExitException",
     "evalue": "Controlled ViZDoom instance exited unexpectedly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-793c29f25285>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Gather training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mstates_mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions_mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards_of_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscounted_rewards_mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_episodes_mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacked_frames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m### These part is used for analytics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-70e492ee4b9c>\u001b[0m in \u001b[0;36mmake_batch\u001b[1;34m(batch_size, stacked_frames)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# Perform action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_episode_finished\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m: Controlled ViZDoom instance exited unexpectedly."
     ]
    }
   ],
   "source": [
    "# Keep track of all rewards total for each batch\n",
    "allRewards = []\n",
    "\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "mean_reward_total = []\n",
    "epoch = 1\n",
    "average_reward = []\n",
    "\n",
    "# Saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training:\n",
    "    # Load the model\n",
    "    #saver.restore(sess, \"./models/model.ckpt\")\n",
    "\n",
    "    while epoch < num_epochs + 1:\n",
    "        # Gather training data\n",
    "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, nb_episodes_mb = make_batch(batch_size, stacked_frames)\n",
    "\n",
    "        ### These part is used for analytics\n",
    "        # Calculate the total reward ot the batch\n",
    "        total_reward_of_that_batch = np.sum(rewards_of_batch)\n",
    "        allRewards.append(total_reward_of_that_batch)\n",
    "\n",
    "        # Calculate the mean reward of the batch\n",
    "        # Total rewards of batch / nb episodes in that batch\n",
    "        mean_reward_of_that_batch = np.divide(total_reward_of_that_batch, nb_episodes_mb)\n",
    "        mean_reward_total.append(mean_reward_of_that_batch)\n",
    "\n",
    "        # Calculate the average reward of all training\n",
    "        # mean_reward_of_that_batch / epoch\n",
    "        average_reward_of_all_training = np.divide(np.sum(mean_reward_total), epoch)\n",
    "\n",
    "        # Calculate maximum reward recorded \n",
    "        maximumRewardRecorded = np.amax(allRewards)\n",
    "\n",
    "        print(\"==========================================\")\n",
    "        print(\"Epoch: \", epoch, \"/\", num_epochs)\n",
    "        print(\"-----------\")\n",
    "        print(\"Number of training episodes: {}\".format(nb_episodes_mb))\n",
    "        print(\"Total reward: {}\".format(total_reward_of_that_batch, nb_episodes_mb))\n",
    "        print(\"Mean Reward of that batch {}\".format(mean_reward_of_that_batch))\n",
    "        print(\"Average Reward of all training: {}\".format(average_reward_of_all_training))\n",
    "        print(\"Max reward for a batch so far: {}\".format(maximumRewardRecorded))\n",
    "\n",
    "        # Feedforward, gradient and backpropagation\n",
    "        loss_, _ = sess.run([PGNetwork.loss, PGNetwork.train_opt], feed_dict={PGNetwork.inputs_: states_mb.reshape((len(states_mb), 84,84,4)),\n",
    "                                                            PGNetwork.actions: actions_mb,\n",
    "                                                                     PGNetwork.discounted_episode_rewards_: discounted_rewards_mb \n",
    "                                                                    })\n",
    "\n",
    "        print(\"Training Loss: {}\".format(loss_))\n",
    "\n",
    "        # Write TF Summaries\n",
    "        summary = sess.run(write_op, feed_dict={PGNetwork.inputs_: states_mb.reshape((len(states_mb), 84,84,4)),\n",
    "                                                            PGNetwork.actions: actions_mb,\n",
    "                                                                     PGNetwork.discounted_episode_rewards_: discounted_rewards_mb,\n",
    "                                                                    PGNetwork.mean_reward_: mean_reward_of_that_batch\n",
    "                                                                    })\n",
    "\n",
    "        #summary = sess.run(write_op, feed_dict={x: s_.reshape(len(s_),84,84,1), y:a_, d_r: d_r_, r: r_, n: n_})\n",
    "        writer.add_summary(summary, epoch)\n",
    "        writer.flush()\n",
    "\n",
    "        # Save Model\n",
    "        if epoch % 10 == 0:\n",
    "            saver.save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model saved\")\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
