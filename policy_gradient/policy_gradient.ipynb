{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "from environment_creation import create_environment\n",
    "import policy_gradient_network as pgn\n",
    "from frame_preprocessing import preprocess_frame\n",
    "from frames_stacking import stack_frames \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()\n",
    "\n",
    "stack_size = 4\n",
    "stacked_frames = deque([np.zeros((84,84), dtype = np.int) for i in range(stack_size)], maxlen = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards, gamma):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "\n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################\n",
    "###Environment hyperparameters\n",
    "state_size = [84, 84, 4] #our input is a stack of 4 frames , 84x84\n",
    "action_size = game.get_available_buttons_size() # 3 possible actions, turn left, turn right, go forward\n",
    "stack_size = 4 #how many frames are stacked together\n",
    "\n",
    "#deep learning model hyperparameters\n",
    "learning_rate = 0.002\n",
    "num_epochs = 1000\n",
    "\n",
    "batch_size = 1000\n",
    "gamma = 0.95 #discount rate\n",
    "\n",
    "training = True\n",
    "######################################\n",
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "PGNetwork = pgn.PGNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup tensorflow writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/pg/test\")\n",
    "\n",
    "#losses\n",
    "tf.summary.scalar(\"Loss\", PGNetwork.loss)\n",
    "\n",
    "tf.summary.scalar(\"Reward_mean\", PGNetwork.mean_reward_ )\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now is the time to train the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll create batches.\n",
    "These batches contains episodes (their number depends on how many rewards we collect: for instance if we have episodes with only 10 rewards we can put batch_size/10 episodes\n",
    "\n",
    "    Make a batch\n",
    "        For each step:\n",
    "            Choose action a\n",
    "            Perform action a\n",
    "            Store s, a, r\n",
    "            If done:\n",
    "                Calculate sum reward\n",
    "                Calculate gamma Gt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(batch_size, stacked_frames):\n",
    "    # Initialize lists: states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards\n",
    "    states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards = [], [], [], [], []\n",
    "    \n",
    "    # Reward of batch is also a trick to keep track of how many timestep we made.\n",
    "    # We use to to verify at the end of each episode if > batch_size or not.\n",
    "    \n",
    "    # Keep track of how many episodes in our batch (useful when we'll need to calculate the average reward per episode)\n",
    "    episode_num  = 1\n",
    "    \n",
    "    # Launch a new episode\n",
    "    game.new_episode()\n",
    "        \n",
    "    # Get a new state\n",
    "    state = game.get_state().screen_buffer\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, True, stack_size)\n",
    "\n",
    "    while True:\n",
    "        # Run State Through Policy & Calculate Action\n",
    "        action_probability_distribution = sess.run(PGNetwork.action_distribution, \n",
    "                                                   feed_dict={PGNetwork.inputs_: state.reshape(1, *state_size)})\n",
    "        \n",
    "        # REMEMBER THAT WE ARE IN A STOCHASTIC POLICY SO WE DON'T ALWAYS TAKE THE ACTION WITH THE HIGHEST PROBABILITY\n",
    "        # (For instance if the action with the best probability for state S is a1 with 70% chances, there is\n",
    "        #30% chance that we take action a2)\n",
    "        action = np.random.choice(range(action_probability_distribution.shape[1]), \n",
    "                                  p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "        action = possible_actions[action]\n",
    "\n",
    "        # Perform action\n",
    "        reward = game.make_action(action)\n",
    "        done = game.is_episode_finished()\n",
    "\n",
    "        # Store results\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards_of_episode.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            # The episode ends so no next state\n",
    "            next_state = np.zeros((84, 84), dtype=np.int)\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False, stack_size)\n",
    "            \n",
    "            # Append the rewards_of_batch to reward_of_episode\n",
    "            rewards_of_batch.append(rewards_of_episode)\n",
    "            \n",
    "            # Calculate gamma Gt\n",
    "            discounted_rewards.append(discount_and_normalize_rewards(rewards_of_episode, gamma))\n",
    "           \n",
    "            # If the number of rewards_of_batch > batch_size stop the minibatch creation\n",
    "            # (Because we have sufficient number of episode mb)\n",
    "            # Remember that we put this condition here, because we want entire episode (Monte Carlo)\n",
    "            # so we can't check that condition for each step but only if an episode is finished\n",
    "            if len(np.concatenate(rewards_of_batch)) > batch_size:\n",
    "                break\n",
    "                \n",
    "            # Reset the transition stores\n",
    "            rewards_of_episode = []\n",
    "            \n",
    "            # Add episode\n",
    "            episode_num += 1\n",
    "            \n",
    "            # Start a new episode\n",
    "            game.new_episode()\n",
    "\n",
    "            # First we need a state\n",
    "            state = game.get_state().screen_buffer\n",
    "\n",
    "            # Stack the frames\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True, stack_size)\n",
    "         \n",
    "        else:\n",
    "            # If not done, the next_state become the current state\n",
    "            next_state = game.get_state().screen_buffer\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False, stack_size)\n",
    "            state = next_state\n",
    "                         \n",
    "    return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards), episode_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the Neural Network\n",
    "- Init the weights\n",
    "- Init the environment\n",
    "- maxReward = 0 #keep track of max reward\n",
    "- for epochs in range(num_epochs):\n",
    "    - get batches\n",
    "    - optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  1 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 984.0\n",
      "Mean Reward of that batch 492.0\n",
      "Average Reward of all training: 492.0\n",
      "Max reward for a batch so far: 984.0\n",
      "Training Loss: -0.07897565513849258\n",
      "==========================================\n",
      "Epoch:  2 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1172.0\n",
      "Mean Reward of that batch 390.6666666666667\n",
      "Average Reward of all training: 441.33333333333337\n",
      "Max reward for a batch so far: 1172.0\n",
      "Training Loss: -0.050171349197626114\n",
      "==========================================\n",
      "Epoch:  3 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 984.0\n",
      "Mean Reward of that batch 492.0\n",
      "Average Reward of all training: 458.22222222222223\n",
      "Max reward for a batch so far: 1172.0\n",
      "Training Loss: -0.016246357932686806\n",
      "==========================================\n",
      "Epoch:  4 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 824.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 446.6666666666667\n",
      "Max reward for a batch so far: 1172.0\n",
      "Training Loss: 0.0035325735807418823\n",
      "==========================================\n",
      "Epoch:  5 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1048.0\n",
      "Mean Reward of that batch 524.0\n",
      "Average Reward of all training: 462.1333333333334\n",
      "Max reward for a batch so far: 1172.0\n",
      "Training Loss: -0.017562035471200943\n",
      "==========================================\n",
      "Epoch:  6 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 888.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 459.11111111111114\n",
      "Max reward for a batch so far: 1172.0\n",
      "Training Loss: -0.17773939669132233\n",
      "==========================================\n",
      "Epoch:  7 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1176.0\n",
      "Mean Reward of that batch 588.0\n",
      "Average Reward of all training: 477.5238095238096\n",
      "Max reward for a batch so far: 1176.0\n",
      "Training Loss: -0.021024778485298157\n",
      "==========================================\n",
      "Epoch:  8 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 477.33333333333337\n",
      "Max reward for a batch so far: 1176.0\n",
      "Training Loss: -0.10422880202531815\n",
      "==========================================\n",
      "Epoch:  9 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 471.8518518518519\n",
      "Max reward for a batch so far: 1176.0\n",
      "Training Loss: -0.19503526389598846\n",
      "==========================================\n",
      "Epoch:  10 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 980.0\n",
      "Mean Reward of that batch 326.6666666666667\n",
      "Average Reward of all training: 457.33333333333337\n",
      "Max reward for a batch so far: 1176.0\n",
      "Training Loss: -0.020947663113474846\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  11 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 459.03030303030306\n",
      "Max reward for a batch so far: 1176.0\n",
      "Training Loss: 0.028243787586688995\n",
      "==========================================\n",
      "Epoch:  12 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 447.11111111111114\n",
      "Max reward for a batch so far: 1176.0\n",
      "Training Loss: -0.014622758142650127\n",
      "==========================================\n",
      "Epoch:  13 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 439.48717948717956\n",
      "Max reward for a batch so far: 1176.0\n",
      "Training Loss: -0.03112085349857807\n",
      "==========================================\n",
      "Epoch:  14 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1980.0\n",
      "Mean Reward of that batch 1980.0\n",
      "Average Reward of all training: 549.5238095238095\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.04067203029990196\n",
      "==========================================\n",
      "Epoch:  15 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1588.0\n",
      "Mean Reward of that batch 529.3333333333334\n",
      "Average Reward of all training: 548.1777777777778\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.011970093473792076\n",
      "==========================================\n",
      "Epoch:  16 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 540.6666666666667\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.01449063140898943\n",
      "==========================================\n",
      "Epoch:  17 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 888.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 534.9803921568628\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.23472444713115692\n",
      "==========================================\n",
      "Epoch:  18 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 529.0370370370371\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.007967309094965458\n",
      "==========================================\n",
      "Epoch:  19 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 523.7192982456141\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.0025682179257273674\n",
      "==========================================\n",
      "Epoch:  20 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 920.0\n",
      "Mean Reward of that batch 460.0\n",
      "Average Reward of all training: 520.5333333333334\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.016829997301101685\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  21 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 516.1269841269842\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.01414938922971487\n",
      "==========================================\n",
      "Epoch:  22 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 514.3030303030304\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.01673419401049614\n",
      "==========================================\n",
      "Epoch:  23 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1300.0\n",
      "Mean Reward of that batch 433.3333333333333\n",
      "Average Reward of all training: 510.78260869565224\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.00989746768027544\n",
      "==========================================\n",
      "Epoch:  24 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1172.0\n",
      "Mean Reward of that batch 390.6666666666667\n",
      "Average Reward of all training: 505.7777777777778\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.009217052720487118\n",
      "==========================================\n",
      "Epoch:  25 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1108.0\n",
      "Mean Reward of that batch 369.3333333333333\n",
      "Average Reward of all training: 500.32000000000005\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.028371494263410568\n",
      "==========================================\n",
      "Epoch:  26 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 980.0\n",
      "Mean Reward of that batch 326.6666666666667\n",
      "Average Reward of all training: 493.6410256410257\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.0039017677772790194\n",
      "==========================================\n",
      "Epoch:  27 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 485.8765432098766\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.027966883033514023\n",
      "==========================================\n",
      "Epoch:  28 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 480.952380952381\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.014243049547076225\n",
      "==========================================\n",
      "Epoch:  29 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1076.0\n",
      "Mean Reward of that batch 358.6666666666667\n",
      "Average Reward of all training: 476.7356321839081\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.00142770039383322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  30 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1332.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 475.64444444444445\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.0029852529987692833\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  31 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 475.65591397849465\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.0304609015583992\n",
      "==========================================\n",
      "Epoch:  32 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 474.1666666666667\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.02499815635383129\n",
      "==========================================\n",
      "Epoch:  33 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 468.4040404040404\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.008977694436907768\n",
      "==========================================\n",
      "Epoch:  34 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 464.8627450980392\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.051747072488069534\n",
      "==========================================\n",
      "Epoch:  35 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 463.80952380952385\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.014397033490240574\n",
      "==========================================\n",
      "Epoch:  36 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1140.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 461.4814814814815\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.021398957818746567\n",
      "==========================================\n",
      "Epoch:  37 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 461.87387387387395\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.0015734037151560187\n",
      "==========================================\n",
      "Epoch:  38 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1204.0\n",
      "Mean Reward of that batch 401.3333333333333\n",
      "Average Reward of all training: 460.280701754386\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.03859582915902138\n",
      "==========================================\n",
      "Epoch:  39 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 459.4529914529915\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.009397486224770546\n",
      "==========================================\n",
      "Epoch:  40 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 456.6666666666667\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.015890561044216156\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  41 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 452.45528455284557\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.027477536350488663\n",
      "==========================================\n",
      "Epoch:  42 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1236.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 451.4920634920635\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.013621010817587376\n",
      "==========================================\n",
      "Epoch:  43 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 450.94573643410854\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.009055349975824356\n",
      "==========================================\n",
      "Epoch:  44 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 448.6060606060606\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.004627670627087355\n",
      "==========================================\n",
      "Epoch:  45 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 446.3703703703704\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.005409771576523781\n",
      "==========================================\n",
      "Epoch:  46 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1268.0\n",
      "Mean Reward of that batch 422.6666666666667\n",
      "Average Reward of all training: 445.8550724637682\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.007527205627411604\n",
      "==========================================\n",
      "Epoch:  47 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 443.09219858156035\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.018994994461536407\n",
      "==========================================\n",
      "Epoch:  48 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 442.77777777777777\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.0033413488417863846\n",
      "==========================================\n",
      "Epoch:  49 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1048.0\n",
      "Mean Reward of that batch 524.0\n",
      "Average Reward of all training: 444.4353741496598\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.039375126361846924\n",
      "==========================================\n",
      "Epoch:  50 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1140.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 443.14666666666665\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.024958956986665726\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  51 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 980.0\n",
      "Mean Reward of that batch 326.6666666666667\n",
      "Average Reward of all training: 440.8627450980392\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.036299388855695724\n",
      "==========================================\n",
      "Epoch:  52 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1016.0\n",
      "Mean Reward of that batch 508.0\n",
      "Average Reward of all training: 442.15384615384613\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.017350494861602783\n",
      "==========================================\n",
      "Epoch:  53 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1180.0\n",
      "Mean Reward of that batch 1180.0\n",
      "Average Reward of all training: 456.07547169811323\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.12817886471748352\n",
      "==========================================\n",
      "Epoch:  54 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1140.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 454.6666666666667\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.016506297513842583\n",
      "==========================================\n",
      "Epoch:  55 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1012.0\n",
      "Mean Reward of that batch 337.3333333333333\n",
      "Average Reward of all training: 452.5333333333333\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.021616719663143158\n",
      "==========================================\n",
      "Epoch:  56 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 452.09523809523813\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.0308055579662323\n",
      "==========================================\n",
      "Epoch:  57 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 449.1461988304094\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.024768762290477753\n",
      "==========================================\n",
      "Epoch:  58 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 884.0\n",
      "Mean Reward of that batch 294.6666666666667\n",
      "Average Reward of all training: 446.4827586206897\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.0026141107082366943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  59 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 446.98305084745766\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.01585059054195881\n",
      "==========================================\n",
      "Epoch:  60 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 447.46666666666675\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.007875485345721245\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  61 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 445.83606557377055\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.03501809388399124\n",
      "==========================================\n",
      "Epoch:  62 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 446.32258064516134\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.0017047524452209473\n",
      "==========================================\n",
      "Epoch:  63 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 446.0317460317461\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.031088579446077347\n",
      "==========================================\n",
      "Epoch:  64 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1016.0\n",
      "Mean Reward of that batch 508.0\n",
      "Average Reward of all training: 447.0\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.0006359156686812639\n",
      "==========================================\n",
      "Epoch:  65 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1140.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 445.96923076923076\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.021877558901906013\n",
      "==========================================\n",
      "Epoch:  66 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1076.0\n",
      "Mean Reward of that batch 358.6666666666667\n",
      "Average Reward of all training: 444.64646464646466\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.01107768528163433\n",
      "==========================================\n",
      "Epoch:  67 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 445.11442786069654\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.04202266409993172\n",
      "==========================================\n",
      "Epoch:  68 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 443.6862745098039\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.02732856757938862\n",
      "==========================================\n",
      "Epoch:  69 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 443.45893719806764\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.041321076452732086\n",
      "==========================================\n",
      "Epoch:  70 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 441.6380952380953\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.02517794258892536\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  71 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 440.3192488262911\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.043491799384355545\n",
      "==========================================\n",
      "Epoch:  72 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 439.03703703703707\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.09244946390390396\n",
      "==========================================\n",
      "Epoch:  73 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 437.351598173516\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.014849672093987465\n",
      "==========================================\n",
      "Epoch:  74 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 435.7117117117117\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.05685232952237129\n",
      "==========================================\n",
      "Epoch:  75 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 435.6088888888889\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.05578075721859932\n",
      "==========================================\n",
      "Epoch:  76 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1140.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 434.8771929824562\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.004871937446296215\n",
      "==========================================\n",
      "Epoch:  77 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1236.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 434.58008658008663\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.028081253170967102\n",
      "==========================================\n",
      "Epoch:  78 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 434.49572649572656\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.016665538772940636\n",
      "==========================================\n",
      "Epoch:  79 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1140.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 433.80590717299583\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.053291406482458115\n",
      "==========================================\n",
      "Epoch:  80 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1236.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 433.5333333333334\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.028575927019119263\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  81 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1048.0\n",
      "Mean Reward of that batch 524.0\n",
      "Average Reward of all training: 434.65020576131695\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.02716113068163395\n",
      "==========================================\n",
      "Epoch:  82 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1336.0\n",
      "Mean Reward of that batch 668.0\n",
      "Average Reward of all training: 437.49593495934965\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.0364118330180645\n",
      "==========================================\n",
      "Epoch:  83 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 437.38152610441773\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.04276389628648758\n",
      "==========================================\n",
      "Epoch:  84 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 437.8412698412699\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.013762257993221283\n",
      "==========================================\n",
      "Epoch:  85 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 438.29019607843145\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.03351546823978424\n",
      "==========================================\n",
      "Epoch:  86 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1332.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 438.35658914728685\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.01053721271455288\n",
      "==========================================\n",
      "Epoch:  87 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 824.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 438.05363984674335\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.0445639044046402\n",
      "==========================================\n",
      "Epoch:  88 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 437.939393939394\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.005270204972475767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  89 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 888.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 438.0074906367042\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.06505566835403442\n",
      "==========================================\n",
      "Epoch:  90 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 437.8962962962963\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.02991815097630024\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  91 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 984.0\n",
      "Mean Reward of that batch 492.0\n",
      "Average Reward of all training: 438.49084249084257\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.036060184240341187\n",
      "==========================================\n",
      "Epoch:  92 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 437.5072463768116\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.03869618847966194\n",
      "==========================================\n",
      "Epoch:  93 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 824.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 437.2329749103943\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.01693897694349289\n",
      "==========================================\n",
      "Epoch:  94 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1208.0\n",
      "Mean Reward of that batch 604.0\n",
      "Average Reward of all training: 439.00709219858163\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.024244405329227448\n",
      "==========================================\n",
      "Epoch:  95 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1208.0\n",
      "Mean Reward of that batch 604.0\n",
      "Average Reward of all training: 440.74385964912284\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.0014963772846385837\n",
      "==========================================\n",
      "Epoch:  96 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1076.0\n",
      "Mean Reward of that batch 358.6666666666667\n",
      "Average Reward of all training: 439.88888888888886\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.003976844251155853\n",
      "==========================================\n",
      "Epoch:  97 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 438.9415807560137\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.007557321339845657\n",
      "==========================================\n",
      "Epoch:  98 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1620.0\n",
      "Mean Reward of that batch 540.0\n",
      "Average Reward of all training: 439.9727891156462\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.03385080024600029\n",
      "==========================================\n",
      "Epoch:  99 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1336.0\n",
      "Mean Reward of that batch 668.0\n",
      "Average Reward of all training: 442.2760942760942\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.023275747895240784\n",
      "==========================================\n",
      "Epoch:  100 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 441.01333333333326\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.1296171098947525\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  101 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1300.0\n",
      "Mean Reward of that batch 433.3333333333333\n",
      "Average Reward of all training: 440.9372937293729\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: -0.04016055166721344\n",
      "==========================================\n",
      "Epoch:  102 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 980.0\n",
      "Mean Reward of that batch 326.6666666666667\n",
      "Average Reward of all training: 439.81699346405225\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.08749131858348846\n",
      "==========================================\n",
      "Epoch:  103 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 920.0\n",
      "Mean Reward of that batch 460.0\n",
      "Average Reward of all training: 440.0129449838187\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.1044798195362091\n",
      "==========================================\n",
      "Epoch:  104 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 916.0\n",
      "Mean Reward of that batch 305.3333333333333\n",
      "Average Reward of all training: 438.7179487179488\n",
      "Max reward for a batch so far: 1980.0\n",
      "Training Loss: 0.003645743243396282\n",
      "==========================================\n",
      "Epoch:  105 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2008.0\n",
      "Mean Reward of that batch 1004.0\n",
      "Average Reward of all training: 444.10158730158736\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: -0.10003461688756943\n",
      "==========================================\n",
      "Epoch:  106 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 916.0\n",
      "Mean Reward of that batch 305.3333333333333\n",
      "Average Reward of all training: 442.7924528301887\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: 0.776561975479126\n",
      "==========================================\n",
      "Epoch:  107 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 884.0\n",
      "Mean Reward of that batch 294.6666666666667\n",
      "Average Reward of all training: 441.40809968847356\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: 0.01372380182147026\n",
      "==========================================\n",
      "Epoch:  108 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 980.0\n",
      "Mean Reward of that batch 326.6666666666667\n",
      "Average Reward of all training: 440.3456790123457\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: -0.09788842499256134\n",
      "==========================================\n",
      "Epoch:  109 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1364.0\n",
      "Mean Reward of that batch 454.6666666666667\n",
      "Average Reward of all training: 440.4770642201835\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: 0.46937495470046997\n",
      "==========================================\n",
      "Epoch:  110 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 439.3454545454546\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: 0.018087221309542656\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  111 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1108.0\n",
      "Mean Reward of that batch 369.3333333333333\n",
      "Average Reward of all training: 438.7147147147147\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: 0.09802185744047165\n",
      "==========================================\n",
      "Epoch:  112 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 438.61904761904765\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: 0.08641686290502548\n",
      "==========================================\n",
      "Epoch:  113 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 824.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 438.3834808259587\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: 0.10242605209350586\n",
      "==========================================\n",
      "Epoch:  114 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1076.0\n",
      "Mean Reward of that batch 358.6666666666667\n",
      "Average Reward of all training: 437.6842105263158\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: -0.28900983929634094\n",
      "==========================================\n",
      "Epoch:  115 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 437.6\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: -1.4470245838165283\n",
      "==========================================\n",
      "Epoch:  116 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 980.0\n",
      "Mean Reward of that batch 326.6666666666667\n",
      "Average Reward of all training: 436.64367816091954\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: 0.2834281027317047\n",
      "==========================================\n",
      "Epoch:  117 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1140.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 436.1595441595441\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: -1.5291213989257812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  118 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 984.0\n",
      "Mean Reward of that batch 492.0\n",
      "Average Reward of all training: 436.6327683615819\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: 0.22881874442100525\n",
      "==========================================\n",
      "Epoch:  119 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1076.0\n",
      "Mean Reward of that batch 358.6666666666667\n",
      "Average Reward of all training: 435.97759103641454\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: 0.8977101445198059\n",
      "==========================================\n",
      "Epoch:  120 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1332.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 436.0444444444445\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: 0.21287259459495544\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  121 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 924.0\n",
      "Mean Reward of that batch 924.0\n",
      "Average Reward of all training: 440.07713498622593\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: 0.12520167231559753\n",
      "==========================================\n",
      "Epoch:  122 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 439.97814207650276\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: 0.12397821247577667\n",
      "==========================================\n",
      "Epoch:  123 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 980.0\n",
      "Mean Reward of that batch 326.6666666666667\n",
      "Average Reward of all training: 439.0569105691057\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: 0.16301560401916504\n",
      "==========================================\n",
      "Epoch:  124 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 439.35483870967744\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: 0.27167972922325134\n",
      "==========================================\n",
      "Epoch:  125 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 920.0\n",
      "Mean Reward of that batch 460.0\n",
      "Average Reward of all training: 439.52\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: 2.198711395263672\n",
      "==========================================\n",
      "Epoch:  126 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1916.0\n",
      "Mean Reward of that batch 1916.0\n",
      "Average Reward of all training: 451.23809523809524\n",
      "Max reward for a batch so far: 2008.0\n",
      "Training Loss: -2.8489346504211426\n",
      "==========================================\n",
      "Epoch:  127 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 464.2204724409449\n",
      "Max reward for a batch so far: 2100.0\n",
      "Training Loss: 0.8397010564804077\n",
      "==========================================\n",
      "Epoch:  128 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1048.0\n",
      "Mean Reward of that batch 524.0\n",
      "Average Reward of all training: 464.6875\n",
      "Max reward for a batch so far: 2100.0\n",
      "Training Loss: 0.13456888496875763\n",
      "==========================================\n",
      "Epoch:  129 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2608.0\n",
      "Mean Reward of that batch 1304.0\n",
      "Average Reward of all training: 471.1937984496124\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 1.0487781763076782\n",
      "==========================================\n",
      "Epoch:  130 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 483.7230769230769\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 0.377218633890152\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  131 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1948.0\n",
      "Mean Reward of that batch 1948.0\n",
      "Average Reward of all training: 494.90076335877865\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -1.166661262512207\n",
      "==========================================\n",
      "Epoch:  132 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2072.0\n",
      "Mean Reward of that batch 1036.0\n",
      "Average Reward of all training: 499.0\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 0.4849858283996582\n",
      "==========================================\n",
      "Epoch:  133 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 511.0375939849624\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -13.493285179138184\n",
      "==========================================\n",
      "Epoch:  134 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1180.0\n",
      "Mean Reward of that batch 1180.0\n",
      "Average Reward of all training: 516.0298507462686\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -40.4215202331543\n",
      "==========================================\n",
      "Epoch:  135 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 527.762962962963\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -0.43103259801864624\n",
      "==========================================\n",
      "Epoch:  136 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1204.0\n",
      "Mean Reward of that batch 401.3333333333333\n",
      "Average Reward of all training: 526.8333333333334\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 3.391165256500244\n",
      "==========================================\n",
      "Epoch:  137 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1404.0\n",
      "Mean Reward of that batch 1404.0\n",
      "Average Reward of all training: 533.2360097323601\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -16.278789520263672\n",
      "==========================================\n",
      "Epoch:  138 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 888.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 532.5893719806764\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -39.000274658203125\n",
      "==========================================\n",
      "Epoch:  139 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 916.0\n",
      "Mean Reward of that batch 305.3333333333333\n",
      "Average Reward of all training: 530.9544364508394\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -1.179958701133728\n",
      "==========================================\n",
      "Epoch:  140 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 529.1904761904763\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 1.6171698570251465\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  141 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 527.4515366430261\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 0.6539248824119568\n",
      "==========================================\n",
      "Epoch:  142 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 525.737089201878\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -0.021080732345581055\n",
      "==========================================\n",
      "Epoch:  143 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 524.0466200466201\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -0.24514754116535187\n",
      "==========================================\n",
      "Epoch:  144 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 522.3796296296297\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 0.793615996837616\n",
      "==========================================\n",
      "Epoch:  145 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 520.735632183908\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -0.11339873820543289\n",
      "==========================================\n",
      "Epoch:  146 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 519.1141552511416\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -0.3992778956890106\n",
      "==========================================\n",
      "Epoch:  147 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 517.5147392290249\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 1.276057481765747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  148 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 515.936936936937\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -0.985708475112915\n",
      "==========================================\n",
      "Epoch:  149 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 514.3803131991052\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 2.693247079849243\n",
      "==========================================\n",
      "Epoch:  150 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 512.8444444444444\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 2.222656488418579\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  151 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 511.32891832229586\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -0.5451768040657043\n",
      "==========================================\n",
      "Epoch:  152 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 509.83333333333337\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -1.2666854858398438\n",
      "==========================================\n",
      "Epoch:  153 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 508.35729847494554\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -1.7246042490005493\n",
      "==========================================\n",
      "Epoch:  154 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 506.9004329004329\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -5.138909339904785\n",
      "==========================================\n",
      "Epoch:  155 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 916.0\n",
      "Mean Reward of that batch 305.3333333333333\n",
      "Average Reward of all training: 505.6\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -3.2970192432403564\n",
      "==========================================\n",
      "Epoch:  156 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1076.0\n",
      "Mean Reward of that batch 358.6666666666667\n",
      "Average Reward of all training: 504.6581196581197\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -9.75832462310791\n",
      "==========================================\n",
      "Epoch:  157 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 504.4755838641189\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -26.688936233520508\n",
      "==========================================\n",
      "Epoch:  158 / 1000\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 988.0\n",
      "Mean Reward of that batch 988.0\n",
      "Average Reward of all training: 507.53586497890296\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -104.27337646484375\n",
      "==========================================\n",
      "Epoch:  159 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1236.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 506.9350104821803\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -87.1546630859375\n",
      "==========================================\n",
      "Epoch:  160 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1364.0\n",
      "Mean Reward of that batch 454.6666666666667\n",
      "Average Reward of all training: 506.6083333333334\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -5.6438798904418945\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  161 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 505.2256728778469\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 2.9845924377441406\n",
      "==========================================\n",
      "Epoch:  162 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 503.8600823045268\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 0.19830012321472168\n",
      "==========================================\n",
      "Epoch:  163 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 502.70756646216773\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 1.5776963233947754\n",
      "==========================================\n",
      "Epoch:  164 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 501.56910569105696\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 2.8063251972198486\n",
      "==========================================\n",
      "Epoch:  165 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 500.6383838383839\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 2.539801597595215\n",
      "==========================================\n",
      "Epoch:  166 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 499.7188755020081\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 2.2818944454193115\n",
      "==========================================\n",
      "Epoch:  167 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 980.0\n",
      "Mean Reward of that batch 326.6666666666667\n",
      "Average Reward of all training: 498.68263473053895\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 1.5494035482406616\n",
      "==========================================\n",
      "Epoch:  168 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 824.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 498.1666666666667\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 3.606637477874756\n",
      "==========================================\n",
      "Epoch:  169 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 496.89940828402365\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 1.2554981708526611\n",
      "==========================================\n",
      "Epoch:  170 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 495.6470588235294\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 1.1605887413024902\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  171 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 495.2514619883041\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 2.562258005142212\n",
      "==========================================\n",
      "Epoch:  172 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1112.0\n",
      "Mean Reward of that batch 556.0\n",
      "Average Reward of all training: 495.6046511627907\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 3.6324656009674072\n",
      "==========================================\n",
      "Epoch:  173 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 980.0\n",
      "Mean Reward of that batch 326.6666666666667\n",
      "Average Reward of all training: 494.62813102119463\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 0.25045281648635864\n",
      "==========================================\n",
      "Epoch:  174 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 980.0\n",
      "Mean Reward of that batch 326.6666666666667\n",
      "Average Reward of all training: 493.6628352490422\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 0.3571842312812805\n",
      "==========================================\n",
      "Epoch:  175 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 492.6476190476191\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 0.23252063989639282\n",
      "==========================================\n",
      "Epoch:  176 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 980.0\n",
      "Mean Reward of that batch 326.6666666666667\n",
      "Average Reward of all training: 491.70454545454544\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 0.05856122449040413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  177 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 884.0\n",
      "Mean Reward of that batch 294.6666666666667\n",
      "Average Reward of all training: 490.5913370998117\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 0.005217626690864563\n",
      "==========================================\n",
      "Epoch:  178 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 916.0\n",
      "Mean Reward of that batch 305.3333333333333\n",
      "Average Reward of all training: 489.55056179775283\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 0.0011556089157238603\n",
      "==========================================\n",
      "Epoch:  179 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 884.0\n",
      "Mean Reward of that batch 294.6666666666667\n",
      "Average Reward of all training: 488.4618249534451\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 0.02698262594640255\n",
      "==========================================\n",
      "Epoch:  180 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 487.6814814814815\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 1.1102149486541748\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  181 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1172.0\n",
      "Mean Reward of that batch 390.6666666666667\n",
      "Average Reward of all training: 487.145488029466\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 0.13323047757148743\n",
      "==========================================\n",
      "Epoch:  182 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1012.0\n",
      "Mean Reward of that batch 337.3333333333333\n",
      "Average Reward of all training: 486.32234432234435\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 0.37871992588043213\n",
      "==========================================\n",
      "Epoch:  183 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 824.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 485.9162112932605\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 3.8528051376342773\n",
      "==========================================\n",
      "Epoch:  184 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 484.8188405797102\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -8.298008918762207\n",
      "==========================================\n",
      "Epoch:  185 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 884.0\n",
      "Mean Reward of that batch 294.6666666666667\n",
      "Average Reward of all training: 483.790990990991\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -28.552183151245117\n",
      "==========================================\n",
      "Epoch:  186 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 888.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 483.57706093189967\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 0.932437002658844\n",
      "==========================================\n",
      "Epoch:  187 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 980.0\n",
      "Mean Reward of that batch 326.6666666666667\n",
      "Average Reward of all training: 482.7379679144385\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -2.144878387451172\n",
      "==========================================\n",
      "Epoch:  188 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1272.0\n",
      "Mean Reward of that batch 636.0\n",
      "Average Reward of all training: 483.5531914893617\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: -38.488128662109375\n",
      "==========================================\n",
      "Epoch:  189 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1108.0\n",
      "Mean Reward of that batch 369.3333333333333\n",
      "Average Reward of all training: 482.9488536155203\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 14.2127685546875\n",
      "==========================================\n",
      "Epoch:  190 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1176.0\n",
      "Mean Reward of that batch 588.0\n",
      "Average Reward of all training: 483.50175438596494\n",
      "Max reward for a batch so far: 2608.0\n",
      "Training Loss: 7.027392864227295\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  191 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 2764.0\n",
      "Mean Reward of that batch 921.3333333333334\n",
      "Average Reward of all training: 485.7940663176266\n",
      "Max reward for a batch so far: 2764.0\n",
      "Training Loss: 2.2725327014923096\n",
      "==========================================\n",
      "Epoch:  192 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1524.0\n",
      "Mean Reward of that batch 508.0\n",
      "Average Reward of all training: 485.9097222222222\n",
      "Max reward for a batch so far: 2764.0\n",
      "Training Loss: -1.4721893072128296\n",
      "==========================================\n",
      "Epoch:  193 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 2196.0\n",
      "Mean Reward of that batch 732.0\n",
      "Average Reward of all training: 487.1848013816925\n",
      "Max reward for a batch so far: 2764.0\n",
      "Training Loss: 9.09689712524414\n"
     ]
    }
   ],
   "source": [
    "# Keep track of all rewards total for each batch\n",
    "allRewards = []\n",
    "\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "mean_reward_total = []\n",
    "epoch = 1\n",
    "average_reward = []\n",
    "\n",
    "# Saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training:\n",
    "    # Load the model\n",
    "    #saver.restore(sess, \"./models/model.ckpt\")\n",
    "\n",
    "    while epoch < num_epochs + 1:\n",
    "        # Gather training data\n",
    "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, nb_episodes_mb = make_batch(batch_size, stacked_frames)\n",
    "\n",
    "        ### These part is used for analytics\n",
    "        # Calculate the total reward ot the batch\n",
    "        total_reward_of_that_batch = np.sum(rewards_of_batch)\n",
    "        allRewards.append(total_reward_of_that_batch)\n",
    "\n",
    "        # Calculate the mean reward of the batch\n",
    "        # Total rewards of batch / nb episodes in that batch\n",
    "        mean_reward_of_that_batch = np.divide(total_reward_of_that_batch, nb_episodes_mb)\n",
    "        mean_reward_total.append(mean_reward_of_that_batch)\n",
    "\n",
    "        # Calculate the average reward of all training\n",
    "        # mean_reward_of_that_batch / epoch\n",
    "        average_reward_of_all_training = np.divide(np.sum(mean_reward_total), epoch)\n",
    "\n",
    "        # Calculate maximum reward recorded \n",
    "        maximumRewardRecorded = np.amax(allRewards)\n",
    "\n",
    "        print(\"==========================================\")\n",
    "        print(\"Epoch: \", epoch, \"/\", num_epochs)\n",
    "        print(\"-----------\")\n",
    "        print(\"Number of training episodes: {}\".format(nb_episodes_mb))\n",
    "        print(\"Total reward: {}\".format(total_reward_of_that_batch, nb_episodes_mb))\n",
    "        print(\"Mean Reward of that batch {}\".format(mean_reward_of_that_batch))\n",
    "        print(\"Average Reward of all training: {}\".format(average_reward_of_all_training))\n",
    "        print(\"Max reward for a batch so far: {}\".format(maximumRewardRecorded))\n",
    "\n",
    "        # Feedforward, gradient and backpropagation\n",
    "        loss_, _ = sess.run([PGNetwork.loss, PGNetwork.train_opt], feed_dict={PGNetwork.inputs_: states_mb.reshape((len(states_mb), 84,84,4)),\n",
    "                                                            PGNetwork.actions: actions_mb,\n",
    "                                                                     PGNetwork.discounted_episode_rewards_: discounted_rewards_mb \n",
    "                                                                    })\n",
    "\n",
    "        print(\"Training Loss: {}\".format(loss_))\n",
    "\n",
    "        # Write TF Summaries\n",
    "        summary = sess.run(write_op, feed_dict={PGNetwork.inputs_: states_mb.reshape((len(states_mb), 84,84,4)),\n",
    "                                                            PGNetwork.actions: actions_mb,\n",
    "                                                                     PGNetwork.discounted_episode_rewards_: discounted_rewards_mb,\n",
    "                                                                    PGNetwork.mean_reward_: mean_reward_of_that_batch\n",
    "                                                                    })\n",
    "\n",
    "        #summary = sess.run(write_op, feed_dict={x: s_.reshape(len(s_),84,84,1), y:a_, d_r: d_r_, r: r_, n: n_})\n",
    "        writer.add_summary(summary, epoch)\n",
    "        writer.flush()\n",
    "\n",
    "        # Save Model\n",
    "        if epoch % 10 == 0:\n",
    "            saver.save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model saved\")\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
