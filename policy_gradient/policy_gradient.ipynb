{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "from environment_creation import create_environment\n",
    "import policy_gradient_network as pgn\n",
    "from frame_preprocessing import preprocess_frame\n",
    "from frames_stacking import stack_frames \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()\n",
    "\n",
    "stack_size = 4\n",
    "stacked_frames = deque([np.zeros((84,84), dtype = np.int) for i in range(stack_size)], maxlen = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards, gamma):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "\n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################\n",
    "###Environment hyperparameters\n",
    "state_size = [84, 84, 4] #our input is a stack of 4 frames , 84x84\n",
    "action_size = game.get_available_buttons_size() # 3 possible actions, turn left, turn right, go forward\n",
    "stack_size = 4 #how many frames are stacked together\n",
    "\n",
    "#deep learning model hyperparameters\n",
    "learning_rate = 0.002\n",
    "num_epochs = 1000\n",
    "\n",
    "batch_size = 2000\n",
    "gamma = 0.95 #discount rate\n",
    "\n",
    "training = True\n",
    "######################################\n",
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "PGNetwork = pgn.PGNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup tensorflow writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/pg/test\")\n",
    "\n",
    "#losses\n",
    "tf.summary.scalar(\"Loss\", PGNetwork.loss)\n",
    "\n",
    "tf.summary.scalar(\"Reward_mean\", PGNetwork.mean_reward_ )\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now is the time to train the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll create batches.\n",
    "These batches contains episodes (their number depends on how many rewards we collect: for instance if we have episodes with only 10 rewards we can put batch_size/10 episodes\n",
    "\n",
    "    Make a batch\n",
    "        For each step:\n",
    "            Choose action a\n",
    "            Perform action a\n",
    "            Store s, a, r\n",
    "            If done:\n",
    "                Calculate sum reward\n",
    "                Calculate gamma Gt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(batch_size, stacked_frames):\n",
    "    # Initialize lists: states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards\n",
    "    states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards = [], [], [], [], []\n",
    "    \n",
    "    # Reward of batch is also a trick to keep track of how many timestep we made.\n",
    "    # We use to to verify at the end of each episode if > batch_size or not.\n",
    "    \n",
    "    # Keep track of how many episodes in our batch (useful when we'll need to calculate the average reward per episode)\n",
    "    episode_num  = 1\n",
    "    \n",
    "    # Launch a new episode\n",
    "    game.new_episode()\n",
    "        \n",
    "    # Get a new state\n",
    "    state = game.get_state().screen_buffer\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, True, stack_size)\n",
    "\n",
    "    while True:\n",
    "        # Run State Through Policy & Calculate Action\n",
    "        action_probability_distribution = sess.run(PGNetwork.action_distribution, \n",
    "                                                   feed_dict={PGNetwork.inputs_: state.reshape(1, *state_size)})\n",
    "        \n",
    "        # REMEMBER THAT WE ARE IN A STOCHASTIC POLICY SO WE DON'T ALWAYS TAKE THE ACTION WITH THE HIGHEST PROBABILITY\n",
    "        # (For instance if the action with the best probability for state S is a1 with 70% chances, there is\n",
    "        #30% chance that we take action a2)\n",
    "        action = np.random.choice(range(action_probability_distribution.shape[1]), \n",
    "                                  p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "        action = possible_actions[action]\n",
    "\n",
    "        # Perform action\n",
    "        reward = game.make_action(action)\n",
    "        done = game.is_episode_finished()\n",
    "\n",
    "        # Store results\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards_of_episode.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            # The episode ends so no next state\n",
    "            next_state = np.zeros((84, 84), dtype=np.int)\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False, stack_size)\n",
    "            \n",
    "            # Append the rewards_of_batch to reward_of_episode\n",
    "            rewards_of_batch.append(rewards_of_episode)\n",
    "            \n",
    "            # Calculate gamma Gt\n",
    "            discounted_rewards.append(discount_and_normalize_rewards(rewards_of_episode, gamma))\n",
    "           \n",
    "            # If the number of rewards_of_batch > batch_size stop the minibatch creation\n",
    "            # (Because we have sufficient number of episode mb)\n",
    "            # Remember that we put this condition here, because we want entire episode (Monte Carlo)\n",
    "            # so we can't check that condition for each step but only if an episode is finished\n",
    "            if len(np.concatenate(rewards_of_batch)) > batch_size:\n",
    "                break\n",
    "                \n",
    "            # Reset the transition stores\n",
    "            rewards_of_episode = []\n",
    "            \n",
    "            # Add episode\n",
    "            episode_num += 1\n",
    "            \n",
    "            # Start a new episode\n",
    "            game.new_episode()\n",
    "\n",
    "            # First we need a state\n",
    "            state = game.get_state().screen_buffer\n",
    "\n",
    "            # Stack the frames\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True, stack_size)\n",
    "         \n",
    "        else:\n",
    "            # If not done, the next_state become the current state\n",
    "            next_state = game.get_state().screen_buffer\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False, stack_size)\n",
    "            state = next_state\n",
    "                         \n",
    "    return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards), episode_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the Neural Network\n",
    "- Init the weights\n",
    "- Init the environment\n",
    "- maxReward = 0 #keep track of max reward\n",
    "- for epochs in range(num_epochs):\n",
    "    - get batches\n",
    "    - optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  1 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1840.0\n",
      "Mean Reward of that batch 460.0\n",
      "Average Reward of all training: 460.0\n",
      "Max reward for a batch so far: 1840.0\n",
      "Training Loss: -0.0076517038978636265\n",
      "==========================================\n",
      "Epoch:  2 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1680.0\n",
      "Mean Reward of that batch 420.0\n",
      "Average Reward of all training: 440.0\n",
      "Max reward for a batch so far: 1840.0\n",
      "Training Loss: -0.010891149751842022\n",
      "==========================================\n",
      "Epoch:  3 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1840.0\n",
      "Mean Reward of that batch 460.0\n",
      "Average Reward of all training: 446.6666666666667\n",
      "Max reward for a batch so far: 1840.0\n",
      "Training Loss: -0.014711610041558743\n",
      "==========================================\n",
      "Epoch:  4 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1968.0\n",
      "Mean Reward of that batch 492.0\n",
      "Average Reward of all training: 458.0\n",
      "Max reward for a batch so far: 1968.0\n",
      "Training Loss: -0.0057493350468575954\n",
      "==========================================\n",
      "Epoch:  5 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1904.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 461.6\n",
      "Max reward for a batch so far: 1968.0\n",
      "Training Loss: 0.013580620288848877\n",
      "==========================================\n",
      "Epoch:  6 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2064.0\n",
      "Mean Reward of that batch 516.0\n",
      "Average Reward of all training: 470.6666666666667\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: -0.012366589158773422\n",
      "==========================================\n",
      "Epoch:  7 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1872.0\n",
      "Mean Reward of that batch 468.0\n",
      "Average Reward of all training: 470.2857142857143\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: -0.016132375225424767\n",
      "==========================================\n",
      "Epoch:  8 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1680.0\n",
      "Mean Reward of that batch 420.0\n",
      "Average Reward of all training: 464.0\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: -0.0133976386860013\n",
      "==========================================\n",
      "Epoch:  9 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1712.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 460.0\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: 0.024504095315933228\n",
      "==========================================\n",
      "Epoch:  10 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1968.0\n",
      "Mean Reward of that batch 492.0\n",
      "Average Reward of all training: 463.2\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: -0.04246050491929054\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  11 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1872.0\n",
      "Mean Reward of that batch 468.0\n",
      "Average Reward of all training: 463.6363636363636\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: -0.052199967205524445\n",
      "==========================================\n",
      "Epoch:  12 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1772.0\n",
      "Mean Reward of that batch 354.4\n",
      "Average Reward of all training: 454.5333333333333\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: -0.07306401431560516\n",
      "==========================================\n",
      "Epoch:  13 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1844.0\n",
      "Mean Reward of that batch 614.6666666666666\n",
      "Average Reward of all training: 466.85128205128206\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: -0.03334096819162369\n",
      "==========================================\n",
      "Epoch:  14 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1680.0\n",
      "Mean Reward of that batch 420.0\n",
      "Average Reward of all training: 463.5047619047619\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: 0.02154506742954254\n",
      "==========================================\n",
      "Epoch:  15 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1996.0\n",
      "Mean Reward of that batch 399.2\n",
      "Average Reward of all training: 459.21777777777777\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: 0.012127644382417202\n",
      "==========================================\n",
      "Epoch:  16 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1648.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 456.26666666666665\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: -0.00024169310927391052\n",
      "==========================================\n",
      "Epoch:  17 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1964.0\n",
      "Mean Reward of that batch 392.8\n",
      "Average Reward of all training: 452.5333333333333\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: 0.015019454061985016\n",
      "==========================================\n",
      "Epoch:  18 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1712.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 451.1703703703704\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: 0.02830083854496479\n",
      "==========================================\n",
      "Epoch:  19 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2064.0\n",
      "Mean Reward of that batch 516.0\n",
      "Average Reward of all training: 454.5824561403508\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: 0.003980438224971294\n",
      "==========================================\n",
      "Epoch:  20 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 1996.0\n",
      "Mean Reward of that batch 399.2\n",
      "Average Reward of all training: 451.81333333333333\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: 0.010962415486574173\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  21 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1936.0\n",
      "Mean Reward of that batch 484.0\n",
      "Average Reward of all training: 453.3460317460317\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: -0.00023389515990857035\n",
      "==========================================\n",
      "Epoch:  22 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1648.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 451.46666666666664\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: 0.02352815866470337\n",
      "==========================================\n",
      "Epoch:  23 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1712.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 450.4463768115942\n",
      "Max reward for a batch so far: 2064.0\n",
      "Training Loss: 0.028683453798294067\n",
      "==========================================\n",
      "Epoch:  24 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2128.0\n",
      "Mean Reward of that batch 532.0\n",
      "Average Reward of all training: 453.84444444444443\n",
      "Max reward for a batch so far: 2128.0\n",
      "Training Loss: 0.02835732512176037\n",
      "==========================================\n",
      "Epoch:  25 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1936.0\n",
      "Mean Reward of that batch 484.0\n",
      "Average Reward of all training: 455.05066666666664\n",
      "Max reward for a batch so far: 2128.0\n",
      "Training Loss: -0.010247544385492802\n",
      "==========================================\n",
      "Epoch:  26 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2192.0\n",
      "Mean Reward of that batch 548.0\n",
      "Average Reward of all training: 458.625641025641\n",
      "Max reward for a batch so far: 2192.0\n",
      "Training Loss: 0.00797234009951353\n",
      "==========================================\n",
      "Epoch:  27 / 1000\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2360.0\n",
      "Mean Reward of that batch 1180.0\n",
      "Average Reward of all training: 485.3432098765432\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: 0.018636628985404968\n",
      "==========================================\n",
      "Epoch:  28 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1748.0\n",
      "Mean Reward of that batch 582.6666666666666\n",
      "Average Reward of all training: 488.8190476190476\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: 0.03027176856994629\n",
      "==========================================\n",
      "Epoch:  29 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2000.0\n",
      "Mean Reward of that batch 500.0\n",
      "Average Reward of all training: 489.2045977011494\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: 0.007362270262092352\n",
      "==========================================\n",
      "Epoch:  30 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1744.0\n",
      "Mean Reward of that batch 436.0\n",
      "Average Reward of all training: 487.4311111111111\n",
      "Max reward for a batch so far: 2360.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.013952933251857758\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  31 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1712.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 485.51397849462364\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: -0.13070173561573029\n",
      "==========================================\n",
      "Epoch:  32 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2064.0\n",
      "Mean Reward of that batch 516.0\n",
      "Average Reward of all training: 486.4666666666667\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: 0.007631264626979828\n",
      "==========================================\n",
      "Epoch:  33 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1780.0\n",
      "Mean Reward of that batch 593.3333333333334\n",
      "Average Reward of all training: 489.70505050505056\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: 0.036599233746528625\n",
      "==========================================\n",
      "Epoch:  34 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2320.0\n",
      "Mean Reward of that batch 580.0\n",
      "Average Reward of all training: 492.3607843137256\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: -0.0027234386652708054\n",
      "==========================================\n",
      "Epoch:  35 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1840.0\n",
      "Mean Reward of that batch 460.0\n",
      "Average Reward of all training: 491.4361904761906\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: -0.03405342623591423\n",
      "==========================================\n",
      "Epoch:  36 / 1000\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 2292.0\n",
      "Mean Reward of that batch 764.0\n",
      "Average Reward of all training: 499.0074074074075\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: -0.030873192474246025\n",
      "==========================================\n",
      "Epoch:  37 / 1000\n",
      "-----------\n",
      "Number of training episodes: 5\n",
      "Total reward: 2220.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 497.5207207207208\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: 0.04438580572605133\n",
      "==========================================\n",
      "Epoch:  38 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1904.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 496.9543859649124\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: 0.035806238651275635\n",
      "==========================================\n",
      "Epoch:  39 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1840.0\n",
      "Mean Reward of that batch 460.0\n",
      "Average Reward of all training: 496.0068376068377\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: -0.009762478061020374\n",
      "==========================================\n",
      "Epoch:  40 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 1776.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 494.7066666666666\n",
      "Max reward for a batch so far: 2360.0\n",
      "Training Loss: -0.031927499920129776\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  41 / 1000\n",
      "-----------\n",
      "Number of training episodes: 4\n",
      "Total reward: 2544.0\n",
      "Mean Reward of that batch 636.0\n",
      "Average Reward of all training: 498.1528455284552\n",
      "Max reward for a batch so far: 2544.0\n",
      "Training Loss: 0.011976877227425575\n"
     ]
    }
   ],
   "source": [
    "# Keep track of all rewards total for each batch\n",
    "allRewards = []\n",
    "\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "mean_reward_total = []\n",
    "epoch = 1\n",
    "average_reward = []\n",
    "\n",
    "# Saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training:\n",
    "    # Load the model\n",
    "    #saver.restore(sess, \"./models/model.ckpt\")\n",
    "\n",
    "    while epoch < num_epochs + 1:\n",
    "        # Gather training data\n",
    "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, nb_episodes_mb = make_batch(batch_size, stacked_frames)\n",
    "\n",
    "        ### These part is used for analytics\n",
    "        # Calculate the total reward ot the batch\n",
    "        total_reward_of_that_batch = np.sum(rewards_of_batch)\n",
    "        allRewards.append(total_reward_of_that_batch)\n",
    "\n",
    "        # Calculate the mean reward of the batch\n",
    "        # Total rewards of batch / nb episodes in that batch\n",
    "        mean_reward_of_that_batch = np.divide(total_reward_of_that_batch, nb_episodes_mb)\n",
    "        mean_reward_total.append(mean_reward_of_that_batch)\n",
    "\n",
    "        # Calculate the average reward of all training\n",
    "        # mean_reward_of_that_batch / epoch\n",
    "        average_reward_of_all_training = np.divide(np.sum(mean_reward_total), epoch)\n",
    "\n",
    "        # Calculate maximum reward recorded \n",
    "        maximumRewardRecorded = np.amax(allRewards)\n",
    "\n",
    "        print(\"==========================================\")\n",
    "        print(\"Epoch: \", epoch, \"/\", num_epochs)\n",
    "        print(\"-----------\")\n",
    "        print(\"Number of training episodes: {}\".format(nb_episodes_mb))\n",
    "        print(\"Total reward: {}\".format(total_reward_of_that_batch, nb_episodes_mb))\n",
    "        print(\"Mean Reward of that batch {}\".format(mean_reward_of_that_batch))\n",
    "        print(\"Average Reward of all training: {}\".format(average_reward_of_all_training))\n",
    "        print(\"Max reward for a batch so far: {}\".format(maximumRewardRecorded))\n",
    "\n",
    "        # Feedforward, gradient and backpropagation\n",
    "        loss_, _ = sess.run([PGNetwork.loss, PGNetwork.train_opt], feed_dict={PGNetwork.inputs_: states_mb.reshape((len(states_mb), 84,84,4)),\n",
    "                                                            PGNetwork.actions: actions_mb,\n",
    "                                                                     PGNetwork.discounted_episode_rewards_: discounted_rewards_mb \n",
    "                                                                    })\n",
    "\n",
    "        print(\"Training Loss: {}\".format(loss_))\n",
    "\n",
    "        # Write TF Summaries\n",
    "        summary = sess.run(write_op, feed_dict={PGNetwork.inputs_: states_mb.reshape((len(states_mb), 84,84,4)),\n",
    "                                                            PGNetwork.actions: actions_mb,\n",
    "                                                                     PGNetwork.discounted_episode_rewards_: discounted_rewards_mb,\n",
    "                                                                    PGNetwork.mean_reward_: mean_reward_of_that_batch\n",
    "                                                                    })\n",
    "\n",
    "        #summary = sess.run(write_op, feed_dict={x: s_.reshape(len(s_),84,84,1), y:a_, d_r: d_r_, r: r_, n: n_})\n",
    "        writer.add_summary(summary, epoch)\n",
    "        writer.flush()\n",
    "\n",
    "        # Save Model\n",
    "        if epoch % 10 == 0:\n",
    "            saver.save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model saved\")\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
