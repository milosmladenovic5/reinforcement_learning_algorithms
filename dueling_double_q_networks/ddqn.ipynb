{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "from environment_creation import create_environment\n",
    "from frame_preprocessing import preprocess_frame\n",
    "from memory import Memory\n",
    "from dddq_neural_network import DDDQNNet\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((100,120), dtype=np.int) for i in range(stack_size)], maxlen=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<vizdoom.vizdoom.DoomGame at 0x1f9364a1b20>,\n",
       " [[1, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 1, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 1, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(stacked_frames, state, is_new_episode, stack_size):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((100,120), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [100,120,4]      # Our input is a stack of 4 frames hence 100x120x4 (Width, height, channels) \n",
    "action_size = game.get_available_buttons_size()              # 7 possible actions\n",
    "learning_rate =  0.00025      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 5000         # Total episodes for training\n",
    "max_steps = 5000              # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "\n",
    "# FIXED Q TARGETS HYPERPARAMETERS \n",
    "max_tau = 10000 #Tau is the C step where we update our target network\n",
    "\n",
    "# EXPLORATION HYPERPARAMETERS for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.00005            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q LEARNING hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "## If you have GPU change to 1million\n",
    "pretrain_length = 10000   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 10000       # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork = DDDQNNet(state_size, action_size, learning_rate, name=\"DQNetwork\")\n",
    "\n",
    "# Instantiate the target network\n",
    "TargetNetwork = DDDQNNet(state_size, action_size, learning_rate, name=\"TargetNetwork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(memory_size)\n",
    "\n",
    "# Render the environment\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True, stack_size)\n",
    "    \n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "\n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        #experience = np.hstack((state, [action, reward], next_state, done))\n",
    "        \n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        \n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "        \n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        \n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True, stack_size)\n",
    "        \n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False, stack_size)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        \n",
    "        # Our state is now the next_state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "With Ïµ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function helps us to copy one set of variables to another\n",
    "# In our case we use it when we want to copy the parameters of DQN to Target_network\n",
    "# Thanks of the very good implementation of Arthur Juliani https://github.com/awjuliani\n",
    "\n",
    "def update_target_graph():\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"DQNetwork\")\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TargetNetwork\")\n",
    "    \n",
    "    op_holder = []\n",
    "    \n",
    "    #update our target_network parameters with DQNNetwork parameters\n",
    "    for from_var, to_var in zip(from_vars, to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training algorithm:\n",
    "\n",
    "    Initialize the weights for DQN\n",
    "    Initialize target value weights w- <- w\n",
    "    Init the environment\n",
    "    Initialize the decay rate (that will use to reduce epsilon)\n",
    "\n",
    "    For episode to max_episode do\n",
    "        Make new episode\n",
    "        Set step to 0\n",
    "        Observe the first state $s_0$\n",
    "\n",
    "        While step < max_steps do:\n",
    "            Increase decay_rate\n",
    "            With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "            Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "\n",
    "            Store transition $\n",
    "\n",
    "            Sample random mini-batch from $D$: $$\n",
    "            Set target $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma Q(s',argmax_{a'}{Q(s', a', w), w^-)}$\n",
    "            Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
    "            Every C steps, reset: $w^- \\leftarrow w$\n",
    "        endfor\n",
    "\n",
    "    endfor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dddqn/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: -110.57624816894531 Training loss: 47.3862 Explore P: 0.9959\n",
      "Model Saved\n",
      "Episode: 1 Total reward: -115.95259094238281 Training loss: 37.8028 Explore P: 0.9935\n",
      "Episode: 2 Total reward: -112.43696594238281 Training loss: 29.0424 Explore P: 0.9895\n",
      "Episode: 3 Total reward: -58.77366638183594 Training loss: 35.3860 Explore P: 0.9855\n",
      "Episode: 4 Total reward: -115.42689514160156 Training loss: 9.7779 Explore P: 0.9815\n",
      "Episode: 5 Total reward: -84.46812438964844 Training loss: 8.8634 Explore P: 0.9775\n",
      "Model Saved\n",
      "Episode: 6 Total reward: -82.31275939941406 Training loss: 3.9465 Explore P: 0.9735\n",
      "Episode: 7 Total reward: -67.365234375 Training loss: 3.7157 Explore P: 0.9696\n",
      "Episode: 8 Total reward: -103.49986267089844 Training loss: 1.3084 Explore P: 0.9657\n",
      "Episode: 9 Total reward: -111.79777526855469 Training loss: 0.9416 Explore P: 0.9603\n",
      "Episode: 10 Total reward: -101.93240356445312 Training loss: 0.5273 Explore P: 0.9564\n",
      "Model Saved\n",
      "Episode: 11 Total reward: -44.43080139160156 Training loss: 1.0510 Explore P: 0.9526\n",
      "Episode: 12 Total reward: -113.26324462890625 Training loss: 0.8228 Explore P: 0.9488\n",
      "Episode: 13 Total reward: -115.25740051269531 Training loss: 0.7231 Explore P: 0.9449\n",
      "Episode: 14 Total reward: -105.43104553222656 Training loss: 0.5179 Explore P: 0.9395\n",
      "Episode: 15 Total reward: -72.8907470703125 Training loss: 0.8244 Explore P: 0.9357\n",
      "Model Saved\n",
      "Episode: 16 Total reward: -111.43414306640625 Training loss: 0.4807 Explore P: 0.9319\n",
      "Episode: 17 Total reward: -93.78120422363281 Training loss: 0.5860 Explore P: 0.9281\n",
      "Episode: 18 Total reward: -107.42170715332031 Training loss: 1.0644 Explore P: 0.9244\n",
      "Episode: 19 Total reward: -87.46961975097656 Training loss: 0.3300 Explore P: 0.9225\n",
      "Episode: 20 Total reward: -85.89599609375 Training loss: 0.1570 Explore P: 0.9188\n",
      "Model Saved\n",
      "Episode: 21 Total reward: -115.14254760742188 Training loss: 0.2024 Explore P: 0.9140\n",
      "Episode: 22 Total reward: -112.96113586425781 Training loss: 1.3245 Explore P: 0.9070\n",
      "Episode: 23 Total reward: -110.40142822265625 Training loss: 0.3219 Explore P: 0.8968\n",
      "Episode: 24 Total reward: -103.07545471191406 Training loss: 0.2510 Explore P: 0.8932\n",
      "Episode: 25 Total reward: -54.668304443359375 Training loss: 0.4664 Explore P: 0.8896\n",
      "Model Saved\n",
      "Episode: 26 Total reward: -101.23333740234375 Training loss: 0.1082 Explore P: 0.8860\n",
      "Episode: 27 Total reward: -114.55668640136719 Training loss: 0.2458 Explore P: 0.8793\n",
      "Episode: 28 Total reward: -114.28143310546875 Training loss: 0.2419 Explore P: 0.8745\n",
      "Episode: 29 Total reward: -102.45428466796875 Training loss: 0.1665 Explore P: 0.8710\n",
      "Episode: 30 Total reward: -110.67654418945312 Training loss: 0.1275 Explore P: 0.8676\n",
      "Model Saved\n",
      "Episode: 31 Total reward: -113.76687622070312 Training loss: 0.1533 Explore P: 0.8656\n",
      "Episode: 32 Total reward: -67.50326538085938 Training loss: 0.1995 Explore P: 0.8621\n",
      "Episode: 33 Total reward: -87.87358093261719 Training loss: 0.2032 Explore P: 0.8572\n",
      "Episode: 34 Total reward: -95.4019775390625 Training loss: 0.0677 Explore P: 0.8515\n",
      "Episode: 35 Total reward: -104.22200012207031 Training loss: 0.0873 Explore P: 0.8479\n",
      "Model Saved\n",
      "Episode: 36 Total reward: -92.10595703125 Training loss: 0.5439 Explore P: 0.8445\n",
      "Episode: 37 Total reward: -80.88345336914062 Training loss: 0.0838 Explore P: 0.8411\n",
      "Episode: 38 Total reward: -115.88780212402344 Training loss: 0.0750 Explore P: 0.8379\n",
      "Episode: 39 Total reward: -86.4864501953125 Training loss: 0.2906 Explore P: 0.8330\n",
      "Episode: 40 Total reward: -109.98979187011719 Training loss: 0.1413 Explore P: 0.8297\n",
      "Model Saved\n",
      "Episode: 41 Total reward: -97.68362426757812 Training loss: 0.3754 Explore P: 0.8263\n",
      "Episode: 42 Total reward: -111.51739501953125 Training loss: 0.2969 Explore P: 0.8231\n",
      "Episode: 43 Total reward: -115.95790100097656 Training loss: 0.2842 Explore P: 0.8176\n",
      "Episode: 44 Total reward: -106.40742492675781 Training loss: 0.1175 Explore P: 0.8155\n",
      "Episode: 45 Total reward: -81.2030029296875 Training loss: 0.1562 Explore P: 0.8122\n",
      "Model Saved\n",
      "Episode: 46 Total reward: -81.71223449707031 Training loss: 9.5015 Explore P: 0.8089\n",
      "Episode: 47 Total reward: -106.14628601074219 Training loss: 0.9540 Explore P: 0.8031\n",
      "Episode: 48 Total reward: -89.06175231933594 Training loss: 0.1041 Explore P: 0.7998\n",
      "Episode: 49 Total reward: -61.63665771484375 Training loss: 1.2556 Explore P: 0.7944\n",
      "Episode: 50 Total reward: -58.17083740234375 Training loss: 0.1147 Explore P: 0.7912\n",
      "Model Saved\n",
      "Episode: 51 Total reward: -93.52444458007812 Training loss: 0.2036 Explore P: 0.7894\n",
      "Episode: 52 Total reward: -105.98567199707031 Training loss: 0.0765 Explore P: 0.7862\n",
      "Episode: 53 Total reward: -106.86122131347656 Training loss: 0.3859 Explore P: 0.7844\n",
      "Episode: 54 Total reward: 16.922119140625 Training loss: 0.2644 Explore P: 0.7710\n",
      "Episode: 55 Total reward: -99.10430908203125 Training loss: 0.1222 Explore P: 0.7679\n",
      "Model Saved\n",
      "Episode: 56 Total reward: -67.21797180175781 Training loss: 0.3877 Explore P: 0.7559\n",
      "Episode: 57 Total reward: -57.916168212890625 Training loss: 0.0997 Explore P: 0.7531\n",
      "Episode: 58 Total reward: -41.19493103027344 Training loss: 0.0435 Explore P: 0.7459\n",
      "Episode: 59 Total reward: -101.75897216796875 Training loss: 0.4057 Explore P: 0.7429\n",
      "Episode: 60 Total reward: -115.97564697265625 Training loss: 0.0763 Explore P: 0.7410\n",
      "Model Saved\n",
      "Episode: 61 Total reward: -65.69337463378906 Training loss: 0.1221 Explore P: 0.7380\n",
      "Episode: 62 Total reward: -14.806747436523438 Training loss: 0.0882 Explore P: 0.7350\n",
      "Episode: 63 Total reward: -61.855072021484375 Training loss: 0.2806 Explore P: 0.7320\n",
      "Episode: 64 Total reward: -73.76068115234375 Training loss: 0.1088 Explore P: 0.7291\n",
      "Episode: 65 Total reward: -64.66096496582031 Training loss: 0.0912 Explore P: 0.7273\n",
      "Model Saved\n",
      "Episode: 66 Total reward: -57.882171630859375 Training loss: 0.2579 Explore P: 0.7246\n",
      "Episode: 67 Total reward: -73.98895263671875 Training loss: 0.4655 Explore P: 0.7216\n",
      "Episode: 68 Total reward: -112.26693725585938 Training loss: 1.0330 Explore P: 0.7189\n",
      "Episode: 69 Total reward: -88.38851928710938 Training loss: 0.0924 Explore P: 0.7162\n",
      "Episode: 70 Total reward: -108.0377197265625 Training loss: 0.4418 Explore P: 0.7122\n",
      "Model Saved\n",
      "Episode: 71 Total reward: -33.31935119628906 Training loss: 0.2373 Explore P: 0.7081\n",
      "Episode: 72 Total reward: -95.29672241210938 Training loss: 0.0998 Explore P: 0.7052\n",
      "Episode: 73 Total reward: -91.24900817871094 Training loss: 0.2910 Explore P: 0.7028\n",
      "Episode: 74 Total reward: -56.972137451171875 Training loss: 0.0728 Explore P: 0.6980\n",
      "Episode: 75 Total reward: -67.13563537597656 Training loss: 0.2857 Explore P: 0.6942\n",
      "Model Saved\n",
      "Episode: 76 Total reward: -113.54046630859375 Training loss: 0.5004 Explore P: 0.6916\n",
      "Episode: 77 Total reward: -105.18794250488281 Training loss: 0.1704 Explore P: 0.6901\n",
      "Episode: 78 Total reward: -102.19033813476562 Training loss: 0.1643 Explore P: 0.6863\n",
      "Episode: 79 Total reward: -99.78929138183594 Training loss: 0.1683 Explore P: 0.6836\n",
      "Episode: 80 Total reward: -49.175933837890625 Training loss: 0.4032 Explore P: 0.6808\n",
      "Model Saved\n",
      "Episode: 81 Total reward: -94.73429870605469 Training loss: 0.5042 Explore P: 0.6773\n",
      "Episode: 82 Total reward: -63.18017578125 Training loss: 0.1478 Explore P: 0.6745\n",
      "Episode: 83 Total reward: -47.103912353515625 Training loss: 0.1060 Explore P: 0.6718\n",
      "Episode: 84 Total reward: -102.6717529296875 Training loss: 2.9434 Explore P: 0.6703\n",
      "Episode: 85 Total reward: -114.38560485839844 Training loss: 0.2655 Explore P: 0.6687\n",
      "Model Saved\n",
      "Episode: 86 Total reward: -88.43199157714844 Training loss: 0.2473 Explore P: 0.6653\n",
      "Episode: 87 Total reward: -78.45326232910156 Training loss: 0.2394 Explore P: 0.6626\n",
      "Episode: 88 Total reward: -64.41020202636719 Training loss: 0.2498 Explore P: 0.6599\n",
      "Episode: 89 Total reward: -94.80433654785156 Training loss: 0.2048 Explore P: 0.6561\n",
      "Episode: 90 Total reward: -64.23954772949219 Training loss: 0.0415 Explore P: 0.6535\n",
      "Model Saved\n",
      "Episode: 91 Total reward: -39.430938720703125 Training loss: 0.0859 Explore P: 0.6508\n",
      "Episode: 92 Total reward: -26.778671264648438 Training loss: 0.3404 Explore P: 0.6472\n",
      "Episode: 93 Total reward: -101.60581970214844 Training loss: 0.1565 Explore P: 0.6412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 94 Total reward: -109.33488464355469 Training loss: 0.1681 Explore P: 0.6396\n",
      "Episode: 95 Total reward: -86.29859924316406 Training loss: 0.6195 Explore P: 0.6370\n",
      "Model Saved\n",
      "Episode: 96 Total reward: -42.55110168457031 Training loss: 0.8006 Explore P: 0.6335\n",
      "Episode: 97 Total reward: -111.52056884765625 Training loss: 0.2566 Explore P: 0.6320\n",
      "Episode: 98 Total reward: -82.16763305664062 Training loss: 0.8413 Explore P: 0.6249\n",
      "Episode: 99 Total reward: -67.3795166015625 Training loss: 0.0562 Explore P: 0.6224\n",
      "Episode: 100 Total reward: -104.04939270019531 Training loss: 0.1029 Explore P: 0.6198\n",
      "Model Saved\n",
      "Episode: 101 Total reward: -45.13116455078125 Training loss: 0.3585 Explore P: 0.6173\n",
      "Episode: 102 Total reward: -66.38711547851562 Training loss: 0.0875 Explore P: 0.6150\n",
      "Episode: 103 Total reward: -75.03628540039062 Training loss: 0.0745 Explore P: 0.6128\n",
      "Model updated\n",
      "Episode: 104 Total reward: -67.68966674804688 Training loss: 8.9984 Explore P: 0.6103\n",
      "Episode: 105 Total reward: -109.93154907226562 Training loss: 0.6192 Explore P: 0.6079\n",
      "Model Saved\n",
      "Episode: 106 Total reward: -87.59782409667969 Training loss: 0.1662 Explore P: 0.6054\n",
      "Episode: 107 Total reward: -60.56852722167969 Training loss: 0.3182 Explore P: 0.6030\n",
      "Episode: 108 Total reward: -97.55758666992188 Training loss: 0.1496 Explore P: 0.6006\n",
      "Episode: 109 Total reward: -17.355392456054688 Training loss: 0.4024 Explore P: 0.5958\n",
      "Episode: 110 Total reward: -77.68241882324219 Training loss: 0.1365 Explore P: 0.5934\n",
      "Model Saved\n",
      "Episode: 111 Total reward: -73.14675903320312 Training loss: 0.2301 Explore P: 0.5910\n",
      "Episode: 112 Total reward: -99.83575439453125 Training loss: 0.0743 Explore P: 0.5886\n",
      "Episode: 113 Total reward: -89.61245727539062 Training loss: 0.1579 Explore P: 0.5863\n",
      "Episode: 114 Total reward: -107.24832153320312 Training loss: 0.0956 Explore P: 0.5791\n",
      "Episode: 115 Total reward: -73.28311157226562 Training loss: 0.0931 Explore P: 0.5768\n",
      "Model Saved\n",
      "Episode: 116 Total reward: -100.79620361328125 Training loss: 0.2174 Explore P: 0.5737\n",
      "Episode: 117 Total reward: -106.45921325683594 Training loss: 0.1302 Explore P: 0.5714\n",
      "Episode: 118 Total reward: -64.92031860351562 Training loss: 0.3192 Explore P: 0.5693\n",
      "Episode: 119 Total reward: -78.89912414550781 Training loss: 0.4042 Explore P: 0.5671\n",
      "Episode: 120 Total reward: -76.06712341308594 Training loss: 0.2530 Explore P: 0.5642\n",
      "Model Saved\n",
      "Episode: 121 Total reward: -6.3708343505859375 Training loss: 0.2746 Explore P: 0.5605\n",
      "Episode: 122 Total reward: -105.24613952636719 Training loss: 0.6133 Explore P: 0.5582\n",
      "Episode: 123 Total reward: -2.08026123046875 Training loss: 0.2485 Explore P: 0.5560\n",
      "Episode: 124 Total reward: -48.008575439453125 Training loss: 0.8082 Explore P: 0.5538\n",
      "Episode: 125 Total reward: -6.004730224609375 Training loss: 0.1613 Explore P: 0.5485\n",
      "Model Saved\n",
      "Episode: 126 Total reward: -58.566131591796875 Training loss: 0.4741 Explore P: 0.5463\n",
      "Episode: 127 Total reward: -89.88606262207031 Training loss: 0.1559 Explore P: 0.5449\n",
      "Episode: 128 Total reward: -59.30433654785156 Training loss: 0.3879 Explore P: 0.5429\n",
      "Episode: 129 Total reward: -100.76324462890625 Training loss: 0.1841 Explore P: 0.5409\n",
      "Episode: 130 Total reward: -4.9305267333984375 Training loss: 0.1762 Explore P: 0.5365\n",
      "Model Saved\n",
      "Episode: 131 Total reward: -73.46726989746094 Training loss: 0.3166 Explore P: 0.5345\n",
      "Episode: 132 Total reward: -106.2451171875 Training loss: 0.1730 Explore P: 0.5324\n",
      "Episode: 133 Total reward: -114.55271911621094 Training loss: 0.2039 Explore P: 0.5310\n",
      "Episode: 134 Total reward: -74.72467041015625 Training loss: 0.6886 Explore P: 0.5289\n",
      "Episode: 135 Total reward: -87.17230224609375 Training loss: 0.2110 Explore P: 0.5278\n",
      "Model Saved\n",
      "Episode: 136 Total reward: -107.02082824707031 Training loss: 0.3265 Explore P: 0.5256\n",
      "Episode: 137 Total reward: -114.33203125 Training loss: 0.3377 Explore P: 0.5235\n",
      "Episode: 138 Total reward: -22.834060668945312 Training loss: 0.4922 Explore P: 0.5201\n",
      "Episode: 139 Total reward: -93.82534790039062 Training loss: 0.0863 Explore P: 0.5179\n",
      "Episode: 140 Total reward: -115.99432373046875 Training loss: 0.1869 Explore P: 0.5142\n",
      "Model Saved\n",
      "Episode: 141 Total reward: -85.100830078125 Training loss: 0.1834 Explore P: 0.5122\n",
      "Episode: 142 Total reward: -95.23988342285156 Training loss: 0.2072 Explore P: 0.5102\n",
      "Episode: 143 Total reward: -99.69158935546875 Training loss: 0.2755 Explore P: 0.5077\n",
      "Episode: 144 Total reward: -98.0950927734375 Training loss: 0.1561 Explore P: 0.5058\n",
      "Episode: 145 Total reward: -54.70918273925781 Training loss: 0.1716 Explore P: 0.5037\n",
      "Model Saved\n",
      "Episode: 146 Total reward: -86.59300231933594 Training loss: 0.1124 Explore P: 0.5017\n",
      "Episode: 147 Total reward: -104.86930847167969 Training loss: 0.7429 Explore P: 0.4990\n",
      "Episode: 148 Total reward: -111.69618225097656 Training loss: 0.2890 Explore P: 0.4934\n",
      "Episode: 149 Total reward: -55.27000427246094 Training loss: 0.0879 Explore P: 0.4915\n",
      "Episode: 150 Total reward: -106.46969604492188 Training loss: 0.1337 Explore P: 0.4895\n",
      "Model Saved\n",
      "Episode: 151 Total reward: -56.16484069824219 Training loss: 0.5362 Explore P: 0.4875\n",
      "Episode: 152 Total reward: -23.303329467773438 Training loss: 0.3782 Explore P: 0.4843\n",
      "Episode: 153 Total reward: -69.6636962890625 Training loss: 0.1744 Explore P: 0.4824\n",
      "Episode: 154 Total reward: -76.29000854492188 Training loss: 0.1055 Explore P: 0.4788\n",
      "Episode: 155 Total reward: -97.36405944824219 Training loss: 0.2859 Explore P: 0.4770\n",
      "Model Saved\n",
      "Episode: 156 Total reward: -76.5408935546875 Training loss: 2.2949 Explore P: 0.4746\n",
      "Episode: 157 Total reward: -63.22227478027344 Training loss: 0.4189 Explore P: 0.4704\n",
      "Episode: 158 Total reward: -113.19038391113281 Training loss: 0.2332 Explore P: 0.4688\n",
      "Episode: 159 Total reward: -83.05870056152344 Training loss: 0.2134 Explore P: 0.4669\n",
      "Episode: 160 Total reward: -95.27589416503906 Training loss: 0.5146 Explore P: 0.4658\n",
      "Model Saved\n",
      "Episode: 161 Total reward: -114.90570068359375 Training loss: 4.4221 Explore P: 0.4654\n",
      "Episode: 162 Total reward: -63.08384704589844 Training loss: 0.4772 Explore P: 0.4637\n",
      "Episode: 163 Total reward: -78.95556640625 Training loss: 0.5891 Explore P: 0.4619\n",
      "Episode: 164 Total reward: -68.38482666015625 Training loss: 0.1903 Explore P: 0.4600\n",
      "Episode: 165 Total reward: -115.97563171386719 Training loss: 0.2091 Explore P: 0.4581\n",
      "Model Saved\n",
      "Episode: 166 Total reward: -90.06625366210938 Training loss: 0.6527 Explore P: 0.4570\n",
      "Episode: 167 Total reward: -80.36436462402344 Training loss: 0.2403 Explore P: 0.4540\n",
      "Episode: 168 Total reward: -66.20278930664062 Training loss: 0.1520 Explore P: 0.4529\n",
      "Episode: 169 Total reward: -54.29695129394531 Training loss: 0.2495 Explore P: 0.4490\n",
      "Episode: 170 Total reward: -67.53553771972656 Training loss: 0.3030 Explore P: 0.4472\n",
      "Model Saved\n",
      "Episode: 171 Total reward: -84.20748901367188 Training loss: 0.1970 Explore P: 0.4454\n",
      "Episode: 172 Total reward: -99.811279296875 Training loss: 0.7367 Explore P: 0.4432\n",
      "Episode: 173 Total reward: -91.05644226074219 Training loss: 0.2549 Explore P: 0.4414\n",
      "Episode: 174 Total reward: -91.40521240234375 Training loss: 0.2089 Explore P: 0.4389\n",
      "Episode: 175 Total reward: -115.21299743652344 Training loss: 0.1456 Explore P: 0.4378\n",
      "Model Saved\n",
      "Episode: 176 Total reward: -107.20516967773438 Training loss: 0.2017 Explore P: 0.4368\n",
      "Episode: 177 Total reward: -82.05941772460938 Training loss: 0.1205 Explore P: 0.4351\n",
      "Episode: 178 Total reward: -83.89656066894531 Training loss: 0.1592 Explore P: 0.4310\n",
      "Episode: 179 Total reward: -102.89784240722656 Training loss: 0.2071 Explore P: 0.4287\n",
      "Episode: 180 Total reward: -73.1785888671875 Training loss: 0.6508 Explore P: 0.4269\n",
      "Model Saved\n",
      "Episode: 181 Total reward: -109.47611999511719 Training loss: 0.2210 Explore P: 0.4252\n",
      "Episode: 182 Total reward: -106.92767333984375 Training loss: 0.2910 Explore P: 0.4243\n",
      "Episode: 183 Total reward: -27.067825317382812 Training loss: 0.2383 Explore P: 0.4227\n",
      "Episode: 184 Total reward: -115.97633361816406 Training loss: 0.1041 Explore P: 0.4217\n",
      "Episode: 185 Total reward: -78.22724914550781 Training loss: 0.4794 Explore P: 0.4196\n",
      "Model Saved\n",
      "Episode: 186 Total reward: -98.52328491210938 Training loss: 0.1853 Explore P: 0.4179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 187 Total reward: -115.94224548339844 Training loss: 0.0661 Explore P: 0.4162\n",
      "Episode: 188 Total reward: -56.39424133300781 Training loss: 0.2189 Explore P: 0.4146\n",
      "Episode: 189 Total reward: -96.15127563476562 Training loss: 0.5088 Explore P: 0.4129\n",
      "Episode: 190 Total reward: -114.42535400390625 Training loss: 0.8223 Explore P: 0.4112\n",
      "Model Saved\n",
      "Episode: 191 Total reward: -81.76736450195312 Training loss: 0.4091 Explore P: 0.4091\n",
      "Episode: 192 Total reward: -87.98257446289062 Training loss: 1.1431 Explore P: 0.4075\n",
      "Episode: 193 Total reward: -78.02105712890625 Training loss: 0.1352 Explore P: 0.4058\n",
      "Episode: 194 Total reward: -77.38835144042969 Training loss: 0.7857 Explore P: 0.4042\n",
      "Episode: 195 Total reward: -103.32853698730469 Training loss: 0.1596 Explore P: 0.4028\n",
      "Model Saved\n",
      "Episode: 196 Total reward: -99.25189208984375 Training loss: 0.1789 Explore P: 0.4018\n",
      "Episode: 197 Total reward: -62.856231689453125 Training loss: 0.3685 Explore P: 0.4002\n",
      "Episode: 198 Total reward: -74.52397155761719 Training loss: 0.3395 Explore P: 0.3971\n",
      "Episode: 199 Total reward: -58.616363525390625 Training loss: 0.4277 Explore P: 0.3954\n",
      "Episode: 200 Total reward: -93.18226623535156 Training loss: 0.6151 Explore P: 0.3939\n",
      "Model Saved\n",
      "Episode: 201 Total reward: -106.49208068847656 Training loss: 0.1152 Explore P: 0.3930\n",
      "Episode: 202 Total reward: -43.55522155761719 Training loss: 1.4787 Explore P: 0.3914\n",
      "Episode: 203 Total reward: -77.47433471679688 Training loss: 0.1496 Explore P: 0.3898\n",
      "Episode: 204 Total reward: -87.87641906738281 Training loss: 0.4988 Explore P: 0.3883\n",
      "Episode: 205 Total reward: -78.42184448242188 Training loss: 0.5401 Explore P: 0.3867\n",
      "Model Saved\n",
      "Episode: 206 Total reward: -73.97557067871094 Training loss: 0.3620 Explore P: 0.3852\n",
      "Episode: 207 Total reward: -90.18058776855469 Training loss: 0.4797 Explore P: 0.3831\n",
      "Episode: 208 Total reward: -66.60293579101562 Training loss: 0.1428 Explore P: 0.3804\n",
      "Episode: 209 Total reward: -112.03414916992188 Training loss: 0.1638 Explore P: 0.3784\n",
      "Episode: 210 Total reward: -115.95138549804688 Training loss: 0.2059 Explore P: 0.3769\n",
      "Model Saved\n",
      "Episode: 211 Total reward: -100.93698120117188 Training loss: 0.2629 Explore P: 0.3754\n",
      "Episode: 212 Total reward: -77.34519958496094 Training loss: 0.2620 Explore P: 0.3745\n",
      "Model updated\n",
      "Episode: 213 Total reward: -62.43818664550781 Training loss: 0.3646 Explore P: 0.3730\n",
      "Episode: 214 Total reward: -54.87347412109375 Training loss: 0.2875 Explore P: 0.3716\n",
      "Episode: 215 Total reward: -97.44142150878906 Training loss: 1.1756 Explore P: 0.3700\n",
      "Model Saved\n",
      "Episode: 216 Total reward: -19.843856811523438 Training loss: 0.4778 Explore P: 0.3680\n",
      "Episode: 217 Total reward: -83.59512329101562 Training loss: 0.1639 Explore P: 0.3665\n",
      "Episode: 218 Total reward: -69.58538818359375 Training loss: 0.2256 Explore P: 0.3650\n",
      "Episode: 219 Total reward: -71.19065856933594 Training loss: 0.3311 Explore P: 0.3615\n",
      "Episode: 220 Total reward: -105.51364135742188 Training loss: 0.1646 Explore P: 0.3607\n",
      "Model Saved\n",
      "Episode: 221 Total reward: -78.41278076171875 Training loss: 0.4736 Explore P: 0.3599\n",
      "Episode: 222 Total reward: -115.21090698242188 Training loss: 0.2758 Explore P: 0.3585\n",
      "Episode: 223 Total reward: -74.41629028320312 Training loss: 0.2504 Explore P: 0.3570\n",
      "Episode: 224 Total reward: -78.74542236328125 Training loss: 0.2380 Explore P: 0.3556\n",
      "Episode: 225 Total reward: -95.74310302734375 Training loss: 0.1416 Explore P: 0.3542\n",
      "Model Saved\n",
      "Episode: 226 Total reward: -111.78538513183594 Training loss: 0.1706 Explore P: 0.3528\n",
      "Episode: 227 Total reward: -115.9755859375 Training loss: 0.2377 Explore P: 0.3514\n",
      "Episode: 228 Total reward: -115.78422546386719 Training loss: 0.3615 Explore P: 0.3506\n",
      "Episode: 229 Total reward: -73.28564453125 Training loss: 0.4125 Explore P: 0.3492\n",
      "Episode: 230 Total reward: -105.81285095214844 Training loss: 0.2830 Explore P: 0.3478\n",
      "Model Saved\n",
      "Episode: 231 Total reward: -108.8487548828125 Training loss: 0.3065 Explore P: 0.3464\n",
      "Episode: 232 Total reward: -89.53152465820312 Training loss: 0.2326 Explore P: 0.3450\n",
      "Episode: 233 Total reward: -115.99822998046875 Training loss: 0.4093 Explore P: 0.3437\n",
      "Episode: 234 Total reward: -84.78355407714844 Training loss: 0.3233 Explore P: 0.3425\n",
      "Episode: 235 Total reward: -88.55369567871094 Training loss: 0.2620 Explore P: 0.3408\n",
      "Model Saved\n",
      "Episode: 236 Total reward: -110.32965087890625 Training loss: 0.4132 Explore P: 0.3394\n",
      "Episode: 237 Total reward: -67.7196044921875 Training loss: 0.3233 Explore P: 0.3375\n",
      "Episode: 238 Total reward: -108.59910583496094 Training loss: 0.2658 Explore P: 0.3368\n",
      "Episode: 239 Total reward: -115.94966125488281 Training loss: 0.2552 Explore P: 0.3354\n",
      "Episode: 240 Total reward: -80.22972106933594 Training loss: 0.1809 Explore P: 0.3338\n",
      "Model Saved\n",
      "Episode: 241 Total reward: -115.64422607421875 Training loss: 0.2652 Explore P: 0.3331\n",
      "Episode: 242 Total reward: -109.16497802734375 Training loss: 0.3408 Explore P: 0.3319\n",
      "Episode: 243 Total reward: -115.99937438964844 Training loss: 0.1701 Explore P: 0.3311\n",
      "Episode: 244 Total reward: -114.57646179199219 Training loss: 0.4847 Explore P: 0.3298\n",
      "Episode: 245 Total reward: -103.17759704589844 Training loss: 1.3811 Explore P: 0.3285\n",
      "Model Saved\n",
      "Episode: 246 Total reward: -91.25216674804688 Training loss: 0.3774 Explore P: 0.3272\n",
      "Episode: 247 Total reward: -115.99264526367188 Training loss: 0.3944 Explore P: 0.3259\n",
      "Episode: 248 Total reward: -105.42620849609375 Training loss: 0.2605 Explore P: 0.3242\n",
      "Episode: 249 Total reward: -72.64826965332031 Training loss: 0.4709 Explore P: 0.3201\n",
      "Episode: 250 Total reward: -90.60597229003906 Training loss: 0.3153 Explore P: 0.3189\n",
      "Model Saved\n",
      "Episode: 251 Total reward: -115.8013916015625 Training loss: 0.2517 Explore P: 0.3176\n",
      "Episode: 252 Total reward: -98.19393920898438 Training loss: 0.5711 Explore P: 0.3164\n",
      "Episode: 253 Total reward: -105.91596984863281 Training loss: 0.7935 Explore P: 0.3151\n",
      "Episode: 254 Total reward: -74.41314697265625 Training loss: 0.4073 Explore P: 0.3134\n",
      "Episode: 255 Total reward: -102.82391357421875 Training loss: 0.4092 Explore P: 0.3117\n",
      "Model Saved\n",
      "Episode: 256 Total reward: -99.55276489257812 Training loss: 0.1879 Explore P: 0.3105\n",
      "Episode: 257 Total reward: -83.33204650878906 Training loss: 0.3474 Explore P: 0.3093\n",
      "Episode: 258 Total reward: -102.65237426757812 Training loss: 0.2285 Explore P: 0.3081\n",
      "Episode: 259 Total reward: -80.6038818359375 Training loss: 0.1929 Explore P: 0.3069\n",
      "Episode: 260 Total reward: -115.99957275390625 Training loss: 0.2344 Explore P: 0.3054\n",
      "Model Saved\n",
      "Episode: 261 Total reward: -73.35359191894531 Training loss: 0.1315 Explore P: 0.3046\n",
      "Episode: 262 Total reward: -115.94442749023438 Training loss: 0.0945 Explore P: 0.3034\n",
      "Episode: 263 Total reward: -79.90225219726562 Training loss: 0.2509 Explore P: 0.3018\n",
      "Episode: 264 Total reward: -101.80052185058594 Training loss: 0.1908 Explore P: 0.3007\n",
      "Episode: 265 Total reward: -67.46778869628906 Training loss: 0.7736 Explore P: 0.2995\n",
      "Model Saved\n",
      "Episode: 266 Total reward: -92.06790161132812 Training loss: 0.1769 Explore P: 0.2983\n",
      "Episode: 267 Total reward: -83.7520751953125 Training loss: 0.6167 Explore P: 0.2971\n",
      "Episode: 268 Total reward: -92.18733215332031 Training loss: 0.3070 Explore P: 0.2960\n",
      "Episode: 269 Total reward: -106.98373413085938 Training loss: 0.5848 Explore P: 0.2949\n",
      "Episode: 270 Total reward: -88.64823913574219 Training loss: 0.5017 Explore P: 0.2925\n",
      "Model Saved\n",
      "Episode: 271 Total reward: -107.51461791992188 Training loss: 0.5855 Explore P: 0.2913\n",
      "Episode: 272 Total reward: -87.05165100097656 Training loss: 0.2292 Explore P: 0.2902\n",
      "Episode: 273 Total reward: -103.54135131835938 Training loss: 0.2616 Explore P: 0.2895\n",
      "Episode: 274 Total reward: -80.34268188476562 Training loss: 0.8623 Explore P: 0.2878\n",
      "Episode: 275 Total reward: -105.24945068359375 Training loss: 0.5339 Explore P: 0.2864\n",
      "Model Saved\n",
      "Episode: 276 Total reward: -74.54225158691406 Training loss: 0.2590 Explore P: 0.2854\n",
      "Episode: 277 Total reward: -92.58828735351562 Training loss: 0.9166 Explore P: 0.2843\n",
      "Episode: 278 Total reward: -89.59934997558594 Training loss: 0.2955 Explore P: 0.2836\n",
      "Episode: 279 Total reward: -78.41238403320312 Training loss: 0.4547 Explore P: 0.2825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 280 Total reward: -114.20535278320312 Training loss: 0.2286 Explore P: 0.2814\n",
      "Model Saved\n",
      "Episode: 281 Total reward: -94.4296875 Training loss: 0.6993 Explore P: 0.2803\n",
      "Episode: 282 Total reward: -83.23300170898438 Training loss: 0.7476 Explore P: 0.2797\n",
      "Episode: 283 Total reward: -76.07620239257812 Training loss: 0.9076 Explore P: 0.2786\n",
      "Episode: 284 Total reward: -96.82943725585938 Training loss: 0.3188 Explore P: 0.2775\n",
      "Episode: 285 Total reward: -61.340728759765625 Training loss: 0.5037 Explore P: 0.2760\n",
      "Model Saved\n",
      "Episode: 286 Total reward: -115.99931335449219 Training loss: 0.2523 Explore P: 0.2749\n",
      "Episode: 287 Total reward: -76.28294372558594 Training loss: 0.2646 Explore P: 0.2742\n",
      "Episode: 288 Total reward: -76.76051330566406 Training loss: 0.3316 Explore P: 0.2731\n",
      "Episode: 289 Total reward: -75.72247314453125 Training loss: 0.5217 Explore P: 0.2720\n",
      "Episode: 290 Total reward: -86.41487121582031 Training loss: 0.2574 Explore P: 0.2710\n",
      "Model Saved\n",
      "Episode: 291 Total reward: -38.39674377441406 Training loss: 0.1648 Explore P: 0.2699\n",
      "Episode: 292 Total reward: -73.01211547851562 Training loss: 0.2364 Explore P: 0.2688\n",
      "Episode: 293 Total reward: -79.89492797851562 Training loss: 0.4035 Explore P: 0.2678\n",
      "Episode: 294 Total reward: -65.24530029296875 Training loss: 0.2995 Explore P: 0.2667\n",
      "Episode: 295 Total reward: -111.77873229980469 Training loss: 0.2187 Explore P: 0.2661\n",
      "Model Saved\n",
      "Episode: 296 Total reward: -84.15533447265625 Training loss: 0.5704 Explore P: 0.2650\n",
      "Episode: 297 Total reward: -109.53398132324219 Training loss: 0.3303 Explore P: 0.2640\n",
      "Episode: 298 Total reward: -113.33515930175781 Training loss: 0.6393 Explore P: 0.2625\n",
      "Episode: 299 Total reward: -90.33969116210938 Training loss: 0.2747 Explore P: 0.2619\n",
      "Episode: 300 Total reward: -104.28242492675781 Training loss: 7.6679 Explore P: 0.2609\n",
      "Model Saved\n",
      "Episode: 301 Total reward: -96.79508972167969 Training loss: 0.3903 Explore P: 0.2598\n",
      "Episode: 302 Total reward: -115.72059631347656 Training loss: 0.4767 Explore P: 0.2585\n",
      "Episode: 303 Total reward: -69.74044799804688 Training loss: 0.4693 Explore P: 0.2575\n",
      "Episode: 304 Total reward: -109.75302124023438 Training loss: 0.6629 Explore P: 0.2564\n",
      "Episode: 305 Total reward: -64.14677429199219 Training loss: 0.2742 Explore P: 0.2554\n",
      "Model Saved\n",
      "Episode: 306 Total reward: -113.6986083984375 Training loss: 0.2887 Explore P: 0.2541\n",
      "Episode: 307 Total reward: -69.47160339355469 Training loss: 0.7991 Explore P: 0.2531\n",
      "Episode: 308 Total reward: -98.81703186035156 Training loss: 0.3269 Explore P: 0.2518\n",
      "Episode: 309 Total reward: -114.48722839355469 Training loss: 0.7232 Explore P: 0.2508\n",
      "Episode: 310 Total reward: -91.52537536621094 Training loss: 0.2180 Explore P: 0.2498\n",
      "Model Saved\n",
      "Episode: 311 Total reward: -107.46186828613281 Training loss: 0.2999 Explore P: 0.2484\n",
      "Episode: 312 Total reward: -70.55305480957031 Training loss: 1.8952 Explore P: 0.2471\n",
      "Episode: 313 Total reward: -98.66856384277344 Training loss: 0.6005 Explore P: 0.2461\n",
      "Episode: 314 Total reward: -101.48191833496094 Training loss: 0.5644 Explore P: 0.2449\n",
      "Episode: 315 Total reward: -91.14457702636719 Training loss: 0.1935 Explore P: 0.2444\n",
      "Model Saved\n",
      "Episode: 316 Total reward: -92.67770385742188 Training loss: 0.2559 Explore P: 0.2434\n",
      "Episode: 317 Total reward: -73.84175109863281 Training loss: 0.4443 Explore P: 0.2425\n",
      "Episode: 318 Total reward: -83.43296813964844 Training loss: 0.5654 Explore P: 0.2416\n",
      "Episode: 319 Total reward: -85.15994262695312 Training loss: 0.1947 Explore P: 0.2406\n",
      "Episode: 320 Total reward: -115.97639465332031 Training loss: 0.4046 Explore P: 0.2393\n",
      "Model Saved\n",
      "Episode: 321 Total reward: -76.09173583984375 Training loss: 0.2110 Explore P: 0.2385\n",
      "Episode: 322 Total reward: -105.90095520019531 Training loss: 0.2189 Explore P: 0.2376\n",
      "Episode: 323 Total reward: -115.97056579589844 Training loss: 1.4479 Explore P: 0.2363\n",
      "Episode: 324 Total reward: -89.28366088867188 Training loss: 0.3621 Explore P: 0.2358\n",
      "Episode: 325 Total reward: -100.8292236328125 Training loss: 0.5232 Explore P: 0.2349\n",
      "Model Saved\n",
      "Episode: 326 Total reward: -27.055328369140625 Training loss: 0.2411 Explore P: 0.2340\n",
      "Episode: 327 Total reward: -98.34222412109375 Training loss: 0.4758 Explore P: 0.2318\n",
      "Model updated\n",
      "Episode: 328 Total reward: -53.324676513671875 Training loss: 0.7565 Explore P: 0.2307\n",
      "Episode: 329 Total reward: -23.246490478515625 Training loss: 0.4494 Explore P: 0.2299\n",
      "Episode: 330 Total reward: 4.3297882080078125 Training loss: 0.3324 Explore P: 0.2290\n",
      "Model Saved\n",
      "Episode: 331 Total reward: -30.933868408203125 Training loss: 0.2730 Explore P: 0.2281\n",
      "Episode: 332 Total reward: -105.43608093261719 Training loss: 0.3440 Explore P: 0.2272\n",
      "Episode: 333 Total reward: -87.3017578125 Training loss: 0.4057 Explore P: 0.2261\n",
      "Episode: 334 Total reward: -53.315093994140625 Training loss: 0.3513 Explore P: 0.2249\n",
      "Episode: 335 Total reward: -91.12139892578125 Training loss: 0.1113 Explore P: 0.2240\n",
      "Model Saved\n",
      "Episode: 336 Total reward: -76.93705749511719 Training loss: 0.2331 Explore P: 0.2228\n",
      "Episode: 337 Total reward: -31.918975830078125 Training loss: 0.1130 Explore P: 0.2220\n",
      "Episode: 338 Total reward: -71.43949890136719 Training loss: 0.3487 Explore P: 0.2211\n",
      "Episode: 339 Total reward: -99.00602722167969 Training loss: 0.3125 Explore P: 0.2202\n",
      "Episode: 340 Total reward: -82.9439697265625 Training loss: 3.7318 Explore P: 0.2198\n",
      "Model Saved\n",
      "Episode: 341 Total reward: -88.99525451660156 Training loss: 0.3906 Explore P: 0.2184\n",
      "Episode: 342 Total reward: -88.72801208496094 Training loss: 0.5872 Explore P: 0.2163\n",
      "Episode: 343 Total reward: -57.84144592285156 Training loss: 0.1194 Explore P: 0.2149\n",
      "Episode: 344 Total reward: -106.51652526855469 Training loss: 0.1932 Explore P: 0.2141\n",
      "Episode: 345 Total reward: -86.796630859375 Training loss: 0.3074 Explore P: 0.2134\n",
      "Model Saved\n",
      "Episode: 346 Total reward: -102.87576293945312 Training loss: 0.2818 Explore P: 0.2129\n",
      "Episode: 347 Total reward: 4.339813232421875 Training loss: 0.2052 Explore P: 0.2121\n",
      "Episode: 348 Total reward: -52.761688232421875 Training loss: 0.1832 Explore P: 0.2115\n",
      "Episode: 349 Total reward: -17.809051513671875 Training loss: 0.2868 Explore P: 0.2107\n",
      "Episode: 350 Total reward: -0.706817626953125 Training loss: 0.6140 Explore P: 0.2095\n",
      "Model Saved\n",
      "Episode: 351 Total reward: -60.71995544433594 Training loss: 0.3361 Explore P: 0.2087\n",
      "Episode: 352 Total reward: -7.316741943359375 Training loss: 0.1856 Explore P: 0.2079\n",
      "Episode: 353 Total reward: -20.662857055664062 Training loss: 0.1818 Explore P: 0.2071\n",
      "Episode: 354 Total reward: -115.11891174316406 Training loss: 0.1366 Explore P: 0.2064\n",
      "Episode: 355 Total reward: -42.75639343261719 Training loss: 0.6630 Explore P: 0.2056\n",
      "Model Saved\n",
      "Episode: 356 Total reward: -47.55278015136719 Training loss: 0.9711 Explore P: 0.2048\n",
      "Episode: 357 Total reward: -72.77091979980469 Training loss: 0.4097 Explore P: 0.2040\n",
      "Episode: 358 Total reward: -115.94557189941406 Training loss: 0.6834 Explore P: 0.2035\n",
      "Episode: 359 Total reward: -101.3621826171875 Training loss: 0.1532 Explore P: 0.2028\n",
      "Episode: 360 Total reward: -34.41456604003906 Training loss: 0.2251 Explore P: 0.2020\n",
      "Model Saved\n",
      "Episode: 361 Total reward: -77.46762084960938 Training loss: 0.5830 Explore P: 0.2013\n",
      "Episode: 362 Total reward: -83.02035522460938 Training loss: 0.5865 Explore P: 0.2004\n",
      "Episode: 363 Total reward: -75.42974853515625 Training loss: 1.3916 Explore P: 0.1986\n",
      "Episode: 364 Total reward: -115.9180908203125 Training loss: 0.2526 Explore P: 0.1981\n",
      "Episode: 365 Total reward: -103.45443725585938 Training loss: 0.2256 Explore P: 0.1973\n",
      "Model Saved\n",
      "Episode: 366 Total reward: -43.33866882324219 Training loss: 0.2769 Explore P: 0.1966\n",
      "Episode: 367 Total reward: -85.33175659179688 Training loss: 0.5713 Explore P: 0.1959\n",
      "Episode: 368 Total reward: -65.5908203125 Training loss: 0.5800 Explore P: 0.1948\n",
      "Episode: 369 Total reward: -73.40469360351562 Training loss: 0.5863 Explore P: 0.1940\n",
      "Episode: 370 Total reward: -95.11813354492188 Training loss: 0.1722 Explore P: 0.1933\n",
      "Model Saved\n",
      "Episode: 371 Total reward: -36.92463684082031 Training loss: 0.1348 Explore P: 0.1918\n",
      "Episode: 372 Total reward: -71.18862915039062 Training loss: 0.3351 Explore P: 0.1911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 373 Total reward: -115.42623901367188 Training loss: 0.3167 Explore P: 0.1903\n",
      "Episode: 374 Total reward: -88.26486206054688 Training loss: 0.4546 Explore P: 0.1896\n",
      "Episode: 375 Total reward: -112.24098205566406 Training loss: 1.1993 Explore P: 0.1889\n",
      "Model Saved\n",
      "Episode: 376 Total reward: -115.9771728515625 Training loss: 0.2250 Explore P: 0.1880\n",
      "Episode: 377 Total reward: -111.08822631835938 Training loss: 0.5546 Explore P: 0.1873\n",
      "Episode: 378 Total reward: -103.59797668457031 Training loss: 0.2350 Explore P: 0.1858\n",
      "Episode: 379 Total reward: -65.19097900390625 Training loss: 0.5464 Explore P: 0.1851\n",
      "Episode: 380 Total reward: -90.09780883789062 Training loss: 0.5532 Explore P: 0.1843\n",
      "Model Saved\n",
      "Episode: 381 Total reward: -112.5201416015625 Training loss: 0.2041 Explore P: 0.1831\n",
      "Episode: 382 Total reward: -115.99713134765625 Training loss: 0.7622 Explore P: 0.1824\n",
      "Episode: 383 Total reward: -37.97607421875 Training loss: 0.2844 Explore P: 0.1817\n",
      "Episode: 384 Total reward: -52.57524108886719 Training loss: 0.1252 Explore P: 0.1810\n",
      "Episode: 385 Total reward: -109.06953430175781 Training loss: 0.2193 Explore P: 0.1803\n",
      "Model Saved\n",
      "Episode: 386 Total reward: -70.83110046386719 Training loss: 0.1290 Explore P: 0.1796\n",
      "Episode: 387 Total reward: -115.99018859863281 Training loss: 0.1949 Explore P: 0.1790\n",
      "Episode: 388 Total reward: -108.16084289550781 Training loss: 0.6968 Explore P: 0.1783\n",
      "Episode: 389 Total reward: -112.162841796875 Training loss: 0.2557 Explore P: 0.1776\n",
      "Episode: 390 Total reward: -115.97578430175781 Training loss: 2.2623 Explore P: 0.1772\n",
      "Model Saved\n",
      "Episode: 391 Total reward: -111.39105224609375 Training loss: 0.2673 Explore P: 0.1765\n",
      "Episode: 392 Total reward: -51.48799133300781 Training loss: 0.1955 Explore P: 0.1758\n",
      "Episode: 393 Total reward: -115.99574279785156 Training loss: 0.2852 Explore P: 0.1752\n",
      "Episode: 394 Total reward: -45.61262512207031 Training loss: 0.5024 Explore P: 0.1745\n",
      "Episode: 395 Total reward: -85.76202392578125 Training loss: 0.4449 Explore P: 0.1739\n",
      "Model Saved\n",
      "Episode: 396 Total reward: -111.25215148925781 Training loss: 0.3203 Explore P: 0.1732\n",
      "Episode: 397 Total reward: -1.8066558837890625 Training loss: 0.5538 Explore P: 0.1718\n",
      "Episode: 398 Total reward: -72.2900390625 Training loss: 0.7178 Explore P: 0.1712\n",
      "Episode: 399 Total reward: -50.78004455566406 Training loss: 0.3003 Explore P: 0.1701\n",
      "Episode: 400 Total reward: -85.14012145996094 Training loss: 0.5118 Explore P: 0.1695\n",
      "Model Saved\n",
      "Episode: 401 Total reward: -74.84928894042969 Training loss: 0.5132 Explore P: 0.1676\n",
      "Episode: 402 Total reward: -115.21865844726562 Training loss: 1.2862 Explore P: 0.1670\n",
      "Episode: 403 Total reward: -68.70999145507812 Training loss: 0.3678 Explore P: 0.1660\n",
      "Episode: 404 Total reward: -70.04011535644531 Training loss: 0.7383 Explore P: 0.1654\n",
      "Episode: 405 Total reward: -106.3763427734375 Training loss: 0.4384 Explore P: 0.1648\n",
      "Model Saved\n",
      "Episode: 406 Total reward: -95.33575439453125 Training loss: 1.0623 Explore P: 0.1628\n",
      "Episode: 407 Total reward: -27.987625122070312 Training loss: 0.6563 Explore P: 0.1621\n",
      "Episode: 408 Total reward: -78.34568786621094 Training loss: 0.3289 Explore P: 0.1615\n",
      "Episode: 409 Total reward: -44.14678955078125 Training loss: 0.9652 Explore P: 0.1606\n",
      "Episode: 410 Total reward: -44.78010559082031 Training loss: 0.3059 Explore P: 0.1600\n",
      "Model Saved\n",
      "Episode: 411 Total reward: -100.43594360351562 Training loss: 0.4417 Explore P: 0.1594\n",
      "Episode: 412 Total reward: -67.30171203613281 Training loss: 0.5126 Explore P: 0.1591\n",
      "Episode: 413 Total reward: -26.449310302734375 Training loss: 0.3601 Explore P: 0.1585\n",
      "Episode: 414 Total reward: -97.24699401855469 Training loss: 0.3930 Explore P: 0.1579\n",
      "Episode: 415 Total reward: -101.89964294433594 Training loss: 0.2742 Explore P: 0.1575\n",
      "Model Saved\n",
      "Episode: 416 Total reward: -79.33503723144531 Training loss: 0.6411 Explore P: 0.1569\n",
      "Episode: 417 Total reward: -88.88301086425781 Training loss: 0.3884 Explore P: 0.1557\n",
      "Episode: 418 Total reward: -62.83058166503906 Training loss: 0.3168 Explore P: 0.1551\n",
      "Episode: 419 Total reward: -58.94792175292969 Training loss: 0.4718 Explore P: 0.1545\n",
      "Episode: 420 Total reward: -56.66093444824219 Training loss: 0.2467 Explore P: 0.1539\n",
      "Model Saved\n",
      "Episode: 421 Total reward: 58.567108154296875 Training loss: 0.4330 Explore P: 0.1531\n",
      "Episode: 422 Total reward: -52.47166442871094 Training loss: 0.4569 Explore P: 0.1525\n",
      "Episode: 423 Total reward: -78.1905517578125 Training loss: 0.7668 Explore P: 0.1519\n",
      "Episode: 424 Total reward: -72.04463195800781 Training loss: 1.1605 Explore P: 0.1514\n",
      "Episode: 425 Total reward: -37.08631896972656 Training loss: 0.9874 Explore P: 0.1508\n",
      "Model Saved\n",
      "Episode: 426 Total reward: -46.548095703125 Training loss: 0.3878 Explore P: 0.1502\n",
      "Episode: 427 Total reward: -47.800750732421875 Training loss: 0.7150 Explore P: 0.1497\n",
      "Episode: 428 Total reward: -99.62611389160156 Training loss: 0.2196 Explore P: 0.1487\n",
      "Episode: 429 Total reward: -92.23600769042969 Training loss: 0.8544 Explore P: 0.1481\n",
      "Episode: 430 Total reward: -27.345046997070312 Training loss: 0.7483 Explore P: 0.1464\n",
      "Model Saved\n",
      "Episode: 431 Total reward: -84.4630126953125 Training loss: 1.1549 Explore P: 0.1458\n",
      "Episode: 432 Total reward: -31.240066528320312 Training loss: 0.8298 Explore P: 0.1450\n",
      "Episode: 433 Total reward: -89.10920715332031 Training loss: 0.5522 Explore P: 0.1445\n",
      "Episode: 434 Total reward: -84.46903991699219 Training loss: 0.3063 Explore P: 0.1442\n",
      "Model updated\n",
      "Episode: 435 Total reward: -31.71490478515625 Training loss: 0.6098 Explore P: 0.1435\n",
      "Model Saved\n",
      "Episode: 436 Total reward: -75.90699768066406 Training loss: 0.2382 Explore P: 0.1424\n",
      "Episode: 437 Total reward: -39.83740234375 Training loss: 0.8593 Explore P: 0.1419\n",
      "Episode: 438 Total reward: -58.243011474609375 Training loss: 0.4573 Explore P: 0.1412\n",
      "Episode: 439 Total reward: -47.70594787597656 Training loss: 0.9806 Explore P: 0.1407\n",
      "Episode: 440 Total reward: -70.25790405273438 Training loss: 0.4798 Explore P: 0.1401\n",
      "Model Saved\n",
      "Episode: 441 Total reward: -52.456634521484375 Training loss: 0.3057 Explore P: 0.1397\n",
      "Episode: 442 Total reward: -26.141006469726562 Training loss: 0.1617 Explore P: 0.1392\n",
      "Episode: 443 Total reward: -31.028152465820312 Training loss: 1.0605 Explore P: 0.1386\n",
      "Episode: 444 Total reward: 9.428665161132812 Training loss: 0.4173 Explore P: 0.1378\n",
      "Episode: 445 Total reward: -37.580535888671875 Training loss: 0.5013 Explore P: 0.1373\n",
      "Model Saved\n",
      "Episode: 446 Total reward: -40.81053161621094 Training loss: 0.7497 Explore P: 0.1364\n",
      "Episode: 447 Total reward: -22.47747802734375 Training loss: 0.4654 Explore P: 0.1359\n",
      "Episode: 448 Total reward: -77.13546752929688 Training loss: 0.3929 Explore P: 0.1354\n",
      "Episode: 449 Total reward: 228.19088745117188 Training loss: 0.3973 Explore P: 0.1337\n",
      "Episode: 450 Total reward: -84.00151062011719 Training loss: 0.6717 Explore P: 0.1332\n",
      "Model Saved\n",
      "Episode: 451 Total reward: 73.53594970703125 Training loss: 0.2602 Explore P: 0.1326\n",
      "Episode: 452 Total reward: -9.545455932617188 Training loss: 0.6158 Explore P: 0.1321\n",
      "Episode: 453 Total reward: -24.328140258789062 Training loss: 0.9770 Explore P: 0.1314\n",
      "Episode: 454 Total reward: -77.98715209960938 Training loss: 0.5565 Explore P: 0.1309\n",
      "Episode: 455 Total reward: -47.07341003417969 Training loss: 0.5286 Explore P: 0.1304\n",
      "Model Saved\n",
      "Episode: 456 Total reward: -81.28089904785156 Training loss: 0.5675 Explore P: 0.1299\n",
      "Episode: 457 Total reward: -72.63375854492188 Training loss: 0.8355 Explore P: 0.1294\n",
      "Episode: 458 Total reward: -16.870147705078125 Training loss: 0.5775 Explore P: 0.1289\n",
      "Episode: 459 Total reward: -56.71733093261719 Training loss: 0.2546 Explore P: 0.1284\n",
      "Episode: 460 Total reward: -6.038330078125 Training loss: 0.3827 Explore P: 0.1278\n",
      "Model Saved\n",
      "Episode: 461 Total reward: -75.64517211914062 Training loss: 0.3165 Explore P: 0.1273\n",
      "Episode: 462 Total reward: -97.5238037109375 Training loss: 0.4094 Explore P: 0.1271\n",
      "Episode: 463 Total reward: -35.87956237792969 Training loss: 1.0933 Explore P: 0.1266\n",
      "Episode: 464 Total reward: -52.49302673339844 Training loss: 0.4572 Explore P: 0.1261\n",
      "Episode: 465 Total reward: -49.20585632324219 Training loss: 0.3850 Explore P: 0.1256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Episode: 466 Total reward: 40.64173889160156 Training loss: 0.3309 Explore P: 0.1252\n",
      "Episode: 467 Total reward: -83.92100524902344 Training loss: 1.1255 Explore P: 0.1247\n",
      "Episode: 468 Total reward: 36.38462829589844 Training loss: 0.3237 Explore P: 0.1243\n",
      "Episode: 469 Total reward: 84.2318115234375 Training loss: 0.3104 Explore P: 0.1238\n",
      "Episode: 470 Total reward: 39.12879943847656 Training loss: 0.5808 Explore P: 0.1234\n",
      "Model Saved\n",
      "Episode: 471 Total reward: 41.696563720703125 Training loss: 0.7029 Explore P: 0.1229\n",
      "Episode: 472 Total reward: 4.04364013671875 Training loss: 0.6262 Explore P: 0.1225\n",
      "Episode: 473 Total reward: -88.25157165527344 Training loss: 0.7172 Explore P: 0.1222\n",
      "Episode: 474 Total reward: 30.411834716796875 Training loss: 0.2918 Explore P: 0.1217\n",
      "Episode: 475 Total reward: -56.63629150390625 Training loss: 0.4268 Explore P: 0.1213\n",
      "Model Saved\n",
      "Episode: 476 Total reward: -8.949920654296875 Training loss: 0.2119 Explore P: 0.1208\n",
      "Episode: 477 Total reward: 27.811203002929688 Training loss: 1.1406 Explore P: 0.1204\n",
      "Episode: 478 Total reward: 0.37591552734375 Training loss: 0.5112 Explore P: 0.1199\n",
      "Episode: 479 Total reward: -16.784149169921875 Training loss: 0.5633 Explore P: 0.1195\n",
      "Episode: 480 Total reward: -14.93499755859375 Training loss: 0.8608 Explore P: 0.1190\n",
      "Model Saved\n",
      "Episode: 481 Total reward: -23.594696044921875 Training loss: 1.0747 Explore P: 0.1186\n",
      "Episode: 482 Total reward: -44.49237060546875 Training loss: 0.7088 Explore P: 0.1182\n",
      "Episode: 483 Total reward: -6.63055419921875 Training loss: 0.3370 Explore P: 0.1177\n",
      "Episode: 484 Total reward: -82.62823486328125 Training loss: 0.3605 Explore P: 0.1175\n",
      "Episode: 485 Total reward: 38.21675109863281 Training loss: 0.2626 Explore P: 0.1170\n",
      "Model Saved\n",
      "Episode: 486 Total reward: -28.218994140625 Training loss: 0.4272 Explore P: 0.1166\n",
      "Episode: 487 Total reward: -20.852401733398438 Training loss: 0.4755 Explore P: 0.1162\n",
      "Episode: 488 Total reward: -39.89788818359375 Training loss: 0.5384 Explore P: 0.1157\n",
      "Episode: 489 Total reward: 8.840911865234375 Training loss: 0.4972 Explore P: 0.1153\n",
      "Episode: 490 Total reward: -36.38008117675781 Training loss: 0.4749 Explore P: 0.1149\n",
      "Model Saved\n",
      "Episode: 491 Total reward: -6.62506103515625 Training loss: 0.5120 Explore P: 0.1145\n",
      "Episode: 492 Total reward: -10.22088623046875 Training loss: 0.5442 Explore P: 0.1140\n",
      "Episode: 493 Total reward: -23.923904418945312 Training loss: 0.5669 Explore P: 0.1136\n",
      "Episode: 494 Total reward: -103.26776123046875 Training loss: 0.3723 Explore P: 0.1133\n",
      "Episode: 495 Total reward: -102.58866882324219 Training loss: 0.4847 Explore P: 0.1131\n",
      "Model Saved\n",
      "Episode: 496 Total reward: 23.549835205078125 Training loss: 0.3078 Explore P: 0.1127\n",
      "Episode: 497 Total reward: -59.102874755859375 Training loss: 1.0068 Explore P: 0.1123\n",
      "Episode: 498 Total reward: -65.69291687011719 Training loss: 0.3805 Explore P: 0.1118\n",
      "Episode: 499 Total reward: -73.58155822753906 Training loss: 0.9379 Explore P: 0.1113\n",
      "Episode: 500 Total reward: 55.11750793457031 Training loss: 0.8029 Explore P: 0.1109\n",
      "Model Saved\n",
      "Episode: 501 Total reward: -32.021636962890625 Training loss: 1.3271 Explore P: 0.1104\n",
      "Episode: 502 Total reward: -27.543426513671875 Training loss: 0.8610 Explore P: 0.1100\n",
      "Episode: 503 Total reward: -60.58262634277344 Training loss: 1.0472 Explore P: 0.1096\n",
      "Episode: 504 Total reward: -49.931427001953125 Training loss: 1.0313 Explore P: 0.1094\n",
      "Episode: 505 Total reward: -69.9580078125 Training loss: 1.1514 Explore P: 0.1090\n",
      "Model Saved\n",
      "Episode: 506 Total reward: -113.80194091796875 Training loss: 1.2493 Explore P: 0.1087\n",
      "Episode: 507 Total reward: -42.77903747558594 Training loss: 0.9641 Explore P: 0.1083\n",
      "Episode: 508 Total reward: -20.175003051757812 Training loss: 0.1836 Explore P: 0.1079\n",
      "Episode: 509 Total reward: -57.54234313964844 Training loss: 0.2126 Explore P: 0.1075\n",
      "Episode: 510 Total reward: -12.7977294921875 Training loss: 0.6370 Explore P: 0.1071\n",
      "Model Saved\n",
      "Episode: 511 Total reward: -35.17906188964844 Training loss: 0.2825 Explore P: 0.1067\n",
      "Episode: 512 Total reward: -62.10693359375 Training loss: 0.4019 Explore P: 0.1063\n",
      "Episode: 513 Total reward: -41.13861083984375 Training loss: 1.3770 Explore P: 0.1059\n",
      "Episode: 514 Total reward: -6.8430938720703125 Training loss: 0.3339 Explore P: 0.1056\n",
      "Episode: 515 Total reward: -44.91926574707031 Training loss: 0.4679 Explore P: 0.1052\n",
      "Model Saved\n",
      "Episode: 516 Total reward: -37.91893005371094 Training loss: 0.4895 Explore P: 0.1048\n",
      "Episode: 517 Total reward: -50.99334716796875 Training loss: 0.7078 Explore P: 0.1044\n",
      "Episode: 518 Total reward: -37.72023010253906 Training loss: 0.7207 Explore P: 0.1042\n",
      "Episode: 519 Total reward: -15.425888061523438 Training loss: 0.3618 Explore P: 0.1038\n",
      "Episode: 520 Total reward: 5.1431732177734375 Training loss: 0.6596 Explore P: 0.1034\n",
      "Model Saved\n",
      "Episode: 521 Total reward: 32.49488830566406 Training loss: 0.5314 Explore P: 0.1030\n",
      "Episode: 522 Total reward: -11.054244995117188 Training loss: 0.3977 Explore P: 0.1025\n",
      "Episode: 523 Total reward: 65.68937683105469 Training loss: 0.6621 Explore P: 0.1021\n",
      "Episode: 524 Total reward: -33.2706298828125 Training loss: 0.8446 Explore P: 0.1018\n",
      "Episode: 525 Total reward: -81.50473022460938 Training loss: 0.1549 Explore P: 0.1016\n",
      "Model Saved\n",
      "Episode: 526 Total reward: -29.497482299804688 Training loss: 1.0566 Explore P: 0.1013\n",
      "Episode: 527 Total reward: -7.9738922119140625 Training loss: 0.3195 Explore P: 0.1009\n",
      "Episode: 528 Total reward: -4.380126953125 Training loss: 0.4113 Explore P: 0.1006\n",
      "Episode: 529 Total reward: -95.73384094238281 Training loss: 0.4180 Explore P: 0.1002\n",
      "Episode: 530 Total reward: -70.94590759277344 Training loss: 0.6825 Explore P: 0.1000\n",
      "Model Saved\n",
      "Episode: 531 Total reward: -43.48533630371094 Training loss: 0.3768 Explore P: 0.0996\n",
      "Episode: 532 Total reward: -58.08082580566406 Training loss: 0.4545 Explore P: 0.0993\n",
      "Episode: 533 Total reward: -49.96366882324219 Training loss: 0.4573 Explore P: 0.0989\n",
      "Episode: 534 Total reward: 39.517181396484375 Training loss: 0.5234 Explore P: 0.0985\n",
      "Episode: 535 Total reward: -59.46220397949219 Training loss: 0.2264 Explore P: 0.0982\n",
      "Model Saved\n",
      "Episode: 536 Total reward: -49.70042419433594 Training loss: 0.6961 Explore P: 0.0978\n",
      "Episode: 537 Total reward: -48.725250244140625 Training loss: 0.4731 Explore P: 0.0975\n",
      "Episode: 538 Total reward: 22.822113037109375 Training loss: 0.6551 Explore P: 0.0971\n",
      "Episode: 539 Total reward: -99.16453552246094 Training loss: 0.2010 Explore P: 0.0968\n",
      "Episode: 540 Total reward: -91.58641052246094 Training loss: 0.1824 Explore P: 0.0965\n",
      "Model Saved\n",
      "Episode: 541 Total reward: -115.07058715820312 Training loss: 0.2847 Explore P: 0.0963\n",
      "Episode: 542 Total reward: -55.35569763183594 Training loss: 0.4201 Explore P: 0.0960\n",
      "Episode: 543 Total reward: -71.83464050292969 Training loss: 0.2538 Explore P: 0.0956\n",
      "Episode: 544 Total reward: -39.41680908203125 Training loss: 0.3883 Explore P: 0.0953\n",
      "Episode: 545 Total reward: -10.069137573242188 Training loss: 0.7605 Explore P: 0.0949\n",
      "Model Saved\n",
      "Episode: 546 Total reward: -61.25749206542969 Training loss: 0.7112 Explore P: 0.0946\n",
      "Episode: 547 Total reward: -33.68061828613281 Training loss: 0.6558 Explore P: 0.0942\n",
      "Episode: 548 Total reward: -37.740753173828125 Training loss: 0.4537 Explore P: 0.0939\n",
      "Episode: 549 Total reward: -54.04969787597656 Training loss: 0.2186 Explore P: 0.0937\n",
      "Episode: 550 Total reward: -97.13021850585938 Training loss: 0.4146 Explore P: 0.0934\n",
      "Model Saved\n",
      "Episode: 551 Total reward: -89.94746398925781 Training loss: 0.4658 Explore P: 0.0930\n",
      "Episode: 552 Total reward: -51.19636535644531 Training loss: 0.4760 Explore P: 0.0927\n",
      "Episode: 553 Total reward: -89.78118896484375 Training loss: 0.2835 Explore P: 0.0923\n",
      "Episode: 554 Total reward: -73.98757934570312 Training loss: 0.7515 Explore P: 0.0920\n",
      "Episode: 555 Total reward: -13.012176513671875 Training loss: 0.5912 Explore P: 0.0916\n",
      "Model Saved\n",
      "Episode: 556 Total reward: -76.12823486328125 Training loss: 0.5522 Explore P: 0.0913\n",
      "Model updated\n",
      "Episode: 557 Total reward: -90.17008972167969 Training loss: 1.3848 Explore P: 0.0910\n",
      "Episode: 558 Total reward: -92.37979125976562 Training loss: 3.5108 Explore P: 0.0908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 559 Total reward: -53.74351501464844 Training loss: 0.4626 Explore P: 0.0905\n",
      "Episode: 560 Total reward: -51.82545471191406 Training loss: 0.2924 Explore P: 0.0899\n",
      "Model Saved\n",
      "Episode: 561 Total reward: -67.82234191894531 Training loss: 0.3052 Explore P: 0.0896\n",
      "Episode: 562 Total reward: -84.06192016601562 Training loss: 2.1924 Explore P: 0.0894\n",
      "Episode: 563 Total reward: -55.214447021484375 Training loss: 0.3310 Explore P: 0.0891\n",
      "Episode: 564 Total reward: -66.06192016601562 Training loss: 0.1552 Explore P: 0.0887\n",
      "Episode: 565 Total reward: -77.30355834960938 Training loss: 0.5341 Explore P: 0.0883\n",
      "Model Saved\n",
      "Episode: 566 Total reward: -80.58245849609375 Training loss: 0.3213 Explore P: 0.0880\n",
      "Episode: 567 Total reward: -29.642776489257812 Training loss: 0.5376 Explore P: 0.0877\n",
      "Episode: 568 Total reward: -71.26087951660156 Training loss: 0.5381 Explore P: 0.0873\n",
      "Episode: 569 Total reward: -69.63667297363281 Training loss: 1.2826 Explore P: 0.0870\n",
      "Episode: 570 Total reward: -69.08769226074219 Training loss: 0.2996 Explore P: 0.0867\n",
      "Model Saved\n",
      "Episode: 571 Total reward: -81.308349609375 Training loss: 0.1586 Explore P: 0.0861\n",
      "Episode: 572 Total reward: -87.732177734375 Training loss: 0.3770 Explore P: 0.0860\n",
      "Episode: 573 Total reward: -53.74488830566406 Training loss: 0.5288 Explore P: 0.0857\n",
      "Episode: 574 Total reward: 4.48211669921875 Training loss: 0.3903 Explore P: 0.0855\n",
      "Episode: 575 Total reward: -25.626678466796875 Training loss: 0.2716 Explore P: 0.0852\n",
      "Model Saved\n",
      "Episode: 576 Total reward: -51.86590576171875 Training loss: 0.3583 Explore P: 0.0849\n",
      "Episode: 577 Total reward: -72.38296508789062 Training loss: 0.5723 Explore P: 0.0844\n",
      "Episode: 578 Total reward: -32.06156921386719 Training loss: 0.6048 Explore P: 0.0842\n",
      "Episode: 579 Total reward: -68.60719299316406 Training loss: 0.4157 Explore P: 0.0837\n",
      "Episode: 580 Total reward: -26.725341796875 Training loss: 0.3724 Explore P: 0.0834\n",
      "Model Saved\n",
      "Episode: 581 Total reward: -103.12159729003906 Training loss: 0.9826 Explore P: 0.0833\n",
      "Episode: 582 Total reward: -53.86235046386719 Training loss: 0.2950 Explore P: 0.0830\n",
      "Episode: 583 Total reward: -81.68907165527344 Training loss: 0.7350 Explore P: 0.0828\n",
      "Episode: 584 Total reward: -67.65608215332031 Training loss: 0.3506 Explore P: 0.0824\n",
      "Episode: 585 Total reward: -82.80754089355469 Training loss: 0.5931 Explore P: 0.0820\n",
      "Model Saved\n",
      "Episode: 586 Total reward: -5.5421905517578125 Training loss: 0.7180 Explore P: 0.0818\n",
      "Episode: 587 Total reward: -71.42837524414062 Training loss: 0.5915 Explore P: 0.0815\n",
      "Episode: 588 Total reward: 14.458343505859375 Training loss: 0.9434 Explore P: 0.0812\n",
      "Episode: 589 Total reward: 2.847381591796875 Training loss: 0.6491 Explore P: 0.0809\n",
      "Episode: 590 Total reward: -47.64549255371094 Training loss: 0.2717 Explore P: 0.0807\n",
      "Model Saved\n",
      "Episode: 591 Total reward: 12.18994140625 Training loss: 0.8036 Explore P: 0.0804\n",
      "Episode: 592 Total reward: -24.020477294921875 Training loss: 0.4594 Explore P: 0.0801\n",
      "Episode: 593 Total reward: -100.93228149414062 Training loss: 12.8098 Explore P: 0.0799\n",
      "Episode: 594 Total reward: -0.204925537109375 Training loss: 0.5932 Explore P: 0.0796\n",
      "Episode: 595 Total reward: -38.63377380371094 Training loss: 1.4348 Explore P: 0.0792\n",
      "Model Saved\n",
      "Episode: 596 Total reward: 11.058807373046875 Training loss: 2.5217 Explore P: 0.0790\n",
      "Episode: 597 Total reward: -29.198623657226562 Training loss: 0.4800 Explore P: 0.0787\n",
      "Episode: 598 Total reward: -32.252777099609375 Training loss: 0.8373 Explore P: 0.0784\n",
      "Episode: 599 Total reward: -50.55000305175781 Training loss: 0.7919 Explore P: 0.0781\n",
      "Episode: 600 Total reward: 75.39547729492188 Training loss: 0.5738 Explore P: 0.0779\n",
      "Model Saved\n",
      "Episode: 601 Total reward: 68.23921203613281 Training loss: 1.1747 Explore P: 0.0776\n",
      "Episode: 602 Total reward: 24.277267456054688 Training loss: 1.4533 Explore P: 0.0773\n",
      "Episode: 603 Total reward: 16.026229858398438 Training loss: 0.7327 Explore P: 0.0770\n",
      "Episode: 604 Total reward: 28.242889404296875 Training loss: 0.5557 Explore P: 0.0768\n",
      "Episode: 605 Total reward: -6.7697601318359375 Training loss: 0.5533 Explore P: 0.0765\n",
      "Model Saved\n",
      "Episode: 606 Total reward: 17.531875610351562 Training loss: 0.8157 Explore P: 0.0762\n",
      "Episode: 607 Total reward: 27.7874755859375 Training loss: 0.5748 Explore P: 0.0759\n",
      "Episode: 608 Total reward: 15.271194458007812 Training loss: 0.8898 Explore P: 0.0757\n",
      "Episode: 609 Total reward: -59.12702941894531 Training loss: 0.4095 Explore P: 0.0754\n",
      "Episode: 610 Total reward: -26.501358032226562 Training loss: 0.9076 Explore P: 0.0751\n",
      "Model Saved\n",
      "Episode: 611 Total reward: -21.96307373046875 Training loss: 0.3570 Explore P: 0.0749\n",
      "Episode: 612 Total reward: -97.22138977050781 Training loss: 0.4146 Explore P: 0.0747\n",
      "Episode: 613 Total reward: 40.05970764160156 Training loss: 0.9475 Explore P: 0.0744\n",
      "Episode: 614 Total reward: -85.422119140625 Training loss: 0.1850 Explore P: 0.0741\n",
      "Episode: 615 Total reward: -109.53955078125 Training loss: 0.3807 Explore P: 0.0739\n",
      "Model Saved\n",
      "Episode: 616 Total reward: -57.90559387207031 Training loss: 0.6752 Explore P: 0.0736\n",
      "Episode: 617 Total reward: 9.013046264648438 Training loss: 0.7905 Explore P: 0.0733\n",
      "Episode: 618 Total reward: -43.90834045410156 Training loss: 0.4276 Explore P: 0.0731\n",
      "Episode: 619 Total reward: -80.390380859375 Training loss: 0.3468 Explore P: 0.0728\n",
      "Episode: 620 Total reward: -90.62730407714844 Training loss: 1.8783 Explore P: 0.0726\n",
      "Model Saved\n",
      "Episode: 621 Total reward: -48.0721435546875 Training loss: 0.4809 Explore P: 0.0724\n",
      "Episode: 622 Total reward: -2.3072662353515625 Training loss: 0.6914 Explore P: 0.0722\n",
      "Episode: 623 Total reward: -52.93634033203125 Training loss: 0.2898 Explore P: 0.0719\n",
      "Episode: 624 Total reward: -78.41049194335938 Training loss: 0.3369 Explore P: 0.0718\n",
      "Episode: 625 Total reward: -12.76239013671875 Training loss: 0.3891 Explore P: 0.0715\n",
      "Model Saved\n",
      "Episode: 626 Total reward: -84.86770629882812 Training loss: 0.4814 Explore P: 0.0713\n",
      "Episode: 627 Total reward: -8.016677856445312 Training loss: 0.3605 Explore P: 0.0710\n",
      "Episode: 628 Total reward: -88.56282043457031 Training loss: 0.2270 Explore P: 0.0707\n",
      "Episode: 629 Total reward: -10.485992431640625 Training loss: 1.4940 Explore P: 0.0705\n",
      "Episode: 630 Total reward: -82.20199584960938 Training loss: 0.5231 Explore P: 0.0701\n",
      "Model Saved\n",
      "Episode: 631 Total reward: 20.521499633789062 Training loss: 0.3494 Explore P: 0.0699\n",
      "Episode: 632 Total reward: -8.8626708984375 Training loss: 0.4547 Explore P: 0.0696\n",
      "Episode: 633 Total reward: -88.78656005859375 Training loss: 0.3930 Explore P: 0.0694\n",
      "Episode: 634 Total reward: -81.83177185058594 Training loss: 0.9495 Explore P: 0.0692\n",
      "Episode: 635 Total reward: -28.802108764648438 Training loss: 0.3761 Explore P: 0.0690\n",
      "Model Saved\n",
      "Episode: 636 Total reward: -54.61177062988281 Training loss: 0.6651 Explore P: 0.0687\n",
      "Episode: 637 Total reward: -103.7415771484375 Training loss: 0.4256 Explore P: 0.0686\n",
      "Episode: 638 Total reward: -10.54620361328125 Training loss: 0.8043 Explore P: 0.0683\n",
      "Episode: 639 Total reward: -95.88114929199219 Training loss: 0.9094 Explore P: 0.0681\n",
      "Episode: 640 Total reward: -8.063751220703125 Training loss: 0.5112 Explore P: 0.0680\n",
      "Model Saved\n",
      "Episode: 641 Total reward: 31.598251342773438 Training loss: 0.4850 Explore P: 0.0677\n",
      "Episode: 642 Total reward: -17.382064819335938 Training loss: 1.2152 Explore P: 0.0675\n",
      "Episode: 643 Total reward: 39.62477111816406 Training loss: 0.3449 Explore P: 0.0673\n",
      "Episode: 644 Total reward: 5.7984466552734375 Training loss: 0.4023 Explore P: 0.0671\n",
      "Episode: 645 Total reward: 67.95462036132812 Training loss: 0.3924 Explore P: 0.0668\n",
      "Model Saved\n",
      "Episode: 646 Total reward: -84.32804870605469 Training loss: 0.3912 Explore P: 0.0667\n",
      "Episode: 647 Total reward: 69.3837890625 Training loss: 0.4975 Explore P: 0.0663\n",
      "Episode: 648 Total reward: -69.47308349609375 Training loss: 0.6158 Explore P: 0.0660\n",
      "Episode: 649 Total reward: -51.02415466308594 Training loss: 0.8531 Explore P: 0.0659\n",
      "Episode: 650 Total reward: 8.31231689453125 Training loss: 0.4181 Explore P: 0.0657\n",
      "Model Saved\n",
      "Episode: 651 Total reward: 34.18522644042969 Training loss: 0.3792 Explore P: 0.0655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 652 Total reward: -57.74993896484375 Training loss: 0.6254 Explore P: 0.0652\n",
      "Episode: 653 Total reward: -37.06684875488281 Training loss: 0.3139 Explore P: 0.0649\n",
      "Episode: 654 Total reward: -53.24530029296875 Training loss: 0.5504 Explore P: 0.0647\n",
      "Episode: 655 Total reward: 29.661941528320312 Training loss: 1.0787 Explore P: 0.0645\n",
      "Model Saved\n",
      "Episode: 656 Total reward: -71.65933227539062 Training loss: 0.5385 Explore P: 0.0643\n",
      "Episode: 657 Total reward: -52.538116455078125 Training loss: 0.5622 Explore P: 0.0641\n",
      "Episode: 658 Total reward: 32.28216552734375 Training loss: 0.5283 Explore P: 0.0639\n",
      "Episode: 659 Total reward: -70.91502380371094 Training loss: 0.4220 Explore P: 0.0636\n",
      "Episode: 660 Total reward: -79.70916748046875 Training loss: 0.3784 Explore P: 0.0634\n",
      "Model Saved\n",
      "Episode: 661 Total reward: -106.28431701660156 Training loss: 0.7306 Explore P: 0.0633\n",
      "Episode: 662 Total reward: -80.38011169433594 Training loss: 1.0219 Explore P: 0.0631\n",
      "Episode: 663 Total reward: 5.284027099609375 Training loss: 0.5127 Explore P: 0.0629\n",
      "Episode: 664 Total reward: -31.814132690429688 Training loss: 0.8071 Explore P: 0.0627\n",
      "Episode: 665 Total reward: -42.627166748046875 Training loss: 0.7907 Explore P: 0.0625\n",
      "Model Saved\n",
      "Episode: 666 Total reward: -23.527679443359375 Training loss: 0.3200 Explore P: 0.0623\n",
      "Episode: 667 Total reward: -84.19938659667969 Training loss: 0.4446 Explore P: 0.0621\n",
      "Episode: 668 Total reward: 15.150558471679688 Training loss: 0.7444 Explore P: 0.0618\n",
      "Episode: 669 Total reward: 35.20777893066406 Training loss: 0.4844 Explore P: 0.0616\n",
      "Episode: 670 Total reward: -21.775588989257812 Training loss: 0.2994 Explore P: 0.0614\n",
      "Model Saved\n",
      "Episode: 671 Total reward: -31.789276123046875 Training loss: 1.5093 Explore P: 0.0612\n",
      "Episode: 672 Total reward: 18.114883422851562 Training loss: 0.2982 Explore P: 0.0610\n",
      "Episode: 673 Total reward: -77.03413391113281 Training loss: 0.5002 Explore P: 0.0608\n",
      "Episode: 674 Total reward: -87.26695251464844 Training loss: 0.5062 Explore P: 0.0607\n",
      "Episode: 675 Total reward: -28.07611083984375 Training loss: 6.6993 Explore P: 0.0605\n",
      "Model Saved\n",
      "Episode: 676 Total reward: 2.246826171875 Training loss: 1.1815 Explore P: 0.0603\n",
      "Episode: 677 Total reward: -45.074188232421875 Training loss: 0.2628 Explore P: 0.0602\n",
      "Episode: 678 Total reward: -48.31761169433594 Training loss: 0.2652 Explore P: 0.0600\n",
      "Episode: 679 Total reward: -16.553939819335938 Training loss: 0.2975 Explore P: 0.0597\n",
      "Episode: 680 Total reward: -39.90763854980469 Training loss: 0.4073 Explore P: 0.0595\n",
      "Model Saved\n",
      "Episode: 681 Total reward: -43.574188232421875 Training loss: 0.3611 Explore P: 0.0593\n",
      "Model updated\n",
      "Episode: 682 Total reward: -7.9101104736328125 Training loss: 1.2791 Explore P: 0.0591\n",
      "Episode: 683 Total reward: 2.6699066162109375 Training loss: 1.0455 Explore P: 0.0589\n",
      "Episode: 684 Total reward: 57.860321044921875 Training loss: 0.2682 Explore P: 0.0587\n",
      "Episode: 685 Total reward: -67.62603759765625 Training loss: 0.1883 Explore P: 0.0586\n",
      "Model Saved\n",
      "Episode: 686 Total reward: -79.88203430175781 Training loss: 0.4611 Explore P: 0.0584\n",
      "Episode: 687 Total reward: -84.90892028808594 Training loss: 0.2162 Explore P: 0.0582\n",
      "Episode: 688 Total reward: 1.466094970703125 Training loss: 0.4759 Explore P: 0.0581\n",
      "Episode: 689 Total reward: -20.260604858398438 Training loss: 0.6673 Explore P: 0.0579\n",
      "Episode: 690 Total reward: 29.386428833007812 Training loss: 0.5451 Explore P: 0.0577\n",
      "Model Saved\n",
      "Episode: 691 Total reward: -0.330078125 Training loss: 0.9897 Explore P: 0.0575\n",
      "Episode: 692 Total reward: -7.3722381591796875 Training loss: 0.5188 Explore P: 0.0573\n",
      "Episode: 693 Total reward: 27.88818359375 Training loss: 0.2795 Explore P: 0.0571\n",
      "Episode: 694 Total reward: 52.23194885253906 Training loss: 1.2819 Explore P: 0.0569\n",
      "Episode: 695 Total reward: 93.04380798339844 Training loss: 0.2223 Explore P: 0.0566\n",
      "Model Saved\n",
      "Episode: 696 Total reward: -30.649337768554688 Training loss: 1.5133 Explore P: 0.0564\n",
      "Episode: 697 Total reward: 56.7061767578125 Training loss: 0.2125 Explore P: 0.0562\n",
      "Episode: 698 Total reward: 45.979278564453125 Training loss: 0.3781 Explore P: 0.0561\n",
      "Episode: 699 Total reward: 2.4577484130859375 Training loss: 0.2860 Explore P: 0.0559\n",
      "Episode: 700 Total reward: -54.8897705078125 Training loss: 0.4686 Explore P: 0.0557\n",
      "Model Saved\n",
      "Episode: 701 Total reward: 27.67376708984375 Training loss: 0.4708 Explore P: 0.0555\n",
      "Episode: 702 Total reward: 20.27252197265625 Training loss: 0.3600 Explore P: 0.0553\n",
      "Episode: 703 Total reward: 18.158950805664062 Training loss: 0.8104 Explore P: 0.0552\n",
      "Episode: 704 Total reward: -43.07093811035156 Training loss: 0.7435 Explore P: 0.0550\n",
      "Episode: 705 Total reward: 12.981292724609375 Training loss: 1.2410 Explore P: 0.0548\n",
      "Model Saved\n",
      "Episode: 706 Total reward: -8.970932006835938 Training loss: 0.6542 Explore P: 0.0546\n",
      "Episode: 707 Total reward: 27.337356567382812 Training loss: 0.7962 Explore P: 0.0544\n",
      "Episode: 708 Total reward: -83.74162292480469 Training loss: 0.5420 Explore P: 0.0543\n",
      "Episode: 709 Total reward: 6.500579833984375 Training loss: 0.6994 Explore P: 0.0542\n",
      "Episode: 710 Total reward: -82.08906555175781 Training loss: 0.5417 Explore P: 0.0541\n",
      "Model Saved\n",
      "Episode: 711 Total reward: -13.19451904296875 Training loss: 0.7025 Explore P: 0.0539\n",
      "Episode: 712 Total reward: -67.69122314453125 Training loss: 0.4765 Explore P: 0.0538\n",
      "Episode: 713 Total reward: 10.755218505859375 Training loss: 0.2895 Explore P: 0.0536\n",
      "Episode: 714 Total reward: -37.515625 Training loss: 0.7751 Explore P: 0.0535\n",
      "Episode: 715 Total reward: -73.18714904785156 Training loss: 1.6168 Explore P: 0.0534\n",
      "Model Saved\n",
      "Episode: 716 Total reward: -59.655975341796875 Training loss: 0.3486 Explore P: 0.0533\n",
      "Episode: 717 Total reward: -52.036956787109375 Training loss: 0.4549 Explore P: 0.0531\n",
      "Episode: 718 Total reward: -57.30293273925781 Training loss: 0.5708 Explore P: 0.0530\n",
      "Episode: 719 Total reward: -16.746124267578125 Training loss: 0.3277 Explore P: 0.0528\n",
      "Episode: 720 Total reward: 2.104583740234375 Training loss: 0.6620 Explore P: 0.0527\n",
      "Model Saved\n",
      "Episode: 721 Total reward: -30.008697509765625 Training loss: 0.2957 Explore P: 0.0525\n",
      "Episode: 722 Total reward: 16.006256103515625 Training loss: 1.3965 Explore P: 0.0523\n",
      "Episode: 723 Total reward: 4.2179412841796875 Training loss: 1.3687 Explore P: 0.0521\n",
      "Episode: 724 Total reward: -105.42875671386719 Training loss: 0.2951 Explore P: 0.0521\n",
      "Episode: 725 Total reward: -17.656936645507812 Training loss: 0.1664 Explore P: 0.0519\n",
      "Model Saved\n",
      "Episode: 726 Total reward: -65.16294860839844 Training loss: 0.7876 Explore P: 0.0518\n",
      "Episode: 727 Total reward: -80.7716064453125 Training loss: 0.6417 Explore P: 0.0516\n",
      "Episode: 728 Total reward: -71.52215576171875 Training loss: 1.0191 Explore P: 0.0515\n",
      "Episode: 729 Total reward: -94.56240844726562 Training loss: 0.2787 Explore P: 0.0514\n",
      "Episode: 730 Total reward: -10.398956298828125 Training loss: 0.5714 Explore P: 0.0512\n",
      "Model Saved\n",
      "Episode: 731 Total reward: -16.347305297851562 Training loss: 0.6672 Explore P: 0.0510\n",
      "Episode: 732 Total reward: -89.6776123046875 Training loss: 0.4670 Explore P: 0.0509\n",
      "Episode: 733 Total reward: -45.410980224609375 Training loss: 0.4516 Explore P: 0.0508\n",
      "Episode: 734 Total reward: -77.93327331542969 Training loss: 0.7327 Explore P: 0.0506\n",
      "Episode: 735 Total reward: -110.07621765136719 Training loss: 0.4311 Explore P: 0.0504\n",
      "Model Saved\n",
      "Episode: 736 Total reward: -49.74531555175781 Training loss: 0.2925 Explore P: 0.0503\n",
      "Episode: 737 Total reward: -88.65226745605469 Training loss: 2.1447 Explore P: 0.0501\n",
      "Episode: 738 Total reward: -58.19551086425781 Training loss: 0.4276 Explore P: 0.0500\n",
      "Episode: 739 Total reward: -95.10441589355469 Training loss: 0.8926 Explore P: 0.0498\n",
      "Episode: 740 Total reward: -76.73118591308594 Training loss: 0.4392 Explore P: 0.0496\n",
      "Model Saved\n",
      "Episode: 741 Total reward: -68.34915161132812 Training loss: 0.8348 Explore P: 0.0495\n",
      "Episode: 742 Total reward: -93.63047790527344 Training loss: 1.0001 Explore P: 0.0493\n",
      "Episode: 743 Total reward: -81.53010559082031 Training loss: 2.0832 Explore P: 0.0492\n",
      "Episode: 744 Total reward: -114.75607299804688 Training loss: 0.3546 Explore P: 0.0491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 745 Total reward: -83.429443359375 Training loss: 1.5945 Explore P: 0.0490\n",
      "Model Saved\n",
      "Episode: 746 Total reward: -64.96124267578125 Training loss: 0.5944 Explore P: 0.0488\n",
      "Episode: 747 Total reward: -82.14738464355469 Training loss: 0.7562 Explore P: 0.0487\n",
      "Episode: 748 Total reward: -82.09222412109375 Training loss: 0.5635 Explore P: 0.0486\n",
      "Episode: 749 Total reward: -52.020782470703125 Training loss: 0.2289 Explore P: 0.0484\n",
      "Episode: 750 Total reward: -64.74761962890625 Training loss: 1.1094 Explore P: 0.0483\n",
      "Model Saved\n",
      "Episode: 751 Total reward: -115.84693908691406 Training loss: 0.8680 Explore P: 0.0482\n",
      "Episode: 752 Total reward: -28.061111450195312 Training loss: 0.5781 Explore P: 0.0480\n",
      "Episode: 753 Total reward: -64.02908325195312 Training loss: 1.7329 Explore P: 0.0479\n",
      "Episode: 754 Total reward: -102.50846862792969 Training loss: 0.5536 Explore P: 0.0479\n",
      "Episode: 755 Total reward: -47.843780517578125 Training loss: 0.4629 Explore P: 0.0477\n",
      "Model Saved\n",
      "Episode: 756 Total reward: -30.724822998046875 Training loss: 0.6441 Explore P: 0.0475\n",
      "Episode: 757 Total reward: -6.967559814453125 Training loss: 1.3055 Explore P: 0.0474\n",
      "Episode: 758 Total reward: -55.369537353515625 Training loss: 0.8329 Explore P: 0.0472\n",
      "Episode: 759 Total reward: -75.97175598144531 Training loss: 0.5489 Explore P: 0.0472\n",
      "Episode: 760 Total reward: -40.50389099121094 Training loss: 0.3391 Explore P: 0.0470\n",
      "Model Saved\n",
      "Episode: 761 Total reward: -74.24635314941406 Training loss: 0.5866 Explore P: 0.0469\n",
      "Episode: 762 Total reward: -65.90188598632812 Training loss: 0.4577 Explore P: 0.0468\n",
      "Episode: 763 Total reward: -43.99717712402344 Training loss: 0.5947 Explore P: 0.0466\n",
      "Episode: 764 Total reward: -33.12884521484375 Training loss: 0.5745 Explore P: 0.0465\n",
      "Episode: 765 Total reward: -26.558517456054688 Training loss: 0.4297 Explore P: 0.0463\n",
      "Model Saved\n",
      "Episode: 766 Total reward: -66.64949035644531 Training loss: 0.2989 Explore P: 0.0462\n",
      "Episode: 767 Total reward: 9.837615966796875 Training loss: 4.1170 Explore P: 0.0460\n",
      "Episode: 768 Total reward: -22.938308715820312 Training loss: 0.7539 Explore P: 0.0460\n",
      "Episode: 769 Total reward: -48.56324768066406 Training loss: 0.4746 Explore P: 0.0458\n",
      "Episode: 770 Total reward: -87.07029724121094 Training loss: 0.2738 Explore P: 0.0457\n",
      "Model Saved\n",
      "Episode: 771 Total reward: -69.12115478515625 Training loss: 0.2716 Explore P: 0.0456\n",
      "Episode: 772 Total reward: -104.22315979003906 Training loss: 0.6791 Explore P: 0.0455\n",
      "Episode: 773 Total reward: -60.35859680175781 Training loss: 0.5810 Explore P: 0.0453\n",
      "Episode: 774 Total reward: -34.20648193359375 Training loss: 0.1650 Explore P: 0.0452\n",
      "Episode: 775 Total reward: -63.489166259765625 Training loss: 0.6098 Explore P: 0.0450\n",
      "Model Saved\n",
      "Episode: 776 Total reward: -84.60226440429688 Training loss: 0.5118 Explore P: 0.0450\n",
      "Episode: 777 Total reward: -71.75273132324219 Training loss: 0.8136 Explore P: 0.0449\n",
      "Episode: 778 Total reward: -51.60090637207031 Training loss: 0.2940 Explore P: 0.0448\n",
      "Episode: 779 Total reward: -69.49560546875 Training loss: 0.6459 Explore P: 0.0446\n",
      "Episode: 780 Total reward: 7.18841552734375 Training loss: 0.6945 Explore P: 0.0445\n",
      "Model Saved\n",
      "Episode: 781 Total reward: -53.10115051269531 Training loss: 0.2869 Explore P: 0.0443\n",
      "Episode: 782 Total reward: -79.68116760253906 Training loss: 1.8102 Explore P: 0.0442\n",
      "Episode: 783 Total reward: -31.266250610351562 Training loss: 1.6938 Explore P: 0.0440\n",
      "Episode: 784 Total reward: -83.26898193359375 Training loss: 0.4726 Explore P: 0.0439\n",
      "Episode: 785 Total reward: -102.81523132324219 Training loss: 0.4707 Explore P: 0.0438\n",
      "Model Saved\n",
      "Episode: 786 Total reward: -47.27630615234375 Training loss: 0.2196 Explore P: 0.0437\n",
      "Episode: 787 Total reward: -89.69111633300781 Training loss: 1.1457 Explore P: 0.0436\n",
      "Episode: 788 Total reward: 24.69012451171875 Training loss: 0.2564 Explore P: 0.0434\n",
      "Episode: 789 Total reward: -10.041397094726562 Training loss: 1.1172 Explore P: 0.0433\n",
      "Episode: 790 Total reward: 17.716659545898438 Training loss: 0.3433 Explore P: 0.0432\n",
      "Model Saved\n",
      "Episode: 791 Total reward: 17.167724609375 Training loss: 0.4135 Explore P: 0.0430\n",
      "Episode: 792 Total reward: -101.12159729003906 Training loss: 0.4528 Explore P: 0.0429\n",
      "Episode: 793 Total reward: -44.049102783203125 Training loss: 0.8431 Explore P: 0.0428\n",
      "Episode: 794 Total reward: -59.536773681640625 Training loss: 0.3647 Explore P: 0.0426\n",
      "Episode: 795 Total reward: -35.89666748046875 Training loss: 0.3421 Explore P: 0.0425\n",
      "Model Saved\n",
      "Episode: 796 Total reward: -33.32740783691406 Training loss: 0.4033 Explore P: 0.0424\n",
      "Episode: 797 Total reward: 73.74148559570312 Training loss: 0.4465 Explore P: 0.0421\n",
      "Episode: 798 Total reward: -46.4986572265625 Training loss: 0.4373 Explore P: 0.0420\n",
      "Episode: 799 Total reward: -30.460067749023438 Training loss: 0.8460 Explore P: 0.0419\n",
      "Episode: 800 Total reward: 8.04412841796875 Training loss: 0.7731 Explore P: 0.0417\n",
      "Model Saved\n",
      "Episode: 801 Total reward: 8.292648315429688 Training loss: 1.3545 Explore P: 0.0416\n",
      "Episode: 802 Total reward: -107.35580444335938 Training loss: 0.5726 Explore P: 0.0415\n",
      "Episode: 803 Total reward: -92.49363708496094 Training loss: 0.1888 Explore P: 0.0415\n",
      "Episode: 804 Total reward: -87.53355407714844 Training loss: 0.5337 Explore P: 0.0413\n",
      "Episode: 805 Total reward: -42.50970458984375 Training loss: 1.5409 Explore P: 0.0412\n",
      "Model Saved\n",
      "Episode: 806 Total reward: -66.63394165039062 Training loss: 1.6543 Explore P: 0.0411\n",
      "Episode: 807 Total reward: -104.19729614257812 Training loss: 0.4863 Explore P: 0.0410\n",
      "Episode: 808 Total reward: -80.32472229003906 Training loss: 0.4058 Explore P: 0.0408\n",
      "Episode: 809 Total reward: -73.32693481445312 Training loss: 0.3073 Explore P: 0.0407\n",
      "Episode: 810 Total reward: -66.1015625 Training loss: 0.6095 Explore P: 0.0406\n",
      "Model Saved\n",
      "Episode: 811 Total reward: -81.48013305664062 Training loss: 0.5682 Explore P: 0.0405\n",
      "Episode: 812 Total reward: -74.48016357421875 Training loss: 0.9033 Explore P: 0.0404\n",
      "Episode: 813 Total reward: -54.802886962890625 Training loss: 0.6178 Explore P: 0.0403\n",
      "Episode: 814 Total reward: -83.04719543457031 Training loss: 1.4770 Explore P: 0.0402\n",
      "Episode: 815 Total reward: -51.45068359375 Training loss: 0.7941 Explore P: 0.0401\n",
      "Model Saved\n",
      "Episode: 816 Total reward: -2.161834716796875 Training loss: 0.3951 Explore P: 0.0400\n",
      "Model updated\n",
      "Episode: 817 Total reward: 24.05767822265625 Training loss: 0.9053 Explore P: 0.0398\n",
      "Episode: 818 Total reward: -84.01734924316406 Training loss: 0.5680 Explore P: 0.0397\n",
      "Episode: 819 Total reward: 51.34942626953125 Training loss: 0.8580 Explore P: 0.0396\n",
      "Episode: 820 Total reward: -88.1358642578125 Training loss: 0.5271 Explore P: 0.0395\n",
      "Model Saved\n",
      "Episode: 821 Total reward: 15.619873046875 Training loss: 0.3240 Explore P: 0.0394\n",
      "Episode: 822 Total reward: 49.63444519042969 Training loss: 0.5580 Explore P: 0.0393\n",
      "Episode: 823 Total reward: -1.956451416015625 Training loss: 0.3574 Explore P: 0.0392\n",
      "Episode: 824 Total reward: 5.244964599609375 Training loss: 0.2948 Explore P: 0.0390\n",
      "Episode: 825 Total reward: -76.55072021484375 Training loss: 0.6676 Explore P: 0.0390\n",
      "Model Saved\n",
      "Episode: 826 Total reward: -48.85520935058594 Training loss: 0.5990 Explore P: 0.0389\n",
      "Episode: 827 Total reward: -51.48930358886719 Training loss: 0.3073 Explore P: 0.0387\n",
      "Episode: 828 Total reward: 11.737777709960938 Training loss: 1.6103 Explore P: 0.0386\n",
      "Episode: 829 Total reward: 66.05052185058594 Training loss: 0.3768 Explore P: 0.0385\n",
      "Episode: 830 Total reward: 56.54145812988281 Training loss: 0.3652 Explore P: 0.0384\n",
      "Model Saved\n",
      "Episode: 831 Total reward: -72.83349609375 Training loss: 0.4848 Explore P: 0.0383\n",
      "Episode: 832 Total reward: 120.61668395996094 Training loss: 0.6444 Explore P: 0.0382\n",
      "Episode: 833 Total reward: -115.85855102539062 Training loss: 0.3397 Explore P: 0.0381\n",
      "Episode: 834 Total reward: -19.705657958984375 Training loss: 1.1920 Explore P: 0.0380\n",
      "Episode: 835 Total reward: -18.368377685546875 Training loss: 0.4264 Explore P: 0.0378\n",
      "Model Saved\n",
      "Episode: 836 Total reward: 41.743316650390625 Training loss: 0.4477 Explore P: 0.0377\n",
      "Episode: 837 Total reward: -93.04446411132812 Training loss: 0.3588 Explore P: 0.0376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 838 Total reward: -49.27473449707031 Training loss: 0.3268 Explore P: 0.0375\n",
      "Episode: 839 Total reward: 52.02732849121094 Training loss: 0.3725 Explore P: 0.0374\n",
      "Episode: 840 Total reward: 20.331863403320312 Training loss: 0.7094 Explore P: 0.0373\n",
      "Model Saved\n",
      "Episode: 841 Total reward: -90.11940002441406 Training loss: 0.2635 Explore P: 0.0372\n",
      "Episode: 842 Total reward: -105.08660888671875 Training loss: 0.7023 Explore P: 0.0370\n",
      "Episode: 843 Total reward: 1.126220703125 Training loss: 0.4947 Explore P: 0.0369\n",
      "Episode: 844 Total reward: -62.209320068359375 Training loss: 0.5883 Explore P: 0.0368\n",
      "Episode: 845 Total reward: -92.32077026367188 Training loss: 0.8467 Explore P: 0.0367\n",
      "Model Saved\n",
      "Episode: 846 Total reward: -99.07498168945312 Training loss: 0.8559 Explore P: 0.0366\n",
      "Episode: 847 Total reward: -18.788543701171875 Training loss: 0.6990 Explore P: 0.0365\n",
      "Episode: 848 Total reward: -68.82591247558594 Training loss: 0.2819 Explore P: 0.0363\n",
      "Episode: 849 Total reward: 4.8558807373046875 Training loss: 0.5639 Explore P: 0.0362\n",
      "Episode: 850 Total reward: -88.84852600097656 Training loss: 0.5727 Explore P: 0.0361\n",
      "Model Saved\n",
      "Episode: 851 Total reward: 13.202133178710938 Training loss: 0.8385 Explore P: 0.0360\n",
      "Episode: 852 Total reward: -95.44198608398438 Training loss: 0.3418 Explore P: 0.0359\n",
      "Episode: 853 Total reward: 55.43461608886719 Training loss: 2.0339 Explore P: 0.0358\n",
      "Episode: 854 Total reward: -49.52801513671875 Training loss: 0.7537 Explore P: 0.0357\n",
      "Episode: 855 Total reward: -33.64814758300781 Training loss: 0.2992 Explore P: 0.0356\n",
      "Model Saved\n",
      "Episode: 856 Total reward: -51.17924499511719 Training loss: 0.5272 Explore P: 0.0355\n",
      "Episode: 857 Total reward: -23.534225463867188 Training loss: 0.5797 Explore P: 0.0354\n",
      "Episode: 858 Total reward: -59.45710754394531 Training loss: 0.2956 Explore P: 0.0353\n",
      "Episode: 859 Total reward: 86.22016906738281 Training loss: 0.3994 Explore P: 0.0351\n",
      "Episode: 860 Total reward: -100.16273498535156 Training loss: 1.0989 Explore P: 0.0350\n",
      "Model Saved\n",
      "Episode: 861 Total reward: 37.53797912597656 Training loss: 0.9805 Explore P: 0.0349\n",
      "Episode: 862 Total reward: -84.69769287109375 Training loss: 0.3062 Explore P: 0.0348\n",
      "Episode: 863 Total reward: -30.470947265625 Training loss: 1.7367 Explore P: 0.0347\n",
      "Episode: 864 Total reward: -103.35096740722656 Training loss: 0.9767 Explore P: 0.0346\n",
      "Episode: 865 Total reward: -49.72636413574219 Training loss: 0.9799 Explore P: 0.0345\n",
      "Model Saved\n",
      "Episode: 866 Total reward: -54.09381103515625 Training loss: 0.8115 Explore P: 0.0344\n",
      "Episode: 867 Total reward: -51.105712890625 Training loss: 1.0296 Explore P: 0.0343\n",
      "Episode: 868 Total reward: 70.15895080566406 Training loss: 0.9602 Explore P: 0.0342\n",
      "Episode: 869 Total reward: -54.11030578613281 Training loss: 0.4869 Explore P: 0.0342\n",
      "Episode: 870 Total reward: -110.89024353027344 Training loss: 0.2917 Explore P: 0.0340\n",
      "Model Saved\n",
      "Episode: 871 Total reward: -60.36500549316406 Training loss: 0.1752 Explore P: 0.0339\n",
      "Episode: 872 Total reward: -115.91299438476562 Training loss: 0.4659 Explore P: 0.0339\n",
      "Episode: 873 Total reward: -2.699554443359375 Training loss: 0.7875 Explore P: 0.0338\n",
      "Episode: 874 Total reward: -9.906158447265625 Training loss: 1.5963 Explore P: 0.0337\n",
      "Episode: 875 Total reward: -41.38008117675781 Training loss: 1.0551 Explore P: 0.0336\n",
      "Model Saved\n",
      "Episode: 876 Total reward: -35.92686462402344 Training loss: 0.3946 Explore P: 0.0335\n",
      "Episode: 877 Total reward: -63.172088623046875 Training loss: 0.5528 Explore P: 0.0334\n",
      "Episode: 878 Total reward: -91.848876953125 Training loss: 0.5441 Explore P: 0.0333\n",
      "Episode: 879 Total reward: -55.80804443359375 Training loss: 0.4320 Explore P: 0.0332\n",
      "Episode: 880 Total reward: -19.983062744140625 Training loss: 0.7614 Explore P: 0.0331\n",
      "Model Saved\n",
      "Episode: 881 Total reward: 76.08528137207031 Training loss: 0.6837 Explore P: 0.0330\n",
      "Episode: 882 Total reward: 3.234100341796875 Training loss: 0.4098 Explore P: 0.0329\n",
      "Episode: 883 Total reward: -40.515899658203125 Training loss: 0.3475 Explore P: 0.0328\n",
      "Episode: 884 Total reward: 11.727493286132812 Training loss: 0.5785 Explore P: 0.0327\n",
      "Episode: 885 Total reward: -8.709274291992188 Training loss: 0.6861 Explore P: 0.0326\n",
      "Model Saved\n",
      "Episode: 886 Total reward: -69.21580505371094 Training loss: 1.0089 Explore P: 0.0325\n",
      "Episode: 887 Total reward: -76.97296142578125 Training loss: 0.8514 Explore P: 0.0324\n",
      "Episode: 888 Total reward: -23.718154907226562 Training loss: 0.5932 Explore P: 0.0324\n",
      "Episode: 889 Total reward: -100.26255798339844 Training loss: 0.3115 Explore P: 0.0323\n",
      "Episode: 890 Total reward: 39.852508544921875 Training loss: 0.3266 Explore P: 0.0322\n",
      "Model Saved\n",
      "Episode: 891 Total reward: -68.07530212402344 Training loss: 0.4027 Explore P: 0.0321\n",
      "Episode: 892 Total reward: 34.4979248046875 Training loss: 1.2864 Explore P: 0.0320\n",
      "Episode: 893 Total reward: 18.940597534179688 Training loss: 0.6262 Explore P: 0.0319\n",
      "Episode: 894 Total reward: -19.030364990234375 Training loss: 0.4102 Explore P: 0.0318\n",
      "Episode: 895 Total reward: 84.32876586914062 Training loss: 0.3880 Explore P: 0.0317\n",
      "Model Saved\n",
      "Episode: 896 Total reward: 28.095535278320312 Training loss: 1.1007 Explore P: 0.0316\n",
      "Episode: 897 Total reward: 36.88639831542969 Training loss: 0.8974 Explore P: 0.0316\n",
      "Episode: 898 Total reward: -6.7941436767578125 Training loss: 0.3445 Explore P: 0.0315\n",
      "Episode: 899 Total reward: -54.358978271484375 Training loss: 1.1134 Explore P: 0.0314\n",
      "Episode: 900 Total reward: -34.5872802734375 Training loss: 0.4408 Explore P: 0.0313\n",
      "Model Saved\n",
      "Episode: 901 Total reward: -78.8961181640625 Training loss: 0.9977 Explore P: 0.0313\n",
      "Episode: 902 Total reward: -42.76142883300781 Training loss: 0.4417 Explore P: 0.0312\n",
      "Episode: 903 Total reward: -90.47683715820312 Training loss: 0.6740 Explore P: 0.0312\n",
      "Episode: 904 Total reward: -10.484664916992188 Training loss: 1.5828 Explore P: 0.0311\n",
      "Episode: 905 Total reward: -16.127105712890625 Training loss: 0.9016 Explore P: 0.0310\n",
      "Model Saved\n",
      "Episode: 906 Total reward: 84.52021789550781 Training loss: 1.2634 Explore P: 0.0309\n",
      "Episode: 907 Total reward: 42.278594970703125 Training loss: 0.3457 Explore P: 0.0308\n",
      "Episode: 908 Total reward: 34.920257568359375 Training loss: 0.4447 Explore P: 0.0307\n",
      "Episode: 909 Total reward: 55.18418884277344 Training loss: 0.7080 Explore P: 0.0306\n",
      "Episode: 910 Total reward: -32.10993957519531 Training loss: 0.3738 Explore P: 0.0306\n",
      "Model Saved\n",
      "Episode: 911 Total reward: 53.74974060058594 Training loss: 0.3549 Explore P: 0.0305\n",
      "Episode: 912 Total reward: -29.826934814453125 Training loss: 0.6453 Explore P: 0.0304\n",
      "Episode: 913 Total reward: -27.808792114257812 Training loss: 0.4034 Explore P: 0.0303\n",
      "Episode: 914 Total reward: -26.32086181640625 Training loss: 0.4783 Explore P: 0.0302\n",
      "Episode: 915 Total reward: -65.41719055175781 Training loss: 0.2115 Explore P: 0.0302\n",
      "Model Saved\n",
      "Episode: 916 Total reward: -115.72531127929688 Training loss: 0.3123 Explore P: 0.0301\n",
      "Episode: 917 Total reward: -57.59422302246094 Training loss: 0.4605 Explore P: 0.0301\n",
      "Episode: 918 Total reward: 53.177337646484375 Training loss: 2.9468 Explore P: 0.0300\n",
      "Episode: 919 Total reward: -11.032958984375 Training loss: 0.4426 Explore P: 0.0299\n",
      "Episode: 920 Total reward: -47.16319274902344 Training loss: 0.5511 Explore P: 0.0299\n",
      "Model Saved\n",
      "Episode: 921 Total reward: 54.76350402832031 Training loss: 0.5823 Explore P: 0.0298\n",
      "Episode: 922 Total reward: 31.005294799804688 Training loss: 0.5449 Explore P: 0.0297\n",
      "Episode: 923 Total reward: 99.33769226074219 Training loss: 0.6732 Explore P: 0.0296\n",
      "Episode: 924 Total reward: -14.074478149414062 Training loss: 0.9453 Explore P: 0.0295\n",
      "Episode: 925 Total reward: -47.04998779296875 Training loss: 0.7035 Explore P: 0.0295\n",
      "Model Saved\n",
      "Episode: 926 Total reward: -15.578659057617188 Training loss: 1.4777 Explore P: 0.0294\n",
      "Episode: 927 Total reward: -51.55766296386719 Training loss: 2.2940 Explore P: 0.0294\n",
      "Episode: 928 Total reward: -87.96551513671875 Training loss: 0.6117 Explore P: 0.0293\n",
      "Episode: 929 Total reward: 11.5123291015625 Training loss: 0.9219 Explore P: 0.0292\n",
      "Episode: 930 Total reward: 44.04534912109375 Training loss: 0.6165 Explore P: 0.0291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Episode: 931 Total reward: -29.962509155273438 Training loss: 1.0022 Explore P: 0.0291\n",
      "Episode: 932 Total reward: -16.070220947265625 Training loss: 1.0140 Explore P: 0.0290\n",
      "Episode: 933 Total reward: -26.640335083007812 Training loss: 1.4401 Explore P: 0.0289\n",
      "Episode: 934 Total reward: -32.76564025878906 Training loss: 0.7647 Explore P: 0.0289\n",
      "Episode: 935 Total reward: 91.84123229980469 Training loss: 0.2977 Explore P: 0.0288\n",
      "Model Saved\n",
      "Episode: 936 Total reward: 56.42646789550781 Training loss: 0.7188 Explore P: 0.0287\n",
      "Episode: 937 Total reward: 73.69035339355469 Training loss: 0.5373 Explore P: 0.0286\n",
      "Episode: 938 Total reward: 50.99821472167969 Training loss: 1.6067 Explore P: 0.0286\n",
      "Episode: 939 Total reward: 43.45637512207031 Training loss: 1.0010 Explore P: 0.0285\n",
      "Episode: 940 Total reward: 52.567626953125 Training loss: 0.6392 Explore P: 0.0284\n",
      "Model Saved\n",
      "Episode: 941 Total reward: -11.382965087890625 Training loss: 1.3194 Explore P: 0.0283\n",
      "Episode: 942 Total reward: 46.459014892578125 Training loss: 1.0561 Explore P: 0.0283\n",
      "Episode: 943 Total reward: -115.1943359375 Training loss: 0.3775 Explore P: 0.0282\n",
      "Episode: 944 Total reward: -57.08514404296875 Training loss: 1.2927 Explore P: 0.0282\n",
      "Model updated\n",
      "Episode: 945 Total reward: -63.60057067871094 Training loss: 4.8399 Explore P: 0.0281\n",
      "Model Saved\n",
      "Episode: 946 Total reward: 92.23350524902344 Training loss: 1.0518 Explore P: 0.0280\n",
      "Episode: 947 Total reward: -51.73002624511719 Training loss: 0.5586 Explore P: 0.0280\n",
      "Episode: 948 Total reward: 48.887054443359375 Training loss: 0.6393 Explore P: 0.0279\n",
      "Episode: 949 Total reward: 66.93948364257812 Training loss: 0.8984 Explore P: 0.0278\n",
      "Episode: 950 Total reward: 105.45407104492188 Training loss: 1.5252 Explore P: 0.0278\n",
      "Model Saved\n",
      "Episode: 951 Total reward: 34.50071716308594 Training loss: 0.5862 Explore P: 0.0277\n",
      "Episode: 952 Total reward: 76.32649230957031 Training loss: 1.0383 Explore P: 0.0276\n",
      "Episode: 953 Total reward: 36.01194763183594 Training loss: 1.8193 Explore P: 0.0275\n",
      "Episode: 954 Total reward: 63.35150146484375 Training loss: 0.2594 Explore P: 0.0275\n",
      "Episode: 955 Total reward: 6.039154052734375 Training loss: 0.4177 Explore P: 0.0274\n",
      "Model Saved\n",
      "Episode: 956 Total reward: 7.5816497802734375 Training loss: 0.7543 Explore P: 0.0273\n",
      "Episode: 957 Total reward: 27.473968505859375 Training loss: 0.7452 Explore P: 0.0273\n",
      "Episode: 958 Total reward: -35.217132568359375 Training loss: 0.3655 Explore P: 0.0272\n",
      "Episode: 959 Total reward: -47.03057861328125 Training loss: 0.7532 Explore P: 0.0272\n",
      "Episode: 960 Total reward: 15.336837768554688 Training loss: 0.2227 Explore P: 0.0271\n",
      "Model Saved\n",
      "Episode: 961 Total reward: 21.643753051757812 Training loss: 1.1461 Explore P: 0.0270\n",
      "Episode: 962 Total reward: -47.403076171875 Training loss: 0.4814 Explore P: 0.0270\n",
      "Episode: 963 Total reward: -70.68307495117188 Training loss: 0.7522 Explore P: 0.0269\n",
      "Episode: 964 Total reward: -0.39569091796875 Training loss: 0.3449 Explore P: 0.0268\n",
      "Episode: 965 Total reward: 96.14425659179688 Training loss: 1.2059 Explore P: 0.0268\n",
      "Model Saved\n",
      "Episode: 966 Total reward: -25.445724487304688 Training loss: 0.4752 Explore P: 0.0267\n",
      "Episode: 967 Total reward: 83.17570495605469 Training loss: 0.2970 Explore P: 0.0266\n",
      "Episode: 968 Total reward: 114.75546264648438 Training loss: 0.2104 Explore P: 0.0266\n",
      "Episode: 969 Total reward: 54.50373840332031 Training loss: 0.7074 Explore P: 0.0265\n",
      "Episode: 970 Total reward: 94.13009643554688 Training loss: 0.2480 Explore P: 0.0264\n",
      "Model Saved\n",
      "Episode: 971 Total reward: -57.347747802734375 Training loss: 1.4862 Explore P: 0.0264\n",
      "Episode: 972 Total reward: 45.91728210449219 Training loss: 0.3821 Explore P: 0.0263\n",
      "Episode: 973 Total reward: 11.504898071289062 Training loss: 0.4497 Explore P: 0.0262\n",
      "Episode: 974 Total reward: -61.42570495605469 Training loss: 0.4444 Explore P: 0.0262\n",
      "Episode: 975 Total reward: -15.661727905273438 Training loss: 1.0267 Explore P: 0.0261\n",
      "Model Saved\n",
      "Episode: 976 Total reward: 14.660919189453125 Training loss: 1.0630 Explore P: 0.0260\n",
      "Episode: 977 Total reward: -8.555084228515625 Training loss: 0.4315 Explore P: 0.0260\n",
      "Episode: 978 Total reward: 27.076690673828125 Training loss: 0.6918 Explore P: 0.0259\n",
      "Episode: 979 Total reward: -2.7254791259765625 Training loss: 0.4717 Explore P: 0.0259\n",
      "Episode: 980 Total reward: 26.40234375 Training loss: 0.3792 Explore P: 0.0258\n",
      "Model Saved\n",
      "Episode: 981 Total reward: 95.31562805175781 Training loss: 0.8746 Explore P: 0.0257\n",
      "Episode: 982 Total reward: 169.7952117919922 Training loss: 0.3870 Explore P: 0.0257\n",
      "Episode: 983 Total reward: -27.75726318359375 Training loss: 0.3883 Explore P: 0.0256\n",
      "Episode: 984 Total reward: 19.942855834960938 Training loss: 0.4361 Explore P: 0.0256\n",
      "Episode: 985 Total reward: -56.899261474609375 Training loss: 0.3650 Explore P: 0.0255\n",
      "Model Saved\n",
      "Episode: 986 Total reward: 136.82395935058594 Training loss: 0.5046 Explore P: 0.0255\n",
      "Episode: 987 Total reward: -23.091964721679688 Training loss: 0.2921 Explore P: 0.0254\n",
      "Episode: 988 Total reward: 67.88581848144531 Training loss: 0.6130 Explore P: 0.0254\n",
      "Episode: 989 Total reward: 91.44749450683594 Training loss: 0.4618 Explore P: 0.0253\n",
      "Episode: 990 Total reward: 96.37297058105469 Training loss: 0.5283 Explore P: 0.0252\n",
      "Model Saved\n",
      "Episode: 991 Total reward: 68.9677734375 Training loss: 0.7603 Explore P: 0.0252\n",
      "Episode: 992 Total reward: -34.43217468261719 Training loss: 0.7848 Explore P: 0.0251\n",
      "Episode: 993 Total reward: 79.07206726074219 Training loss: 0.6392 Explore P: 0.0251\n",
      "Episode: 994 Total reward: 89.45149230957031 Training loss: 0.8084 Explore P: 0.0250\n",
      "Episode: 995 Total reward: 75.65689086914062 Training loss: 0.5185 Explore P: 0.0250\n",
      "Model Saved\n",
      "Episode: 996 Total reward: 1.922637939453125 Training loss: 0.3627 Explore P: 0.0249\n",
      "Episode: 997 Total reward: 84.86842346191406 Training loss: 0.7373 Explore P: 0.0248\n",
      "Episode: 998 Total reward: 9.142227172851562 Training loss: 0.6253 Explore P: 0.0248\n",
      "Episode: 999 Total reward: 30.845672607421875 Training loss: 1.5341 Explore P: 0.0247\n",
      "Episode: 1000 Total reward: 9.323638916015625 Training loss: 0.7307 Explore P: 0.0247\n",
      "Model Saved\n",
      "Episode: 1001 Total reward: -49.02964782714844 Training loss: 0.7437 Explore P: 0.0247\n",
      "Episode: 1002 Total reward: -12.751953125 Training loss: 0.6007 Explore P: 0.0246\n",
      "Episode: 1003 Total reward: 130.45263671875 Training loss: 0.3959 Explore P: 0.0245\n",
      "Episode: 1004 Total reward: 158.900146484375 Training loss: 0.7021 Explore P: 0.0245\n",
      "Episode: 1005 Total reward: 129.6399383544922 Training loss: 0.5338 Explore P: 0.0244\n",
      "Model Saved\n",
      "Episode: 1006 Total reward: 71.66966247558594 Training loss: 0.4288 Explore P: 0.0244\n",
      "Episode: 1007 Total reward: 80.41845703125 Training loss: 0.4865 Explore P: 0.0243\n",
      "Episode: 1008 Total reward: 141.02540588378906 Training loss: 0.7704 Explore P: 0.0242\n",
      "Episode: 1009 Total reward: 2.951568603515625 Training loss: 0.4632 Explore P: 0.0242\n",
      "Episode: 1010 Total reward: -60.34754943847656 Training loss: 1.0260 Explore P: 0.0241\n",
      "Model Saved\n",
      "Episode: 1011 Total reward: 20.814208984375 Training loss: 2.1213 Explore P: 0.0241\n",
      "Episode: 1012 Total reward: -83.72090148925781 Training loss: 0.6470 Explore P: 0.0241\n",
      "Episode: 1013 Total reward: -48.720703125 Training loss: 0.6087 Explore P: 0.0240\n",
      "Episode: 1014 Total reward: -15.647933959960938 Training loss: 1.1804 Explore P: 0.0240\n",
      "Episode: 1015 Total reward: -79.02101135253906 Training loss: 0.6667 Explore P: 0.0240\n",
      "Model Saved\n",
      "Episode: 1016 Total reward: 46.885406494140625 Training loss: 0.6502 Explore P: 0.0239\n",
      "Episode: 1017 Total reward: 57.22833251953125 Training loss: 0.5233 Explore P: 0.0238\n",
      "Episode: 1018 Total reward: 61.110595703125 Training loss: 1.1777 Explore P: 0.0238\n",
      "Episode: 1019 Total reward: 57.41343688964844 Training loss: 2.2124 Explore P: 0.0237\n",
      "Episode: 1020 Total reward: -8.811538696289062 Training loss: 0.3161 Explore P: 0.0237\n",
      "Model Saved\n",
      "Episode: 1021 Total reward: 54.872406005859375 Training loss: 0.7220 Explore P: 0.0236\n",
      "Episode: 1022 Total reward: 30.598846435546875 Training loss: 0.4915 Explore P: 0.0235\n",
      "Episode: 1023 Total reward: 19.757614135742188 Training loss: 0.9741 Explore P: 0.0235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1024 Total reward: -20.822830200195312 Training loss: 0.8533 Explore P: 0.0234\n",
      "Episode: 1025 Total reward: -61.5177001953125 Training loss: 0.3766 Explore P: 0.0234\n",
      "Model Saved\n",
      "Episode: 1026 Total reward: -82.12954711914062 Training loss: 0.9077 Explore P: 0.0233\n",
      "Episode: 1027 Total reward: 60.385162353515625 Training loss: 0.8783 Explore P: 0.0233\n",
      "Episode: 1028 Total reward: 10.349716186523438 Training loss: 0.2890 Explore P: 0.0232\n",
      "Episode: 1029 Total reward: -32.46330261230469 Training loss: 0.5167 Explore P: 0.0231\n",
      "Episode: 1030 Total reward: 109.65664672851562 Training loss: 0.7882 Explore P: 0.0231\n",
      "Model Saved\n",
      "Episode: 1031 Total reward: -61.597686767578125 Training loss: 0.8768 Explore P: 0.0230\n",
      "Episode: 1032 Total reward: -32.702911376953125 Training loss: 1.5530 Explore P: 0.0230\n",
      "Episode: 1033 Total reward: -39.45594787597656 Training loss: 0.6420 Explore P: 0.0229\n",
      "Episode: 1034 Total reward: -11.250625610351562 Training loss: 0.5990 Explore P: 0.0229\n",
      "Episode: 1035 Total reward: -44.10737609863281 Training loss: 0.5417 Explore P: 0.0228\n",
      "Model Saved\n",
      "Episode: 1036 Total reward: -112.56985473632812 Training loss: 0.8445 Explore P: 0.0228\n",
      "Episode: 1037 Total reward: 87.90425109863281 Training loss: 1.2994 Explore P: 0.0227\n",
      "Episode: 1038 Total reward: 8.26202392578125 Training loss: 0.4147 Explore P: 0.0227\n",
      "Episode: 1039 Total reward: 38.86265563964844 Training loss: 1.0447 Explore P: 0.0226\n",
      "Episode: 1040 Total reward: 35.85951232910156 Training loss: 0.4646 Explore P: 0.0226\n",
      "Model Saved\n",
      "Episode: 1041 Total reward: 127.83802795410156 Training loss: 0.2064 Explore P: 0.0225\n",
      "Episode: 1042 Total reward: 53.79515075683594 Training loss: 0.3167 Explore P: 0.0225\n",
      "Episode: 1043 Total reward: 158.8754425048828 Training loss: 0.5934 Explore P: 0.0224\n",
      "Episode: 1044 Total reward: -76.12721252441406 Training loss: 0.6225 Explore P: 0.0224\n",
      "Episode: 1045 Total reward: -52.06549072265625 Training loss: 0.3958 Explore P: 0.0223\n",
      "Model Saved\n",
      "Episode: 1046 Total reward: -38.07820129394531 Training loss: 0.3492 Explore P: 0.0223\n",
      "Episode: 1047 Total reward: -60.45452880859375 Training loss: 0.6100 Explore P: 0.0223\n",
      "Episode: 1048 Total reward: -22.424789428710938 Training loss: 0.6779 Explore P: 0.0222\n",
      "Episode: 1049 Total reward: 58.6746826171875 Training loss: 0.4579 Explore P: 0.0222\n",
      "Episode: 1050 Total reward: -94.2471923828125 Training loss: 0.7467 Explore P: 0.0221\n",
      "Model Saved\n",
      "Episode: 1051 Total reward: -5.413818359375 Training loss: 1.5257 Explore P: 0.0221\n",
      "Episode: 1052 Total reward: -41.16670227050781 Training loss: 0.6006 Explore P: 0.0220\n",
      "Episode: 1053 Total reward: -15.484848022460938 Training loss: 1.9616 Explore P: 0.0220\n",
      "Episode: 1054 Total reward: 50.32121276855469 Training loss: 0.6393 Explore P: 0.0219\n",
      "Episode: 1055 Total reward: 23.450515747070312 Training loss: 0.4329 Explore P: 0.0219\n",
      "Model Saved\n",
      "Episode: 1056 Total reward: -74.77630615234375 Training loss: 0.4439 Explore P: 0.0218\n",
      "Episode: 1057 Total reward: -33.683563232421875 Training loss: 0.9566 Explore P: 0.0218\n",
      "Episode: 1058 Total reward: 144.1182403564453 Training loss: 0.9195 Explore P: 0.0218\n",
      "Episode: 1059 Total reward: 128.8542022705078 Training loss: 0.5650 Explore P: 0.0217\n",
      "Episode: 1060 Total reward: -27.288070678710938 Training loss: 0.4421 Explore P: 0.0217\n",
      "Model Saved\n",
      "Episode: 1061 Total reward: 7.44403076171875 Training loss: 0.3854 Explore P: 0.0216\n",
      "Episode: 1062 Total reward: -62.27874755859375 Training loss: 0.2726 Explore P: 0.0216\n",
      "Episode: 1063 Total reward: 112.45481872558594 Training loss: 0.8067 Explore P: 0.0215\n",
      "Episode: 1064 Total reward: 163.92625427246094 Training loss: 0.5083 Explore P: 0.0215\n",
      "Episode: 1065 Total reward: 84.8458251953125 Training loss: 0.5118 Explore P: 0.0214\n",
      "Model Saved\n",
      "Episode: 1066 Total reward: -17.06170654296875 Training loss: 0.9369 Explore P: 0.0214\n",
      "Episode: 1067 Total reward: -75.73304748535156 Training loss: 0.7541 Explore P: 0.0214\n",
      "Episode: 1068 Total reward: 15.902679443359375 Training loss: 0.2676 Explore P: 0.0213\n",
      "Episode: 1069 Total reward: -92.37356567382812 Training loss: 0.4228 Explore P: 0.0213\n",
      "Episode: 1070 Total reward: -86.21685791015625 Training loss: 4.5735 Explore P: 0.0212\n",
      "Model Saved\n",
      "Episode: 1071 Total reward: -19.913650512695312 Training loss: 0.5814 Explore P: 0.0212\n",
      "Episode: 1072 Total reward: 3.5767974853515625 Training loss: 0.6769 Explore P: 0.0212\n",
      "Episode: 1073 Total reward: -4.09619140625 Training loss: 0.9544 Explore P: 0.0211\n",
      "Episode: 1074 Total reward: 231.40380859375 Training loss: 0.7192 Explore P: 0.0210\n",
      "Model updated\n",
      "Episode: 1075 Total reward: 150.4136199951172 Training loss: 3.6303 Explore P: 0.0210\n",
      "Model Saved\n",
      "Episode: 1076 Total reward: -11.237045288085938 Training loss: 0.8448 Explore P: 0.0209\n",
      "Episode: 1077 Total reward: -2.1824493408203125 Training loss: 0.9583 Explore P: 0.0209\n",
      "Episode: 1078 Total reward: 183.20481872558594 Training loss: 1.0737 Explore P: 0.0209\n",
      "Episode: 1079 Total reward: -50.78776550292969 Training loss: 0.7835 Explore P: 0.0208\n",
      "Episode: 1080 Total reward: -11.403564453125 Training loss: 1.2173 Explore P: 0.0208\n",
      "Model Saved\n",
      "Episode: 1081 Total reward: -62.2664794921875 Training loss: 0.3398 Explore P: 0.0208\n",
      "Episode: 1082 Total reward: 195.6370849609375 Training loss: 0.3773 Explore P: 0.0207\n",
      "Episode: 1083 Total reward: 66.12742614746094 Training loss: 0.7779 Explore P: 0.0207\n",
      "Episode: 1084 Total reward: -25.677963256835938 Training loss: 0.3742 Explore P: 0.0206\n",
      "Episode: 1085 Total reward: 49.528961181640625 Training loss: 1.3810 Explore P: 0.0206\n",
      "Model Saved\n",
      "Episode: 1086 Total reward: 59.192962646484375 Training loss: 1.2053 Explore P: 0.0205\n",
      "Episode: 1087 Total reward: 28.656585693359375 Training loss: 1.5790 Explore P: 0.0205\n",
      "Episode: 1088 Total reward: -39.84706115722656 Training loss: 0.7807 Explore P: 0.0205\n",
      "Episode: 1089 Total reward: -21.733642578125 Training loss: 0.2636 Explore P: 0.0204\n",
      "Episode: 1090 Total reward: -15.287521362304688 Training loss: 0.7003 Explore P: 0.0204\n",
      "Model Saved\n",
      "Episode: 1091 Total reward: -64.78997802734375 Training loss: 1.0679 Explore P: 0.0204\n",
      "Episode: 1092 Total reward: -115.80328369140625 Training loss: 1.3959 Explore P: 0.0203\n",
      "Episode: 1093 Total reward: -55.249481201171875 Training loss: 0.6942 Explore P: 0.0203\n",
      "Episode: 1094 Total reward: -52.896636962890625 Training loss: 0.4128 Explore P: 0.0203\n",
      "Episode: 1095 Total reward: -86.00553894042969 Training loss: 0.8110 Explore P: 0.0203\n",
      "Model Saved\n",
      "Episode: 1096 Total reward: -36.860809326171875 Training loss: 0.4453 Explore P: 0.0202\n",
      "Episode: 1097 Total reward: 269.9778594970703 Training loss: 0.5881 Explore P: 0.0201\n",
      "Episode: 1098 Total reward: 72.939208984375 Training loss: 1.1797 Explore P: 0.0201\n",
      "Episode: 1099 Total reward: 55.3482666015625 Training loss: 0.7791 Explore P: 0.0201\n",
      "Episode: 1100 Total reward: 157.83729553222656 Training loss: 0.3966 Explore P: 0.0200\n",
      "Model Saved\n",
      "Episode: 1101 Total reward: 2.451324462890625 Training loss: 0.8981 Explore P: 0.0200\n",
      "Episode: 1102 Total reward: -33.99897766113281 Training loss: 0.4314 Explore P: 0.0200\n",
      "Episode: 1103 Total reward: -13.7291259765625 Training loss: 0.3019 Explore P: 0.0199\n",
      "Episode: 1104 Total reward: 44.654693603515625 Training loss: 0.6566 Explore P: 0.0199\n",
      "Episode: 1105 Total reward: 0.2740631103515625 Training loss: 2.3866 Explore P: 0.0199\n",
      "Model Saved\n",
      "Episode: 1106 Total reward: 108.87796020507812 Training loss: 0.7479 Explore P: 0.0198\n",
      "Episode: 1107 Total reward: 10.854248046875 Training loss: 1.2149 Explore P: 0.0198\n",
      "Episode: 1108 Total reward: 181.90065002441406 Training loss: 0.8056 Explore P: 0.0197\n",
      "Episode: 1109 Total reward: -0.32843017578125 Training loss: 0.5512 Explore P: 0.0197\n"
     ]
    }
   ],
   "source": [
    "# Saver will help us to save our model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \n",
    "        decay_step = 0\n",
    "        \n",
    "        # Set tau = 0\n",
    "        tau = 0\n",
    "\n",
    "        # Init the game\n",
    "        game.init()\n",
    "        \n",
    "        # Update the parameters of our TargetNetwork with DQN_weights\n",
    "        update_target = update_target_graph()\n",
    "        sess.run(update_target)\n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "            \n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "            \n",
    "            # Make a new episode and observe the first state\n",
    "            game.new_episode()\n",
    "            \n",
    "            state = game.get_state().screen_buffer\n",
    "            \n",
    "            # Remember that stack frame function also call our preprocess function.\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True, stack_size)\n",
    "        \n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                \n",
    "                # Increase the C step\n",
    "                tau += 1\n",
    "                \n",
    "                # Increase decay_step\n",
    "                decay_step +=1\n",
    "                \n",
    "                # With Ïµ select a random action atat, otherwise select a = argmaxQ(st,a)\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "                # Do the action\n",
    "                reward = game.make_action(action)\n",
    "\n",
    "                # Look if the episode is finished\n",
    "                done = game.is_episode_finished()\n",
    "                \n",
    "                # Add the reward to total reward\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros((120,140), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False, stack_size)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "\n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                              'Total reward: {}'.format(total_reward),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Explore P: {:.4f}'.format(explore_probability))\n",
    "\n",
    "                    # Add experience to memory\n",
    "                    experience = state, action, reward, next_state, done\n",
    "                    memory.store(experience)\n",
    "\n",
    "                else:\n",
    "                    # Get the next state\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    \n",
    "                    # Stack the frame of the next_state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False, stack_size)\n",
    "                    \n",
    "\n",
    "                    # Add experience to memory\n",
    "                    experience = state, action, reward, next_state, done\n",
    "                    memory.store(experience)\n",
    "                    \n",
    "                    # st+1 is now our current state\n",
    "                    state = next_state\n",
    "\n",
    "\n",
    "                ### LEARNING PART            \n",
    "                # Obtain random mini-batch from memory\n",
    "                tree_idx, batch, ISWeights_mb = memory.sample(batch_size)\n",
    "                \n",
    "                states_mb = np.array([each[0][0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[0][1] for each in batch])\n",
    "                rewards_mb = np.array([each[0][2] for each in batch]) \n",
    "                next_states_mb = np.array([each[0][3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[0][4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                \n",
    "                ### DOUBLE DQN Logic\n",
    "                # Use DQNNetwork to select the action to take at next_state (a') (action with the highest Q-value)\n",
    "                # Use TargetNetwork to calculate the Q_val of Q(s',a')\n",
    "                \n",
    "                # Get Q values for next_state \n",
    "                q_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                # Calculate Qtarget for all actions that state\n",
    "                q_target_next_state = sess.run(TargetNetwork.output, feed_dict = {TargetNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                \n",
    "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma * Qtarget(s',a') \n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "                    \n",
    "                    # We got a'\n",
    "                    action = np.argmax(q_next_state[i])\n",
    "\n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        # Take the Qtarget for action a'\n",
    "                        target = rewards_mb[i] + gamma * q_target_next_state[i][action]\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                \n",
    "                _, loss, absolute_errors = sess.run([DQNetwork.optimizer, DQNetwork.loss, DQNetwork.absolute_errors],\n",
    "                                    feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                               DQNetwork.target_Q: targets_mb,\n",
    "                                               DQNetwork.actions_: actions_mb,\n",
    "                                              DQNetwork.ISWeights_: ISWeights_mb})\n",
    "              \n",
    "                \n",
    "                \n",
    "                # Update priority\n",
    "                memory.batch_update(tree_idx, absolute_errors)\n",
    "                \n",
    "                \n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                   DQNetwork.target_Q: targets_mb,\n",
    "                                                   DQNetwork.actions_: actions_mb,\n",
    "                                              DQNetwork.ISWeights_: ISWeights_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "                if tau > max_tau:\n",
    "                    # Update the parameters of our TargetNetwork with DQN_weights\n",
    "                    update_target = update_target_graph()\n",
    "                    sess.run(update_target)\n",
    "                    tau = 0\n",
    "                    print(\"Model updated\")\n",
    "\n",
    "            # Save model every 5 episodes\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
